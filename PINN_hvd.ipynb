{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/shared/pkg/devel/python/3.6.5/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "from torch import Tensor, ones, stack, load\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import horovod.torch as hvd\n",
    "import h5py as h5\n",
    "import os#\n",
    "import torch\n",
    "sys.path.append('/home/suarez08/PhD_PINNs')\n",
    "import PINNFramework as pf\n",
    "import sympy as sp\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import minterpy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_x = 2 # n-th eigenvalue of analytical solution\n",
    "ev_y = 1\n",
    "#Parameters for\n",
    "DIM = 2 \n",
    "DEG = 3\n",
    "LP=2\n",
    "POINTKIND = 'gauss_leg'#'leja'\n",
    "USEDATA = False\n",
    "#Doamin Bounds\n",
    "lb = np.array([-1.0, -1.0, 0.0])\n",
    "ub = np.array([1.0, 1.0, 0])\n",
    "# Number of Epoch\n",
    "n_epoch = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2-D Dataset from the analytical solution\n",
    "def Herm_pol(n):\n",
    "    p =  sp.Symbol('p')\n",
    "    Hn = sp.lambdify(p,sp.hermite(n, p))\n",
    "    return Hn\n",
    "\n",
    "def eigenvalue (ev_x, ev_y):\n",
    "    a =  sp.Symbol('a')\n",
    "    b =  sp.Symbol('b')\n",
    "    c =  sp.Symbol('c')\n",
    "    Hna = sp.hermite(ev_x,a)\n",
    "    Hnb = sp.hermite(ev_y,b)\n",
    "    N = (1/((2**ev_x*sp.factorial(ev_x))**(1/2))*(np.pi**(-1/4)))*(1/((2**ev_y*sp.factorial(ev_y))**(1/2))*(np.pi**(-1/4)))\n",
    "    Psi = N*sp.exp(-(a**2+b**2)/2)*Hna*Hnb\n",
    "    return int(sp.simplify((-1/2*(sp.diff(Psi,a,a)+sp.diff(Psi,b,b))+\n",
    "                            1/2*(a**2+b**2)*Psi)/Psi))\n",
    "lam = eigenvalue(ev_x, ev_y)#2*e_l+1#\n",
    "def Psi (x,y):\n",
    "    x = torch.Tensor(x).cuda()\n",
    "    y= torch.Tensor(y).cuda()\n",
    "    Hx = Herm_pol(ev_x)\n",
    "    Hy = Herm_pol(ev_y)     \n",
    "    psi_t = torch.exp(torch.complex(torch.Tensor([0]),torch.Tensor([0])))\n",
    "    N = 1/((2**ev_x*scipy.math.factorial(ev_x))**(1/2))*(np.pi**(-1/4))*1/((2**ev_y*scipy.math.factorial(ev_y))**(1/2))*(np.pi**(-1/4))\n",
    "    #1/(1+10*(x**2+y**2))#\n",
    "    return N*torch.exp(-(x**2+y**2)/2)*Hx(x)*Hy(y)\n",
    "def set_gen_1d(POLYDEG,n_bdy):\n",
    "    unscaled_pts = np.polynomial.legendre.leggauss(POLYDEG)\n",
    "    scaled_pts = []\n",
    "    weights = []\n",
    "    for i in range(2):\n",
    "        if (n_bdy[1][i]-n_bdy[0][i])/2 == 0:\n",
    "            arg_min =[k for k,j in enumerate(unscaled_pts[0]) if abs(j-n_bdy[1][i]) ==\n",
    "                       min(abs(unscaled_pts[0]-n_bdy[1][i]))] #[k for k,j in enumerate(unscaled_pts[0]) if j-n_bdy[1][i] ==\n",
    "            #           min(unscaled_pts[0]-n_bdy[1][i])]\n",
    "            scaled_pts.append([unscaled_pts[0][arg_min[0]]])\n",
    "            weights.append([1])\n",
    "        else:\n",
    "            m = (n_bdy[1][i]-n_bdy[0][i])/2\n",
    "            b = (n_bdy[0][i]+n_bdy[1][i])/2\n",
    "            scaled_pts.append(unscaled_pts[0]*m+b)\n",
    "            weights.append(unscaled_pts[1]*m)\n",
    "    Grid = np.array([[[scaled_pts[0][i],scaled_pts[1][j]]\n",
    "                   for i in range(len(scaled_pts[0]))]for j in range(len(scaled_pts[1]))]).reshape(len(scaled_pts[0])\n",
    "                                                              *len(scaled_pts[1]),2)\n",
    "    Weights = np.array([[weights[0][i]*weights[1][j]\n",
    "                   for i in range(len(weights[0]))]for j in range(len(weights[1]))]).reshape(len(weights[0])\n",
    "                                                              *len(weights[1]))\n",
    "    return Grid, Weights\n",
    "\n",
    "def set_gen_sob(POLYDEG,n_bdy, uns_points, uns_weights):\n",
    "    #unscaled_pts = np.polynomial.legendre.leggauss(POLYDEG)\n",
    "    scaled_pts = []\n",
    "    weights = []\n",
    "    for i in range(2):\n",
    "        if (n_bdy[1][i]-n_bdy[0][i])/2 == 0:\n",
    "            arg_min = [k for k,j in enumerate(uns_points) if j-n_bdy[1][i] == min(uns_points-n_bdy[1][i])]\n",
    "            scaled_pts.append(uns_points[arg_min])\n",
    "            weights.append([1])\n",
    "        else:\n",
    "            m = (n_bdy[1][i]-n_bdy[0][i])/2\n",
    "            b = (n_bdy[0][i]+n_bdy[1][i])/2\n",
    "            scaled_pts.append(uns_points*m+b)\n",
    "            weights.append(np.multiply(uns_weights,m))\n",
    "    Grid = np.array([[[[scaled_pts[0][i],scaled_pts[1][j],scaled_pts[2][k]]\n",
    "                   for i in range(len(scaled_pts[0]))]for j in range(len(scaled_pts[1]))]\n",
    "                 for k in range(len(scaled_pts[2]))]).reshape(len(scaled_pts[0])\n",
    "                                                              *len(scaled_pts[1])*\n",
    "                                                              len(scaled_pts[2]),3)\n",
    "    Weights = np.array([[[weights[0][i]*weights[1][j]*weights[2][k]\n",
    "                   for i in range(len(weights[0]))]for j in range(len(weights[1]))]\n",
    "                 for k in range(len(weights[2]))]).reshape(len(weights[0])\n",
    "                                                              *len(weights[1])*\n",
    "                                                              len(weights[2]))\n",
    "    return Grid, Weights\n",
    "def MSE_set(n_bdy,  nb, x_dim=1, y_dim=1, nsteps=1, dt=0.1):\n",
    "    x_0 = np.linspace(n_bdy[0][0], n_bdy[1][0], x_dim)\n",
    "    y_0 = np.linspace(n_bdy[0][1], n_bdy[1][1], y_dim)\n",
    "    X, Y = np.meshgrid(x_0, y_0)\n",
    "    X_0 = X.reshape(-1)\n",
    "    Y_0 = Y.reshape(-1)\n",
    "    idx_x = np.random.choice(len(X_0), nb, replace=False)\n",
    "    x = np.array([X_0[idx_x]]).T\n",
    "    y = np.array([Y_0[idx_x]]).T\n",
    "    #T = np.zeros(x.shape)\n",
    "    return np.array(np.concatenate([x,y], axis=1))\n",
    "\n",
    "\n",
    "class BoundaryConditionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, boundary_set) -> object:\n",
    "        \"\"\"_bdy, x_dim, y_dim, nsteps, d\n",
    "        Constructor of the Boundary condition dataset, with x_bdy an array with the\n",
    "        lower and uper bound in the x direction and respectively y_bdy. Only for square domain.\n",
    "        \"\"\"\n",
    "        self.Bdy_training = boundary_set\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns data for initial state\n",
    "        \"\"\"\n",
    "        return Tensor(self.Bdy_training).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        There exists no batch processing. So the size is 1\n",
    "        \"\"\"\n",
    "        return 1\n",
    "class InitialConditionDataset(Dataset):\n",
    "\n",
    "    def __init__(self,initial_set, norm = 'L2', n_bdy=[], x_dim=1, y_dim=1, nx =1 ):\n",
    "        \"\"\"\n",
    "        Constructor of the boundary condition dataset\n",
    "        Args:\n",
    "          n0 (int)\n",
    "        \"\"\"\n",
    "        super(type(self)).__init__()\n",
    "        if norm =='Mse' or norm== 'Wass':\n",
    "            x_0 = np.linspace(n_bdy[0][0], n_bdy[1][0], x_dim)\n",
    "            y_0 = np.linspace(n_bdy[0][1], n_bdy[1][1], y_dim)\n",
    "            X, Y = np.meshgrid(x_0, y_0)\n",
    "            X_0 = X.reshape(-1)\n",
    "            Y_0 = Y.reshape(-1)\n",
    "            idx_x = np.random.choice(len(X_0), nx, replace=False)\n",
    "            self.x = np.array([X_0[idx_x]]).T\n",
    "            self.y = np.array([Y_0[idx_x]]).T\n",
    "            sol = Psi(self.x, self.y)  # Psi(x,y,t=0,f)\n",
    "            self.u = sol\n",
    "            #self.t = np.array([np.zeros(len(self.x))]).T\n",
    "        elif norm == 'Quad' or norm == 'Sobolev_1':\n",
    "            X = np.array(initial_set[0])\n",
    "            Y = np.array(initial_set[1])\n",
    "            #X, Y = np.meshgrid(x_0, y_0)\n",
    "            self.x = np.array([X.reshape(-1)]).T\n",
    "            self.y = np.array([Y.reshape(-1)]).T\n",
    "            sol = Psi(self.x,self.y)\n",
    "            self.u = sol\n",
    "            #self.t = np.array([np.zeros(len(self.x))]).T\n",
    "        else:\n",
    "            raise ValueError('Norm not defined')\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        There exists no batch processing. So the size is 1\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = np.concatenate([self.x,self.y],axis=1)\n",
    "        y = self.u\n",
    "        return Tensor(x).float(), Tensor(y).float()\n",
    "\n",
    "class PDEDataset(Dataset):\n",
    "    def __init__(self, residual_set):\n",
    "        self.xf = np.array(residual_set).T\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns data for initial state\n",
    "        \"\"\"\n",
    "        return Tensor(self.xf).float().cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        There exists no batch processing. So the size is 1\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_legg_leja_ordered(n: int):    \n",
    "    n = int(n)\n",
    "    points1 = np.polynomial.legendre.leggauss(n+1)[0]\n",
    "    #chebychev_2nd_order(n + 1)[::-1]\n",
    "    points2 = points1  # TODO\n",
    "    ord = np.arange(1, n + 1)\n",
    "\n",
    "    lj = np.zeros([1, n + 1])\n",
    "    lj[0] = 0\n",
    "    m = 0\n",
    "\n",
    "    for k in range(0, n):\n",
    "        jj = 0\n",
    "        for i in range(0, n - k):\n",
    "            P = 1\n",
    "            for j in range(k + 1):\n",
    "                idx_pts = int(lj[0, j])\n",
    "                P = P * (points1[idx_pts] - points1[ord[i]])\n",
    "            P = np.abs(P)\n",
    "            if (P >= m):\n",
    "                jj = i\n",
    "                m = P\n",
    "        m = 0\n",
    "        lj[0, k + 1] = ord[jj]\n",
    "        ord = np.delete(ord, jj)\n",
    "\n",
    "    leja_points = np.zeros([n + 1, 1])\n",
    "    for i in range(n + 1):\n",
    "        leja_points[i, 0] = points2[int(lj[0, i])]\n",
    "    return leja_points\n",
    "def Hk_coeff(deg, k_sob, n_bdy):\n",
    "    print(n_bdy)\n",
    "    Df = [(n_bdy[1][i] - n_bdy[0][i])/2 for i in range(len(n_bdy))]\n",
    "    B = [(n_bdy[1][i] + n_bdy[0][i])/2 for i in range(len(n_bdy))]\n",
    "    def leg_points (n):\n",
    "        return np.polynomial.legendre.leggauss(n)[0]\n",
    "    mi = mp.multi_index.MultiIndex.from_degree(spatial_dimension=2, poly_degree=deg, lp_degree=np.inf)\n",
    "    ord_grid =mp.grid.Grid.from_generator(mi,gen_legg_leja_ordered).unisolvent_nodes\n",
    "    ord_grid =mp.grid.Grid(mi).unisolvent_nodes\n",
    "    x_g, y_g = np.meshgrid(np.polynomial.legendre.leggauss(deg+1)[0],\n",
    "                           np.polynomial.legendre.leggauss(deg+1)[0])\n",
    "    X_g = x_g.reshape((deg+1)**2)\n",
    "    Y_g = y_g.reshape((deg+1)**2)\n",
    "    #T = np.linspace(-1,-1, len(X_g))\n",
    "    un_grid = np.concatenate(([X_g],[Y_g]),axis=0).T\n",
    "    w_x, w_y = np.meshgrid(np.polynomial.legendre.leggauss(deg+1)[1],\n",
    "                           np.polynomial.legendre.leggauss(deg+1)[1])\n",
    "    un_weights = np.multiply(w_x.reshape((deg+1)**2),w_y.reshape((deg+1)**2))\n",
    "    ord_index = [[ i for i,j in enumerate(un_grid) if j[0] == ord_grid[k][0] and \n",
    "                 j[1] == ord_grid[k][1]][0] for k in range(len(ord_grid))]\n",
    "    ord_sc_weights = Df[0]*Df[1]*un_weights[ord_index]\n",
    "    ord_sc_grid = un_grid[ord_index]\n",
    "    #Compute Sobolev weights \n",
    "    W_HK = []\n",
    "    if k_sob == 1 or k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "        coeff_L_x = []\n",
    "        coeff_L_y = []\n",
    "        for i in range(len(mi)):\n",
    "            alpha = np.zeros(len(mi))\n",
    "            alpha[i] = 1 \n",
    "            #custom_grid = mp.Grid(mi,generating_values = grid_r[0][:,0])\n",
    "            lag_poly = mp.LagrangePolynomial(alpha, mi)\n",
    "            derivator_lag_poly = mp.Derivator(lag_poly, mp.LagrangePolynomial)\n",
    "            d_c = derivator_lag_poly.get_gradient_poly().coeffs\n",
    "            #alpha = np.divide(1,(1+d_c))\n",
    "            #sobolev_w = np.divide(lag_poly.coeffs,2)\n",
    "            coeff_L_x.append(d_c[:,0])\n",
    "            coeff_L_y.append(d_c[:,1])\n",
    "        h_w_x = torch.einsum('ab,b->ab',\n",
    "                 torch.tensor(coeff_L_x),torch.Tensor(ord_sc_weights))\n",
    "        h_w_y = torch.einsum('ab,b->ab',\n",
    "                     torch.tensor(coeff_L_y),torch.Tensor(ord_sc_weights))\n",
    "        H_x = torch.einsum('ab,cb->ac',torch.tensor(coeff_L_x), h_w_x)\n",
    "        H_y = torch.einsum('ab,cb->ac',torch.tensor(coeff_L_y), h_w_y)\n",
    "        Cx = 1/(deg**2)\n",
    "        H_1 = [H_x*1/Df[0]**2*Cx,H_y*1/Df[1]**2*Cx]\n",
    "        W_HK.append(H_1)\n",
    "        if k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "            HxH_x=torch.einsum('ab,cd->abcd',torch.tensor(coeff_L_x),torch.tensor(coeff_L_x))\n",
    "            HxH_y=torch.einsum('ab,cd->abcd',torch.tensor(coeff_L_y),torch.tensor(coeff_L_y))\n",
    "            HxH = [HxH_x, HxH_y]\n",
    "            ind_H2 = [[0,0],[0,1],[1,1]]\n",
    "            H_2 = []\n",
    "            for i in range(len(ind_H2)):\n",
    "                H_2.append(torch.einsum('abcd,bd->ac',Cx*1/Df[ind_H2[i][0]]**2*HxH[ind_H2[i][0]],\n",
    "                                        H_1[ind_H2[i][1]]))\n",
    "            W_HK.append(H_2)\n",
    "            if  k_sob == 3 or k_sob == 4:\n",
    "                H_3 = []\n",
    "                ind_H3 = [[0,0], [0,1],[0,2],[1,0],[1,2]]\n",
    "                for i in range(len(ind_H3)):\n",
    "                    H_3.append(torch.einsum('abcd,bd->ac',Cx*1/Df[ind_H3[i][0]]**2*HxH[ind_H3[i][0]],\n",
    "                                            H_2[ind_H3[i][1]]))\n",
    "                W_HK.append(H_3)\n",
    "                if k_sob == 4:\n",
    "                    H_4 = []\n",
    "                    ind_H4 = [[0,0],[0,1],[0,2],[0,3],[0,4],[1,0],[1,3],[1,4]]\n",
    "                    for i in range(len(ind_H4)):\n",
    "                        H_4.append(torch.einsum('abcd,bd->ac',Cx*1/Df[ind_H4[i][0]]**2*HxH[ind_H4[i][0]],\n",
    "                                                H_3[ind_H4[i][1]]))\n",
    "                    W_HK.append(H_4)\n",
    "        scaled_grid  = np.concatenate(([ord_sc_grid[:,0]*Df[0]+B[0]],[ord_sc_grid[:,1]*Df[1]+B[1]]),axis=0)\n",
    "    return scaled_grid, ord_sc_weights, W_HK\n",
    "def Hk_coeff_bdy(deg, k_sob, n_bdy):\n",
    "    mi = mp.multi_index.MultiIndex.from_degree(spatial_dimension=1, poly_degree=deg, lp_degree=np.inf)\n",
    "    ord_grid =mp.grid.Grid(mi).unisolvent_nodes.T[0]\n",
    "    un_grid , un_weights = np.polynomial.legendre.leggauss(deg+1)\n",
    "    ord_index = [[ i for i,j in enumerate(un_grid) if j == ord_grid[k]][0]\n",
    "                 for k in range(len(ord_grid))]\n",
    "    ord_sc_weights = un_weights[ord_index]\n",
    "    ord_sc_grid = un_grid[ord_index]\n",
    "    weights = []\n",
    "    if k_sob == 1 or k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "        coeff_L_x = []\n",
    "        for i in range(len(mi)):\n",
    "            alpha = np.zeros(len(mi))\n",
    "            alpha[i] = 1 \n",
    "            lag_poly = mp.LagrangePolynomial(alpha, mi)\n",
    "            derivator_lag_poly = mp.Derivator(lag_poly, mp.LagrangePolynomial)\n",
    "            d_c = derivator_lag_poly.get_gradient_poly().coeffs\n",
    "            coeff_L_x.append(d_c[:,0])\n",
    "        h_w_x = torch.einsum('ab,b->ab',\n",
    "                 torch.tensor(coeff_L_x),torch.Tensor(ord_sc_weights))\n",
    "        H_1 = torch.einsum('ab,cb->ac',torch.tensor(coeff_L_x), h_w_x)\n",
    "        weights.append(H_1)\n",
    "        if k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "            HxH_x=torch.einsum('ab,cd->abcd',torch.tensor(coeff_L_x),torch.tensor(coeff_L_x)) \n",
    "            H_2 = torch.einsum('abcd,bd->ac',HxH_x,H_1)\n",
    "            weights.append(H_2)\n",
    "            if  k_sob == 3 or k_sob == 4:\n",
    "                H_3 = torch.einsum('abcd,bd->ac',HxH_x,H_2)\n",
    "                weights.append(H_3)\n",
    "                if k_sob == 4:\n",
    "                    H_4 = torch.einsum('abcd,bd->ac',HxH_x,H_3)\n",
    "                    weights.append(H_4)\n",
    "    return ord_sc_grid, ord_sc_weights, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    hvd.init()\n",
    "\n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    # Domain bounds\n",
    "    #Create Datasets for the different Losses\n",
    "    def schroedinger1d(x, u):\n",
    "        global lam\n",
    "        omega = 1\n",
    "        pred = u\n",
    "        u = pred[:, 0]\n",
    "        #v = pred[:, 1]\n",
    "        grads = ones(u.shape, device=pred.device)  # move to the same device as prediction\n",
    "        grad_u = grad(u, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v = grad(v, x, create_graph=True, grad_outputs=grads)[0]\n",
    "\n",
    "        # calculate first order derivatives\n",
    "        u_x = grad_u[:, 0]\n",
    "        u_y = grad_u[:, 1]\n",
    "        #u_t = grad_u[:, 2]\n",
    "        # v_x = grad_v[:, 0]\n",
    "        #v_y = grad_v[:, 1]\n",
    "        #v_t = grad_v[:, 2]\n",
    "        # calculate second order derivatives\n",
    "        grad_u_x = grad(u_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v_x = grad(v_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        u_xx = grad_u_x[:, 0]\n",
    "        u_yy = grad_u_x[:, 1]\n",
    "        #v_xx = grad_v_x[:, 0]\n",
    "        #v_yy = grad_v_x[:, 1]\n",
    "        f_r = -1/2*(u_xx + u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u - lam*u\n",
    "        #print(torch.mean((-1/2*(u_xx - u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u)/u),torch.std((-1/2*(u_xx - u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u)/u))\n",
    "        #f_u = -1 * u_t - 0.5 * v_xx - 0.5 * v_yy + omega * 0.5 * (x[:, 0] ** 2) * v + omega * 0.5 * (x[:, 1] ** 2) * v\n",
    "        # fv is the imaginary part of the schrodinger equation\n",
    "        #f_v = -1 * v_t + 0.5 * u_xx + 0.5 * u_yy - omega * 0.5 * (x[:, 0] ** 2) * u - omega * 0.5 * (x[:, 1] ** 2) * u\n",
    "        return f_r#stack([f_u, f_v], 1)  # concatenate real part and imaginary part\n",
    "    def res_left(x, u):\n",
    "        omega = 1\n",
    "        pred = u\n",
    "        u = pred#pred[:, 0]\n",
    "        f_left =  lam*u\n",
    "        return f_left\n",
    "    def res_right(x, u):\n",
    "        omega = 1\n",
    "        pred = u\n",
    "        u = pred[:, 0]\n",
    "        #v = pred[:, 1]\n",
    "        grads = ones(u.shape, device=pred.device)  # move to the same device as prediction\n",
    "        grad_u = grad(u, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v = grad(v, x, create_graph=True, grad_outputs=grads)[0]\n",
    "\n",
    "        # calculate first order derivatives\n",
    "        u_x = grad_u[:, 0]\n",
    "        u_y = grad_u[:, 1]\n",
    "        #u_t = grad_u[:, 2]\n",
    "        #v_x = grad_v[:, 0]\n",
    "        #v_y = grad_v[:, 1]\n",
    "        #v_t = grad_v[:, 2]\n",
    "        # calculate second order derivatives\n",
    "        grad_u_x = grad(u_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v_x = grad(v_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        u_xx = grad_u_x[:, 0]\n",
    "        u_yy = grad_u_x[:, 1]\n",
    "        #v_xx = grad_v_x[:, 0]\n",
    "        #v_yy = grad_v_x[:, 1]\n",
    "        f_right = -1/2*(u_xx + u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u       \n",
    "        return f_right\n",
    "    def hom_dir(x):\n",
    "        P = Psi(x[:,0], x[:,1])\n",
    "        #P_r = P.real\n",
    "        #P_im = P.imag\n",
    "        return P# torch.stack((P_r,P_im),1)\n",
    "    def Dataset_loss (Norm, bounds , n_points, deg = 1, k_res=0, k_ini =0, k_bdy = 0):\n",
    "        [lb, ub] = bounds\n",
    "        #residual_bdy = [[lb[0], lb[1], lb[2]], [ub[0], ub[1], ub[2]]]\n",
    "        #initial_bdy = [[lb[0], lb[1], lb[2]], [ub[0], ub[1], lb[2]]]\n",
    "        #boundary_bdy = [[[lb[0], lb[1], lb[2]], [lb[0], ub[1], ub[2]]], [[ub[0], lb[1], lb[2]], [ub[0], ub[1], ub[2]]],\n",
    "        #                [[lb[0], lb[1], lb[2]], [ub[0], lb[1], ub[2]]],\n",
    "        #                [[lb[0], ub[1], lb[2]], [ub[0], ub[1], ub[2]]]]\n",
    "        residual_bdy = [[lb[0], lb[1]], [ub[0], ub[1]]]\n",
    "        initial_bdy = [[lb[0], lb[1]], [ub[0], ub[1]]]\n",
    "        boundary_bdy = [[[lb[0], lb[1]], [lb[0], ub[1]]], [[ub[0], lb[1]], [ub[0], ub[1]]],\n",
    "                        [[lb[0], lb[1]], [ub[0], lb[1]]],\n",
    "                        [[lb[0], ub[1]], [ub[0], ub[1]]]]\n",
    "        if Norm == 'Mse' or Norm == 'Wass':\n",
    "            boundary_set = np.concatenate([MSE_set(boundary_bdy[i], nb= int(n_points/4),\n",
    "                                                       x_dim=1000, y_dim=1000, nsteps=1, dt=0.1) for i in range(4)], axis=0)\n",
    "            #boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            residual_set = MSE_set(residual_bdy, nb= n_points, x_dim=1000, y_dim=1000, nsteps=1, dt=0.1).T\n",
    "            #RS_1d = set_gen_1d(deg, residual_bdy)\n",
    "            #residual_set = RS_1d[0].T\n",
    "            initial_set = []\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],[[0],[0],[0]]]\n",
    "        elif Norm == 'Quad':\n",
    "            I_C_1d = set_gen_1d(deg, initial_bdy)\n",
    "            initial_set = I_C_1d[0].T\n",
    "            initial_weights = I_C_1d[1]\n",
    "            boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            boundary_weights = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[1] for i in range(4)], axis=0)\n",
    "            #boundary_set, boundary_weights, Hb_x, Hb_y,  Hb_xx, Hb_xy, Hb_yy = H1_coeff_bdy(deg)\n",
    "            RS_1d = set_gen_1d(deg, residual_bdy)\n",
    "            residual_set = RS_1d[0].T\n",
    "            residual_weights = RS_1d[1]\n",
    "            #Hb_x, Hb_y,Hb_xx, Hb_xy, Hb_yy = [[],[],[],[],[]]\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],[boundary_weights, residual_weights, initial_weights]]\n",
    "        elif Norm == 'Sobolev_1_old':\n",
    "            n_bdy = [[-1,-1],[1,1]]\n",
    "            initial_set, initial_weights, Hkw_ini = Hk_coeff(deg, k_ini, n_bdy)\n",
    "            residual_set, res_weights , Hkw_res = Hk_coeff(deg, k_res, n_bdy)\n",
    "            #Create Boundary Set\n",
    "            boundary_set, boundary_weights, Hkw_bdy = Hk_coeff_bdy(int(deg*deg/4), k_bdy, n_bdy)\n",
    "            #boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            #boundary_weights = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[1] for i in range(4)], axis=0)\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],\n",
    "                        [[boundary_weights, Hkw_bdy], \n",
    "                         [res_weights , Hkw_res],\n",
    "                         [initial_weights, Hkw_ini]]]\n",
    "            ### CREATE THE LOSS FUNCTIONS\n",
    "        elif Norm == 'Sobolev_1':\n",
    "            ## Crete Loss functions\n",
    "        #Boundary term#initial_set, initial_weights, Hkw_ini = Hk_coeff(deg,2)\n",
    "            #residual_set, res_weights , Hkw_res = Hk_coeff(deg,0)\n",
    "            #Create Boundary Set\n",
    "            part_bdy = 1\n",
    "            part_res = 3\n",
    "            deg_bdy = DEG\n",
    "            deg_res = DEG\n",
    "            deg_ini = DEG\n",
    "            dx_bdy = abs(lb[0]-ub[0])/part_bdy\n",
    "            dy_bdy = abs(lb[1]-ub[1])/part_bdy\n",
    "            dx_res = abs(lb[0]-ub[0])/part_res\n",
    "            dy_res = abs(lb[1]-ub[1])/part_res\n",
    "            def part_dom_bdy(lb, dx_bdy, part_bdy, deg, k_der):\n",
    "                W_k = []\n",
    "                Gr = []\n",
    "                W_2 = []\n",
    "                for i in range(part_bdy):\n",
    "                    n_bdy = [lb+i*dx_bdy,lb+(i+1)*dx_bdy]\n",
    "                    weights, ord_sc_grid, ord_sc_weights = Hk_coeff_bdy(deg, k_der, n_bdy)\n",
    "                    W_k.append(weights)\n",
    "                    Gr.append(ord_sc_grid)\n",
    "                    W_2.append(ord_sc_weights)\n",
    "                return W_k, Gr, W_2\n",
    "            def part_dom(lb, dx_bdy, dy_bdy, part_bdy, deg, k_der):\n",
    "                W_k = []\n",
    "                Gr_x = []\n",
    "                Gr_y = []\n",
    "                Gr_z = []\n",
    "                W_2= []\n",
    "                for i in range(part_bdy):\n",
    "                    for j in range(part_bdy):\n",
    "                        n_bdy = [[lb[0]+i*dx_bdy, lb[1]+j*dy_bdy],[lb[0]+(i+1)*dx_bdy,lb[1]+(j+1)*dy_bdy]]\n",
    "                        ord_sc_grid, ord_sc_weights, Hkw_res = Hk_coeff(deg, k_der, n_bdy)\n",
    "                        W_k.append(Hkw_res)\n",
    "                        Gr_x.append(ord_sc_grid[0])\n",
    "                        Gr_y.append(ord_sc_grid[1])\n",
    "                        #Gr_z.append(ord_sc_grid[2])\n",
    "                        W_2.append(ord_sc_weights)\n",
    "                grid_x = np.concatenate(np.array(Gr_x))\n",
    "                grid_y = np.concatenate(np.array(Gr_y))\n",
    "                #grid_z = np.concatenate(np.array(Gr_z))\n",
    "                Gr = np.array(list(zip(grid_x,grid_y)))\n",
    "                return np.array(W_k), np.array(W_2), Gr.T\n",
    "            #Wb_k_x, Grb_x, Wb_2_x = part_dom_bdy(lb[0], dx_bdy, part_bdy, deg_bdy, 0)\n",
    "            #Wb_k_y, Grb_y, Wb_2_y = part_dom_bdy(lb[1], dy_bdy, part_bdy, deg_bdy, 0)\n",
    "            #boundary_set = [Grb_x, Grb_y]\n",
    "            #Wr_k, residual_set, Wr_2, deg = part_dom(lb, dx_res, dy_res, part_res, deg_res, 0)\n",
    "            # Residual Loss\n",
    "            Wr_k, Wr_2, residual_set = part_dom(lb, dx_res, dy_res, part_res, deg_res, k_res)\n",
    "            Wr_2 = np.concatenate(Wr_2)\n",
    "            #Initial condition Loss\n",
    "            Wi_k, Wi_2, initial_set= part_dom(lb, dx_res, dy_res, part_res, deg_ini, k_ini)\n",
    "            Wi_2 = np.concatenate(Wi_2)\n",
    "            #boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            #boundary_weights = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[1] for i in range(4)], axis=0)\n",
    "            n_bdy = [[-1,-1],[1,1]]\n",
    "            boundary_set, boundary_weights, Hkw_bdy = Hk_coeff_bdy(int(deg*deg/4), k_bdy, n_bdy)\n",
    "            #initial_set, ini_weights, Hkw_ini = Hk_coeff(deg,1)\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],\n",
    "                        [[boundary_weights, Hkw_bdy], \n",
    "                         [Wr_k, Wr_2],\n",
    "                         [Wi_k, Wi_2]]]#[Wi_k, Wi_2 ,deg]]]ยก\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError('Loss not defined'))\n",
    "        \n",
    "            ### CREATE THE LOSS FUNCTIONS\n",
    "        bc_dataset = BoundaryConditionDataset(Datasets[0][0])\n",
    "        dirichlet_bc = pf.DirichletBC(func=hom_dir, dataset=bc_dataset,\n",
    "                                      quad_weights=Datasets[1][0],sob_weights=Datasets[1][0]\n",
    "                                      ,name='Dirichlet BC', \n",
    "                                      norm=Norm)\n",
    "        # Residual Terms\n",
    "        pde_dataset = PDEDataset(Datasets[0][1])\n",
    "        pde_loss = pf.PDELoss(pde_dataset, schroedinger1d,\n",
    "                              func_left = res_left, func_right = res_right,\n",
    "                              quad_weights=Datasets[1][1],sob_weights=Datasets[1][1], norm = Norm)\n",
    "        #print(np.array(Datasets[0][2]).shape)\n",
    "        #Initial Condition Term\n",
    "        ic_dataset = InitialConditionDataset(Datasets[0][2], norm=Norm, n_bdy=initial_bdy, x_dim=1000, y_dim=1000, nx=n_points)\n",
    "        initial_condition = pf.InitialCondition(ic_dataset,  quad_weights=Datasets[1][2], \n",
    "                                                norm=Norm, sob_weights=Datasets[1][2])\n",
    "        #test_loss = tl.My_Loss(ic_dataset,  quad_weights=Datasets[1][2], norm=Norm)\n",
    "        return [dirichlet_bc, pde_loss, initial_condition], [bc_dataset, pde_dataset, ic_dataset], Datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'/home/suarez08/PhD_PINNs/Results_Simulation/14.06/'+str(ev_x)+'_invp_'+str(n_epoch)\n",
    "# Call the datasets functions, losses and weights for the training and for the performance measure\n",
    "[dirichlet_bc_2, pde_loss_2, initial_condition_2], [bc_dataset_2, pde_dataset_2, ic_dataset_2], [boundary_weights_2,\n",
    "                                                                                     residual_weights_2,\n",
    "                                                                                     initial_weights_2] = Dataset_loss('Quad', [lb, ub], 1, deg=DEG)\n",
    "[dirichlet_bc, pde_loss, initial_condition], [bc_dataset, pde_dataset, ic_dataset], [boundary_weights,\n",
    "                                                                                     residual_weights,\n",
    "                                                                                     initial_weights] = Dataset_loss('Mse', [lb, ub], len(initial_weights_2), deg=DEG)\n",
    "[dirichlet_bc_3, pde_loss_3, initial_condition_3], [bc_dataset_3, pde_dataset_3, ic_dataset_3], [boundary_weights_3,\n",
    "                                                                                     residual_weights_3,\n",
    "                                                                                     initial_weights_3] = Dataset_loss('Sobolev_1', [lb, ub], len(initial_weights_2), deg=DEG,\n",
    "                                                                                                                      k_res =2, k_ini = 4)\n",
    "model_1 = pf.models.MLP(input_size=2, output_size=1, hidden_size=50, num_hidden=10, lb=lb, ub=ub, activation=torch.nn.ELU())\n",
    "model_2 = pf.models.MLP(input_size=2, output_size=1, hidden_size=50, num_hidden=10, lb=lb, ub=ub, activation=torch.nn.ELU())\n",
    "model_3 = pf.models.MLP(input_size=2, output_size=1, hidden_size=50, num_hidden=10, lb=lb, ub=ub, activation=torch.nn.ELU())\n",
    "performance_var = [initial_condition, [dirichlet_bc], pde_loss]\n",
    "#pinn_1 = pf.PINN(model_1, 3, 1, pde_loss,initial_condition, performance_var, [dirichlet_bc], use_gpu=False\n",
    "#                )\n",
    "#loss_1 = pinn_1.fit(n_epoch, 'Adam', 1e-3,\n",
    "#                   pinn_path=folder+'best_model_Mse_'+str(DEG)+'_'+str(n_epoch)+'_.pt')\n",
    "#pinn_2 = pf.PINN(model_2, 3, 1, pde_loss_2, initial_condition_2, performance_var, [dirichlet_bc_2] ,use_gpu=False)\n",
    "#loss_2= pinn_2.fit(n_epoch, 'Adam', 1e-3,pinn_path=folder+\n",
    "#                'best_model_Quad_'+str(DEG)+'_'+str(n_epoch)+'_.pt')\n",
    "pinn_3 = pf.PINN(model_3, 2, 1, pde_loss_3, initial_condition_3, performance_var, [dirichlet_bc_3] ,use_gpu=True, use_horovod = True)\n",
    "loss_3 = pinn_3.fit(n_epoch, 'Adam', 1e-3,\n",
    "                    pinn_path = folder+'best_model_Wass_'+str(DEG)+'_'+str(n_epoch)+'_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
