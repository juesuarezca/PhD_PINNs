{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/shared/pkg/devel/python/3.6.5/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "from pyDOE import lhs\n",
    "from torch import Tensor, ones, stack, load\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import horovod\n",
    "import h5py as h5\n",
    "import os#\n",
    "import torch\n",
    "import PINNFramework as pf\n",
    "import sympy as sp\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import minterpy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_x = 2 # n-th eigenvalue of analytical solution\n",
    "ev_y = 2\n",
    "#Parameters for\n",
    "DIM = 2 \n",
    "DEG = 8\n",
    "LP=2\n",
    "POINTKIND = 'gauss_leg'#'leja'\n",
    "USEDATA = False\n",
    "#Doamin Bounds\n",
    "lb = np.array([-2.0, -2.0])\n",
    "ub = np.array([2.0, 2.0])\n",
    "# Number of Epoch\n",
    "n_epoch = 2000\n",
    "#Create 2-D Datast from the analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 2-D Dataset from the analytical solution\n",
    "def Herm_pol(n):\n",
    "    p =  sp.Symbol('p')\n",
    "    Hn = sp.lambdify(p,sp.hermite(n, p))\n",
    "    return Hn\n",
    "\n",
    "def eigenvalue (ev_x, ev_y):\n",
    "    a =  sp.Symbol('a')\n",
    "    b =  sp.Symbol('b')\n",
    "    c =  sp.Symbol('c')\n",
    "    Hna = sp.hermite(ev_x,a)\n",
    "    Hnb = sp.hermite(ev_y,b)\n",
    "    N = (1/((2**ev_x*sp.factorial(ev_x))**(1/2))*(np.pi**(-1/4)))*(1/((2**ev_y*sp.factorial(ev_y))**(1/2))*(np.pi**(-1/4)))\n",
    "    Psi = N*sp.exp(-(a**2+b**2)/2)*Hna*Hnb\n",
    "    return int(sp.simplify((-1/2*(sp.diff(Psi,a,a)+sp.diff(Psi,b,b))+\n",
    "                            1/2*(a**2+b**2)*Psi)/Psi))\n",
    "lam = eigenvalue(ev_x, ev_y)#2*e_l+1#\n",
    "def Psi (x,y):\n",
    "    x = torch.Tensor(x)\n",
    "    y= torch.Tensor(y)\n",
    "    Hx = Herm_pol(ev_x)\n",
    "    Hy = Herm_pol(ev_y)     \n",
    "    psi_t = torch.exp(torch.complex(torch.Tensor([0]),torch.Tensor([0])))\n",
    "    N = 1/((2**ev_x*scipy.math.factorial(ev_x))**(1/2))*(np.pi**(-1/4))*1/((2**ev_y*scipy.math.factorial(ev_y))**(1/2))*(np.pi**(-1/4))\n",
    "    #1/(1+10*(x**2+y**2))#\n",
    "    return N*torch.exp(-(x**2+y**2)/2)*Hx(x)*Hy(y)\n",
    "def set_gen_1d(POLYDEG,n_bdy):\n",
    "    unscaled_pts = np.polynomial.legendre.leggauss(POLYDEG)\n",
    "    scaled_pts = []\n",
    "    weights = []\n",
    "    for i in range(2):\n",
    "        if (n_bdy[1][i]-n_bdy[0][i])/2 == 0:\n",
    "            arg_min =[k for k,j in enumerate(unscaled_pts[0]) if abs(j-n_bdy[1][i]) ==\n",
    "                       min(abs(unscaled_pts[0]-n_bdy[1][i]))] #[k for k,j in enumerate(unscaled_pts[0]) if j-n_bdy[1][i] ==\n",
    "            #           min(unscaled_pts[0]-n_bdy[1][i])]\n",
    "            scaled_pts.append([unscaled_pts[0][arg_min[0]]])\n",
    "            weights.append([1])\n",
    "        else:\n",
    "            m = (n_bdy[1][i]-n_bdy[0][i])/2\n",
    "            b = (n_bdy[0][i]+n_bdy[1][i])/2\n",
    "            scaled_pts.append(unscaled_pts[0]*m+b)\n",
    "            weights.append(unscaled_pts[1]*m)\n",
    "    Grid = np.array([[[scaled_pts[0][i],scaled_pts[1][j]]\n",
    "                   for i in range(len(scaled_pts[0]))]for j in range(len(scaled_pts[1]))]).reshape(len(scaled_pts[0])\n",
    "                                                              *len(scaled_pts[1]),2)\n",
    "    Weights = np.array([[weights[0][i]*weights[1][j]\n",
    "                   for i in range(len(weights[0]))]for j in range(len(weights[1]))]).reshape(len(weights[0])\n",
    "                                                              *len(weights[1]))\n",
    "    return Grid, Weights\n",
    "\n",
    "def set_gen_sob(POLYDEG,n_bdy, uns_points, uns_weights):\n",
    "    #unscaled_pts = np.polynomial.legendre.leggauss(POLYDEG)\n",
    "    scaled_pts = []\n",
    "    weights = []\n",
    "    for i in range(2):\n",
    "        if (n_bdy[1][i]-n_bdy[0][i])/2 == 0:\n",
    "            arg_min = [k for k,j in enumerate(uns_points) if j-n_bdy[1][i] == min(uns_points-n_bdy[1][i])]\n",
    "            scaled_pts.append(uns_points[arg_min])\n",
    "            weights.append([1])\n",
    "        else:\n",
    "            m = (n_bdy[1][i]-n_bdy[0][i])/2\n",
    "            b = (n_bdy[0][i]+n_bdy[1][i])/2\n",
    "            scaled_pts.append(uns_points*m+b)\n",
    "            weights.append(np.multiply(uns_weights,m))\n",
    "    Grid = np.array([[[[scaled_pts[0][i],scaled_pts[1][j],scaled_pts[2][k]]\n",
    "                   for i in range(len(scaled_pts[0]))]for j in range(len(scaled_pts[1]))]\n",
    "                 for k in range(len(scaled_pts[2]))]).reshape(len(scaled_pts[0])\n",
    "                                                              *len(scaled_pts[1])*\n",
    "                                                              len(scaled_pts[2]),3)\n",
    "    Weights = np.array([[[weights[0][i]*weights[1][j]*weights[2][k]\n",
    "                   for i in range(len(weights[0]))]for j in range(len(weights[1]))]\n",
    "                 for k in range(len(weights[2]))]).reshape(len(weights[0])\n",
    "                                                              *len(weights[1])*\n",
    "                                                              len(weights[2]))\n",
    "    return Grid, Weights\n",
    "def MSE_set(n_bdy,  nb, x_dim=1, y_dim=1, nsteps=1, dt=0.1):\n",
    "    x_0 = np.linspace(n_bdy[0][0], n_bdy[1][0], x_dim)\n",
    "    y_0 = np.linspace(n_bdy[0][1], n_bdy[1][1], y_dim)\n",
    "    X, Y = np.meshgrid(x_0, y_0)\n",
    "    X_0 = X.reshape(-1)\n",
    "    Y_0 = Y.reshape(-1)\n",
    "    idx_x = np.random.choice(len(X_0), nb, replace=False)\n",
    "    x = np.array([X_0[idx_x]]).T\n",
    "    y = np.array([Y_0[idx_x]]).T\n",
    "    #T = np.zeros(x.shape)\n",
    "    return np.array(np.concatenate([x,y], axis=1))\n",
    "\n",
    "\n",
    "class BoundaryConditionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, boundary_set) -> object:\n",
    "        \"\"\"_bdy, x_dim, y_dim, nsteps, d\n",
    "        Constructor of the Boundary condition dataset, with x_bdy an array with the\n",
    "        lower and uper bound in the x direction and respectively y_bdy. Only for square domain.\n",
    "        \"\"\"\n",
    "        self.Bdy_training = boundary_set\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns data for initial state\n",
    "        \"\"\"\n",
    "        return Tensor(self.Bdy_training).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        There exists no batch processing. So the size is 1\n",
    "        \"\"\"\n",
    "        return 1\n",
    "class InitialConditionDataset(Dataset):\n",
    "\n",
    "    def __init__(self,initial_set, norm = 'L2', n_bdy=[], x_dim=1, y_dim=1, nx =1 ):\n",
    "        \"\"\"\n",
    "        Constructor of the boundary condition dataset\n",
    "        Args:\n",
    "          n0 (int)\n",
    "        \"\"\"\n",
    "        super(type(self)).__init__()\n",
    "        if norm =='Mse' or norm== 'Wass':\n",
    "            x_0 = np.linspace(n_bdy[0][0], n_bdy[1][0], x_dim)\n",
    "            y_0 = np.linspace(n_bdy[0][1], n_bdy[1][1], y_dim)\n",
    "            X, Y = np.meshgrid(x_0, y_0)\n",
    "            X_0 = X.reshape(-1)\n",
    "            Y_0 = Y.reshape(-1)\n",
    "            idx_x = np.random.choice(len(X_0), nx, replace=False)\n",
    "            self.x = np.array([X_0[idx_x]]).T\n",
    "            self.y = np.array([Y_0[idx_x]]).T\n",
    "            sol = Psi(self.x, self.y)  # Psi(x,y,t=0,f)\n",
    "            self.u = sol\n",
    "            #self.t = np.array([np.zeros(len(self.x))]).T\n",
    "        elif norm == 'Quad' or norm == 'Sobolev_1':\n",
    "            X = np.array(initial_set[0])\n",
    "            Y = np.array(initial_set[1])\n",
    "            #X, Y = np.meshgrid(x_0, y_0)\n",
    "            self.x = np.array([X.reshape(-1)]).T\n",
    "            self.y = np.array([Y.reshape(-1)]).T\n",
    "            sol = Psi(self.x,self.y)\n",
    "            self.u = sol\n",
    "            #self.t = np.array([np.zeros(len(self.x))]).T\n",
    "        else:\n",
    "            raise ValueError('Norm not defined')\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        There exists no batch processing. So the size is 1\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = np.concatenate([self.x,self.y],axis=1)\n",
    "        y = self.u\n",
    "        return Tensor(x).float(), Tensor(y).float()\n",
    "\n",
    "class PDEDataset(Dataset):\n",
    "    def __init__(self, residual_set):\n",
    "        self.xf = np.array(residual_set).T\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns data for initial state\n",
    "        \"\"\"\n",
    "        return Tensor(self.xf).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        There exists no batch processing. So the size is 1\n",
    "        \"\"\"\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_legg_leja_ordered(n: int):\n",
    "    n = int(n)\n",
    "    points1 = np.polynomial.legendre.leggauss(n+1)[0]\n",
    "    #chebychev_2nd_order(n + 1)[::-1]\n",
    "    points2 = points1  # TODO\n",
    "    ord = np.arange(1, n + 1)\n",
    "\n",
    "    lj = np.zeros([1, n + 1])\n",
    "    lj[0] = 0\n",
    "    m = 0\n",
    "\n",
    "    for k in range(0, n):\n",
    "        jj = 0\n",
    "        for i in range(0, n - k):\n",
    "            P = 1\n",
    "            for j in range(k + 1):\n",
    "                idx_pts = int(lj[0, j])\n",
    "                P = P * (points1[idx_pts] - points1[ord[i]])\n",
    "            P = np.abs(P)\n",
    "            if (P >= m):\n",
    "                jj = i\n",
    "                m = P\n",
    "        m = 0\n",
    "        lj[0, k + 1] = ord[jj]\n",
    "        ord = np.delete(ord, jj)\n",
    "\n",
    "    leja_points = np.zeros([n + 1, 1])\n",
    "    for i in range(n + 1):\n",
    "        leja_points[i, 0] = points2[int(lj[0, i])]\n",
    "    return leja_points\n",
    "def Hk_coeff(deg, k_sob, n_bdy):\n",
    "    print(n_bdy)\n",
    "    Df = [(n_bdy[1][i] - n_bdy[0][i])/2 for i in range(len(n_bdy))]\n",
    "    B = [(n_bdy[1][i] + n_bdy[0][i])/2 for i in range(len(n_bdy))]\n",
    "    def leg_points (n):\n",
    "        return np.polynomial.legendre.leggauss(n)[0]\n",
    "    mi = mp.multi_index.MultiIndex.from_degree(spatial_dimension=2, poly_degree=deg, lp_degree=np.inf)\n",
    "    ord_grid =mp.grid.Grid.from_generator(mi,gen_legg_leja_ordered).unisolvent_nodes\n",
    "    ord_grid =mp.grid.Grid(mi).unisolvent_nodes\n",
    "    x_g, y_g = np.meshgrid(np.polynomial.legendre.leggauss(deg+1)[0],\n",
    "                           np.polynomial.legendre.leggauss(deg+1)[0])\n",
    "    X_g = x_g.reshape((deg+1)**2)\n",
    "    Y_g = y_g.reshape((deg+1)**2)\n",
    "    #T = np.linspace(-1,-1, len(X_g))\n",
    "    un_grid = np.concatenate(([X_g],[Y_g]),axis=0).T\n",
    "    w_x, w_y = np.meshgrid(np.polynomial.legendre.leggauss(deg+1)[1],\n",
    "                           np.polynomial.legendre.leggauss(deg+1)[1])\n",
    "    un_weights = np.multiply(w_x.reshape((deg+1)**2),w_y.reshape((deg+1)**2))\n",
    "    ord_index = [[ i for i,j in enumerate(un_grid) if j[0] == ord_grid[k][0] and \n",
    "                 j[1] == ord_grid[k][1]][0] for k in range(len(ord_grid))]\n",
    "    ord_sc_weights = Df[0]*Df[1]*un_weights[ord_index]\n",
    "    ord_sc_grid = un_grid[ord_index]\n",
    "    #Compute Sobolev weights \n",
    "    W_HK = []\n",
    "    if k_sob == 1 or k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "        coeff_L_x = []\n",
    "        coeff_L_y = []\n",
    "        for i in range(len(mi)):\n",
    "            alpha = np.zeros(len(mi))\n",
    "            alpha[i] = 1 \n",
    "            #custom_grid = mp.Grid(mi,generating_values = grid_r[0][:,0])\n",
    "            lag_poly = mp.LagrangePolynomial(alpha, mi)\n",
    "            derivator_lag_poly = mp.Derivator(lag_poly, mp.LagrangePolynomial)\n",
    "            d_c = derivator_lag_poly.get_gradient_poly().coeffs\n",
    "            #alpha = np.divide(1,(1+d_c))\n",
    "            #sobolev_w = np.divide(lag_poly.coeffs,2)\n",
    "            coeff_L_x.append(d_c[:,0])\n",
    "            coeff_L_y.append(d_c[:,1])\n",
    "        h_w_x = torch.einsum('ab,b->ab',\n",
    "                 torch.tensor(coeff_L_x),torch.Tensor(ord_sc_weights))\n",
    "        h_w_y = torch.einsum('ab,b->ab',\n",
    "                     torch.tensor(coeff_L_y),torch.Tensor(ord_sc_weights))\n",
    "        H_x = torch.einsum('ab,cb->ac',torch.tensor(coeff_L_x), h_w_x)\n",
    "        H_y = torch.einsum('ab,cb->ac',torch.tensor(coeff_L_y), h_w_y)\n",
    "        Cx = 1/(deg**2)\n",
    "        H_1 = [H_x*1/Df[0]**2*Cx,H_y*1/Df[1]**2*Cx]\n",
    "        W_HK.append(H_1)\n",
    "        if k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "            HxH_x=torch.einsum('ab,cd->abcd',torch.tensor(coeff_L_x),torch.tensor(coeff_L_x))\n",
    "            HxH_y=torch.einsum('ab,cd->abcd',torch.tensor(coeff_L_y),torch.tensor(coeff_L_y))\n",
    "            HxH = [HxH_x, HxH_y]\n",
    "            ind_H2 = [[0,0],[0,1],[1,1]]\n",
    "            H_2 = []\n",
    "            for i in range(len(ind_H2)):\n",
    "                H_2.append(torch.einsum('abcd,bd->ac',Cx*1/Df[ind_H2[i][0]]**2*HxH[ind_H2[i][0]],\n",
    "                                        H_1[ind_H2[i][1]]))\n",
    "            W_HK.append(H_2)\n",
    "            if  k_sob == 3 or k_sob == 4:\n",
    "                H_3 = []\n",
    "                ind_H3 = [[0,0], [0,1],[0,2],[1,0],[1,2]]\n",
    "                for i in range(len(ind_H3)):\n",
    "                    H_3.append(torch.einsum('abcd,bd->ac',Cx*1/Df[ind_H3[i][0]]**2*HxH[ind_H3[i][0]],\n",
    "                                            H_2[ind_H3[i][1]]))\n",
    "                W_HK.append(H_3)\n",
    "                if k_sob == 4:\n",
    "                    H_4 = []\n",
    "                    ind_H4 = [[0,0],[0,1],[0,2],[0,3],[0,4],[1,0],[1,3],[1,4]]\n",
    "                    for i in range(len(ind_H4)):\n",
    "                        H_4.append(torch.einsum('abcd,bd->ac',Cx*1/Df[ind_H4[i][0]]**2*HxH[ind_H4[i][0]],\n",
    "                                                H_3[ind_H4[i][1]]))\n",
    "                    W_HK.append(H_4)\n",
    "        scaled_grid  = np.concatenate(([ord_sc_grid[:,0]*Df[0]+B[0]],[ord_sc_grid[:,1]*Df[1]+B[1]]),axis=0)\n",
    "    return scaled_grid, ord_sc_weights, W_HK\n",
    "def Hk_coeff_bdy(deg, k_sob, n_bdy):\n",
    "    Df = (n_bdy[1] - n_bdy[0])/2 \n",
    "    B =  (n_bdy[1] + n_bdy[0])/2 \n",
    "    mi = mp.multi_index.MultiIndex.from_degree(spatial_dimension=1, poly_degree=deg, lp_degree=np.inf)\n",
    "    ord_grid =mp.grid.Grid(mi).unisolvent_nodes.T[0]\n",
    "    un_grid , un_weights = np.polynomial.legendre.leggauss(deg+1)\n",
    "    ord_index = [[ i for i,j in enumerate(un_grid) if j == ord_grid[k]][0]\n",
    "                 for k in range(len(ord_grid))]\n",
    "    ord_sc_weights = Df**2*un_weights[ord_index]\n",
    "    ord_sc_grid = un_grid[ord_index]\n",
    "    weights = []\n",
    "    if k_sob == 1 or k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "        coeff_L_x = []\n",
    "        for i in range(len(mi)):\n",
    "            alpha = np.zeros(len(mi))\n",
    "            alpha[i] = 1 \n",
    "            lag_poly = mp.LagrangePolynomial(alpha, mi)\n",
    "            derivator_lag_poly = mp.Derivator(lag_poly, mp.LagrangePolynomial)\n",
    "            d_c = derivator_lag_poly.get_gradient_poly().coeffs\n",
    "            coeff_L_x.append(d_c[:,0])\n",
    "        h_w_x = torch.einsum('ab,b->ab',\n",
    "                 torch.tensor(coeff_L_x),torch.Tensor(ord_sc_weights))\n",
    "        Cx = 1/(deg**2)\n",
    "        H_1 = torch.einsum('ab,cb->ac',torch.tensor(coeff_L_x), h_w_x)\n",
    "        weights.append(Cx*1/(Df**2)*H_1)\n",
    "        if k_sob == 2 or k_sob ==3 or k_sob==4:\n",
    "            HxH_x=torch.einsum('ab,cd->abcd',torch.tensor(coeff_L_x),torch.tensor(coeff_L_x)) \n",
    "            H_2 = torch.einsum('abcd,bd->ac',Cx*1/(Df**2)*HxH_x,H_1)\n",
    "            weights.append(H_2)\n",
    "            if  k_sob == 3 or k_sob == 4:\n",
    "                H_3 = torch.einsum('abcd,bd->ac',Cx*1/(Df**2)*HxH_x,H_2)\n",
    "                weights.append(H_3)\n",
    "                if k_sob == 4:\n",
    "                    H_4 = torch.einsum('abcd,bd->ac',Cx*1/(Df**2)*HxH_x,H_3)\n",
    "                    weights.append(H_4)\n",
    "    scaled_grid = ord_sc_grid*Df + B\n",
    "    return ord_sc_grid, ord_sc_weights, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Domain bounds\n",
    "    #Create Datasets for the different Losses\n",
    "    def schroedinger1d(x, u):\n",
    "        global lam\n",
    "        omega = 1\n",
    "        pred = u\n",
    "        u = pred[:, 0]\n",
    "        #v = pred[:, 1]\n",
    "        grads = ones(u.shape, device=pred.device)  # move to the same device as prediction\n",
    "        grad_u = grad(u, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v = grad(v, x, create_graph=True, grad_outputs=grads)[0]\n",
    "\n",
    "        # calculate first order derivatives\n",
    "        u_x = grad_u[:, 0]\n",
    "        u_y = grad_u[:, 1]\n",
    "        #u_t = grad_u[:, 2]\n",
    "        # v_x = grad_v[:, 0]\n",
    "        #v_y = grad_v[:, 1]\n",
    "        #v_t = grad_v[:, 2]\n",
    "        # calculate second order derivatives\n",
    "        grad_u_x = grad(u_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v_x = grad(v_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        u_xx = grad_u_x[:, 0]\n",
    "        u_yy = grad_u_x[:, 1]\n",
    "        #v_xx = grad_v_x[:, 0]\n",
    "        #v_yy = grad_v_x[:, 1]\n",
    "        f_r = -1/2*(u_xx + u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u - lam*u\n",
    "        #print(torch.mean((-1/2*(u_xx - u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u)/u),torch.std((-1/2*(u_xx - u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u)/u))\n",
    "        #f_u = -1 * u_t - 0.5 * v_xx - 0.5 * v_yy + omega * 0.5 * (x[:, 0] ** 2) * v + omega * 0.5 * (x[:, 1] ** 2) * v\n",
    "        # fv is the imaginary part of the schrodinger equation\n",
    "        #f_v = -1 * v_t + 0.5 * u_xx + 0.5 * u_yy - omega * 0.5 * (x[:, 0] ** 2) * u - omega * 0.5 * (x[:, 1] ** 2) * u\n",
    "        return f_r#stack([f_u, f_v], 1)  # concatenate real part and imaginary part\n",
    "    def res_left(x, u):\n",
    "        omega = 1\n",
    "        pred = u\n",
    "        u = pred#pred[:, 0]\n",
    "        f_left =  lam*u\n",
    "        return f_left\n",
    "    def res_right(x, u):\n",
    "        omega = 1\n",
    "        pred = u\n",
    "        u = pred[:, 0]\n",
    "        #v = pred[:, 1]\n",
    "        grads = ones(u.shape, device=pred.device)  # move to the same device as prediction\n",
    "        grad_u = grad(u, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v = grad(v, x, create_graph=True, grad_outputs=grads)[0]\n",
    "\n",
    "        # calculate first order derivatives\n",
    "        u_x = grad_u[:, 0]\n",
    "        u_y = grad_u[:, 1]\n",
    "        #u_t = grad_u[:, 2]\n",
    "        #v_x = grad_v[:, 0]\n",
    "        #v_y = grad_v[:, 1]\n",
    "        #v_t = grad_v[:, 2]\n",
    "        # calculate second order derivatives\n",
    "        grad_u_x = grad(u_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        #grad_v_x = grad(v_x, x, create_graph=True, grad_outputs=grads)[0]\n",
    "        u_xx = grad_u_x[:, 0]\n",
    "        u_yy = grad_u_x[:, 1]\n",
    "        #v_xx = grad_v_x[:, 0]\n",
    "        #v_yy = grad_v_x[:, 1]\n",
    "        f_right = -1/2*(u_xx + u_yy) + 1/2*(x[:, 0] ** 2 + x[:, 1] ** 2)*u       \n",
    "        return f_right\n",
    "    def hom_dir(x):\n",
    "        P = Psi(x[:,0], x[:,1])\n",
    "        #P_r = P.real\n",
    "        #P_im = P.imag\n",
    "        return P# torch.stack((P_r,P_im),1)\n",
    "    def Dataset_loss (Norm, bounds , n_points, deg = 1, k_res=0, k_ini =0, k_bdy = 0):\n",
    "        [lb, ub] = bounds\n",
    "        #residual_bdy = [[lb[0], lb[1], lb[2]], [ub[0], ub[1], ub[2]]]\n",
    "        #initial_bdy = [[lb[0], lb[1], lb[2]], [ub[0], ub[1], lb[2]]]\n",
    "        #boundary_bdy = [[[lb[0], lb[1], lb[2]], [lb[0], ub[1], ub[2]]], [[ub[0], lb[1], lb[2]], [ub[0], ub[1], ub[2]]],\n",
    "        #                [[lb[0], lb[1], lb[2]], [ub[0], lb[1], ub[2]]],\n",
    "        #                [[lb[0], ub[1], lb[2]], [ub[0], ub[1], ub[2]]]]\n",
    "        residual_bdy = [[lb[0], lb[1]], [ub[0], ub[1]]]\n",
    "        initial_bdy = [[lb[0], lb[1]], [ub[0], ub[1]]]\n",
    "        boundary_bdy = [[[lb[0], lb[1]], [lb[0], ub[1]]], [[ub[0], lb[1]], [ub[0], ub[1]]],\n",
    "                        [[lb[0], lb[1]], [ub[0], lb[1]]],\n",
    "                        [[lb[0], ub[1]], [ub[0], ub[1]]]]\n",
    "        if Norm == 'Mse' or Norm == 'Wass':\n",
    "            boundary_set = np.concatenate([MSE_set(boundary_bdy[i], nb= int(n_points/4),\n",
    "                                                       x_dim=1000, y_dim=1000, nsteps=1, dt=0.1) for i in range(4)], axis=0)\n",
    "            #boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            residual_set = MSE_set(residual_bdy, nb= n_points, x_dim=1000, y_dim=1000, nsteps=1, dt=0.1).T\n",
    "            #RS_1d = set_gen_1d(deg, residual_bdy)\n",
    "            #residual_set = RS_1d[0].T\n",
    "            initial_set = []\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],[[0],[0],[0]]]\n",
    "        elif Norm == 'Quad':\n",
    "            I_C_1d = set_gen_1d(deg, initial_bdy)\n",
    "            initial_set = I_C_1d[0].T\n",
    "            initial_weights = I_C_1d[1]\n",
    "            boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            boundary_weights = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[1] for i in range(4)], axis=0)\n",
    "            #boundary_set, boundary_weights, Hb_x, Hb_y,  Hb_xx, Hb_xy, Hb_yy = H1_coeff_bdy(deg)\n",
    "            RS_1d = set_gen_1d(deg, residual_bdy)\n",
    "            residual_set = RS_1d[0].T\n",
    "            residual_weights = RS_1d[1]\n",
    "            #Hb_x, Hb_y,Hb_xx, Hb_xy, Hb_yy = [[],[],[],[],[]]\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],[boundary_weights, residual_weights, initial_weights]]\n",
    "        elif Norm == 'Sobolev_1_old':\n",
    "            n_bdy = [[-1,-1],[1,1]]\n",
    "            initial_set, initial_weights, Hkw_ini = Hk_coeff(deg, k_ini, n_bdy)\n",
    "            residual_set, res_weights , Hkw_res = Hk_coeff(deg, k_res, n_bdy)\n",
    "            #Create Boundary Set\n",
    "            boundary_set, boundary_weights, Hkw_bdy = Hk_coeff_bdy(int(deg*deg/4), k_bdy, n_bdy)\n",
    "            #boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            #boundary_weights = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[1] for i in range(4)], axis=0)\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],\n",
    "                        [[boundary_weights, Hkw_bdy], \n",
    "                         [res_weights , Hkw_res],\n",
    "                         [initial_weights, Hkw_ini]]]\n",
    "            ### CREATE THE LOSS FUNCTIONS\n",
    "        elif Norm == 'Sobolev_1':\n",
    "            ## Crete Loss functions\n",
    "        #Boundary term#initial_set, initial_weights, Hkw_ini = Hk_coeff(deg,2)\n",
    "            #residual_set, res_weights , Hkw_res = Hk_coeff(deg,0)\n",
    "            #Create Boundary Set\n",
    "            part_bdy = 1\n",
    "            part_res = 3\n",
    "            deg_bdy = DEG\n",
    "            deg_res = DEG\n",
    "            deg_ini = DEG\n",
    "            dx_bdy = abs(lb[0]-ub[0])/part_bdy\n",
    "            dy_bdy = abs(lb[1]-ub[1])/part_bdy\n",
    "            dx_res = abs(lb[0]-ub[0])/part_res\n",
    "            dy_res = abs(lb[1]-ub[1])/part_res\n",
    "            def part_dom_bdy(lb, dx_bdy, part_bdy, deg, k_der):\n",
    "                W_k = []\n",
    "                Gr = []\n",
    "                W_2 = []\n",
    "                for i in range(part_bdy):\n",
    "                    n_bdy = [lb+i*dx_bdy,lb+(i+1)*dx_bdy]\n",
    "                    weights, ord_sc_grid, ord_sc_weights = Hk_coeff_bdy(deg, k_der, n_bdy)\n",
    "                    W_k.append(weights)\n",
    "                    Gr.append(ord_sc_grid)\n",
    "                    W_2.append(ord_sc_weights)\n",
    "                return W_k, Gr, W_2\n",
    "            def part_dom(lb, dx_bdy, dy_bdy, part_bdy, deg, k_der):\n",
    "                W_k = []\n",
    "                Gr_x = []\n",
    "                Gr_y = []\n",
    "                Gr_z = []\n",
    "                W_2= []\n",
    "                for i in range(part_bdy):\n",
    "                    for j in range(part_bdy):\n",
    "                        n_bdy = [[lb[0]+i*dx_bdy, lb[1]+j*dy_bdy],[lb[0]+(i+1)*dx_bdy,lb[1]+(j+1)*dy_bdy]]\n",
    "                        ord_sc_grid, ord_sc_weights, Hkw_res = Hk_coeff(deg, k_der, n_bdy)\n",
    "                        W_k.append(Hkw_res)\n",
    "                        Gr_x.append(ord_sc_grid[0])\n",
    "                        Gr_y.append(ord_sc_grid[1])\n",
    "                        #Gr_z.append(ord_sc_grid[2])\n",
    "                        W_2.append(ord_sc_weights)\n",
    "                grid_x = np.concatenate(np.array(Gr_x))\n",
    "                grid_y = np.concatenate(np.array(Gr_y))\n",
    "                #grid_z = np.concatenate(np.array(Gr_z))\n",
    "                Gr = np.array(list(zip(grid_x,grid_y)))\n",
    "                return np.array(W_k), np.array(W_2), Gr.T\n",
    "            #Wb_k_x, Grb_x, Wb_2_x = part_dom_bdy(lb[0], dx_bdy, part_bdy, deg_bdy, 1)\n",
    "            #Wb_k_y, Grb_y, Wb_2_y = part_dom_bdy(lb[1], dy_bdy, part_bdy, deg_bdy, 1)\n",
    "            #boundary_set = [Grb_x, Grb_y]\n",
    "            #Wr_k, residual_set, Wr_2, deg = part_dom(lb, dx_res, dy_res, part_res, deg_res, 0)\n",
    "            # Residual Loss\n",
    "            Wr_k, Wr_2, residual_set = part_dom(lb, dx_res, dy_res, part_res, deg_res, k_res)\n",
    "            Wr_2 = np.concatenate(Wr_2)\n",
    "            #Initial condition Loss\n",
    "            Wi_k, Wi_2, initial_set= part_dom(lb, dx_res, dy_res, part_res, deg_ini, k_ini)\n",
    "            Wi_2 = np.concatenate(Wi_2)\n",
    "            #boundary_set = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[0] for i in range(4)], axis=0)\n",
    "            #boundary_weights = np.concatenate([set_gen_1d(deg, boundary_bdy[i])[1] for i in range(4)], axis=0)\n",
    "            n_bdy = [-2,2]\n",
    "            boundary_set, boundary_weights, Hkw_bdy = Hk_coeff_bdy(deg_bdy, k_bdy, n_bdy)\n",
    "            #initial_set, ini_weights, Hkw_ini = Hk_coeff(deg,1)\n",
    "            Datasets = [[boundary_set, residual_set, initial_set],\n",
    "                        [[boundary_weights, Hkw_bdy], \n",
    "                         [Wr_k, Wr_2],\n",
    "                         [Wi_k, Wi_2]]]#[Wi_k, Wi_2 ,deg]]]ยก\n",
    "            \n",
    "        else:\n",
    "            raise(ValueError('Loss not defined'))\n",
    "        \n",
    "            ### CREATE THE LOSS FUNCTIONS\n",
    "        bc_dataset = BoundaryConditionDataset(Datasets[0][0])\n",
    "        dirichlet_bc = pf.DirichletBC(func=hom_dir, dataset=bc_dataset,\n",
    "                                      quad_weights=Datasets[1][0],sob_weights=Datasets[1][0]\n",
    "                                      ,name='Dirichlet BC', \n",
    "                                      norm=Norm)\n",
    "        # Residual Terms\n",
    "        pde_dataset = PDEDataset(Datasets[0][1])\n",
    "        pde_loss = pf.PDELoss(pde_dataset, schroedinger1d,\n",
    "                              func_left = res_left, func_right = res_right,\n",
    "                              quad_weights=Datasets[1][1],sob_weights=Datasets[1][1], norm = Norm)\n",
    "        #print(np.array(Datasets[0][2]).shape)\n",
    "        #Initial Condition Term\n",
    "        ic_dataset = InitialConditionDataset(Datasets[0][2], norm=Norm, n_bdy=initial_bdy, x_dim=1000, y_dim=1000, nx=n_points)\n",
    "        initial_condition = pf.InitialCondition(ic_dataset,  quad_weights=Datasets[1][2], \n",
    "                                                norm=Norm, sob_weights=Datasets[1][2])\n",
    "        #test_loss = tl.My_Loss(ic_dataset,  quad_weights=Datasets[1][2], norm=Norm)\n",
    "        return [dirichlet_bc, pde_loss, initial_condition], [bc_dataset, pde_dataset, ic_dataset], Datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quad\n",
      "Mse\n",
      "[[-2.0, -2.0], [-0.6666666666666667, -0.6666666666666667]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suarez08/PhD_PINNs/minterpy/transformation_operators.py:37: UserWarning: building a full transformation matrix from a barycentric transformation. this is inefficient.\n",
      "  warn('building a full transformation matrix from a barycentric transformation. this is inefficient.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.0, -0.6666666666666667], [-0.6666666666666667, 0.6666666666666665]]\n",
      "[[-2.0, 0.6666666666666665], [-0.6666666666666667, 2.0]]\n",
      "[[-0.6666666666666667, -2.0], [0.6666666666666665, -0.6666666666666667]]\n",
      "[[-0.6666666666666667, -0.6666666666666667], [0.6666666666666665, 0.6666666666666665]]\n",
      "[[-0.6666666666666667, 0.6666666666666665], [0.6666666666666665, 2.0]]\n",
      "[[0.6666666666666665, -2.0], [2.0, -0.6666666666666667]]\n",
      "[[0.6666666666666665, -0.6666666666666667], [2.0, 0.6666666666666665]]\n",
      "[[0.6666666666666665, 0.6666666666666665], [2.0, 2.0]]\n",
      "[[-2.0, -2.0], [-0.6666666666666667, -0.6666666666666667]]\n",
      "[[-2.0, -0.6666666666666667], [-0.6666666666666667, 0.6666666666666665]]\n",
      "[[-2.0, 0.6666666666666665], [-0.6666666666666667, 2.0]]\n",
      "[[-0.6666666666666667, -2.0], [0.6666666666666665, -0.6666666666666667]]\n",
      "[[-0.6666666666666667, -0.6666666666666667], [0.6666666666666665, 0.6666666666666665]]\n",
      "[[-0.6666666666666667, 0.6666666666666665], [0.6666666666666665, 2.0]]\n",
      "[[0.6666666666666665, -2.0], [2.0, -0.6666666666666667]]\n",
      "[[0.6666666666666665, -0.6666666666666667], [2.0, 0.6666666666666665]]\n",
      "[[0.6666666666666665, 0.6666666666666665], [2.0, 2.0]]\n",
      "Sobolev_1\n",
      "Measure Loss tensor(0.0725, grad_fn=<AddBackward0>)\n",
      "tensor(1.1449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.14489005850893 Epoch 0 from 2000\n",
      "Measure Loss tensor(0.0708, grad_fn=<AddBackward0>)\n",
      "tensor(0.3899, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.3898589325436657 Epoch 1 from 2000\n",
      "Measure Loss tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "tensor(0.5803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.5802651833667374 Epoch 2 from 2000\n",
      "Measure Loss tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "tensor(0.3650, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.3649731130219394 Epoch 3 from 2000\n",
      "Measure Loss tensor(0.0647, grad_fn=<AddBackward0>)\n",
      "tensor(0.2318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.23182151907152154 Epoch 4 from 2000\n",
      "Measure Loss tensor(0.0568, grad_fn=<AddBackward0>)\n",
      "tensor(0.3262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.3262380600275444 Epoch 5 from 2000\n",
      "Measure Loss tensor(0.0529, grad_fn=<AddBackward0>)\n",
      "tensor(0.3696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.36955920895188493 Epoch 6 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(0.2938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.29384889929222174 Epoch 7 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.2324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.23244482891372129 Epoch 8 from 2000\n",
      "Measure Loss tensor(0.0558, grad_fn=<AddBackward0>)\n",
      "tensor(0.2237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.22374944027159238 Epoch 9 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(0.2488, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.24880100760580925 Epoch 10 from 2000\n",
      "Measure Loss tensor(0.0624, grad_fn=<AddBackward0>)\n",
      "tensor(0.2693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2693338693618785 Epoch 11 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.2604, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.26038184302660994 Epoch 12 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(0.2374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.23742164820734413 Epoch 13 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.2172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2172083357186075 Epoch 14 from 2000\n",
      "Measure Loss tensor(0.0469, grad_fn=<AddBackward0>)\n",
      "tensor(0.2047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2046554597124081 Epoch 15 from 2000\n",
      "Measure Loss tensor(0.0429, grad_fn=<AddBackward0>)\n",
      "tensor(0.2070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.20702025939878413 Epoch 16 from 2000\n",
      "Measure Loss tensor(0.0415, grad_fn=<AddBackward0>)\n",
      "tensor(0.2133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2133472590455469 Epoch 17 from 2000\n",
      "Measure Loss tensor(0.0423, grad_fn=<AddBackward0>)\n",
      "tensor(0.2141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.21414215082991944 Epoch 18 from 2000\n",
      "Measure Loss tensor(0.0443, grad_fn=<AddBackward0>)\n",
      "tensor(0.2157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.21573903715774995 Epoch 19 from 2000\n",
      "Measure Loss tensor(0.0457, grad_fn=<AddBackward0>)\n",
      "tensor(0.2145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2144752045579767 Epoch 20 from 2000\n",
      "Measure Loss tensor(0.0463, grad_fn=<AddBackward0>)\n",
      "tensor(0.2133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2133039726328468 Epoch 21 from 2000\n",
      "Measure Loss tensor(0.0462, grad_fn=<AddBackward0>)\n",
      "tensor(0.2100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2100273125259599 Epoch 22 from 2000\n",
      "Measure Loss tensor(0.0454, grad_fn=<AddBackward0>)\n",
      "tensor(0.2019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.2018520759111526 Epoch 23 from 2000\n",
      "Measure Loss tensor(0.0440, grad_fn=<AddBackward0>)\n",
      "tensor(0.1960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19596187875069093 Epoch 24 from 2000\n",
      "Measure Loss tensor(0.0420, grad_fn=<AddBackward0>)\n",
      "tensor(0.1935, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19347629965448093 Epoch 25 from 2000\n",
      "Measure Loss tensor(0.0404, grad_fn=<AddBackward0>)\n",
      "tensor(0.1951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19510735712353783 Epoch 26 from 2000\n",
      "Measure Loss tensor(0.0396, grad_fn=<AddBackward0>)\n",
      "tensor(0.1978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19782640206105015 Epoch 27 from 2000\n",
      "Measure Loss tensor(0.0399, grad_fn=<AddBackward0>)\n",
      "tensor(0.1971, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19711400414825486 Epoch 28 from 2000\n",
      "Measure Loss tensor(0.0406, grad_fn=<AddBackward0>)\n",
      "tensor(0.1954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19540082647941998 Epoch 29 from 2000\n",
      "Measure Loss tensor(0.0412, grad_fn=<AddBackward0>)\n",
      "tensor(0.1925, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.19249878467904663 Epoch 30 from 2000\n",
      "Measure Loss tensor(0.0413, grad_fn=<AddBackward0>)\n",
      "tensor(0.1905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.190476518973828 Epoch 31 from 2000\n",
      "Measure Loss tensor(0.0412, grad_fn=<AddBackward0>)\n",
      "tensor(0.1887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.18866932681789494 Epoch 32 from 2000\n",
      "Measure Loss tensor(0.0407, grad_fn=<AddBackward0>)\n",
      "tensor(0.1857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.18568167201905159 Epoch 33 from 2000\n",
      "Measure Loss tensor(0.0397, grad_fn=<AddBackward0>)\n",
      "tensor(0.1834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.18337677597363164 Epoch 34 from 2000\n",
      "Measure Loss tensor(0.0382, grad_fn=<AddBackward0>)\n",
      "tensor(0.1818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.1817576395481309 Epoch 35 from 2000\n",
      "Measure Loss tensor(0.0367, grad_fn=<AddBackward0>)\n",
      "tensor(0.1819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.18191403954377605 Epoch 36 from 2000\n",
      "Measure Loss tensor(0.0357, grad_fn=<AddBackward0>)\n",
      "tensor(0.1821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.18206835586006792 Epoch 37 from 2000\n",
      "Measure Loss tensor(0.0354, grad_fn=<AddBackward0>)\n",
      "tensor(0.1809, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.18085137052415434 Epoch 38 from 2000\n",
      "Measure Loss tensor(0.0352, grad_fn=<AddBackward0>)\n",
      "tensor(0.1786, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.17857578593384632 Epoch 39 from 2000\n",
      "Measure Loss tensor(0.0350, grad_fn=<AddBackward0>)\n",
      "tensor(0.1756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.1756486062412728 Epoch 40 from 2000\n",
      "Measure Loss tensor(0.0348, grad_fn=<AddBackward0>)\n",
      "tensor(0.1735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.17353076799541356 Epoch 41 from 2000\n",
      "Measure Loss tensor(0.0345, grad_fn=<AddBackward0>)\n",
      "tensor(0.1713, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.17134061513426707 Epoch 42 from 2000\n",
      "Measure Loss tensor(0.0337, grad_fn=<AddBackward0>)\n",
      "tensor(0.1695, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.1694501230487769 Epoch 43 from 2000\n",
      "Measure Loss tensor(0.0325, grad_fn=<AddBackward0>)\n",
      "tensor(0.1672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.16723674578567468 Epoch 44 from 2000\n",
      "Measure Loss tensor(0.0312, grad_fn=<AddBackward0>)\n",
      "tensor(0.1652, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.16518731114495402 Epoch 45 from 2000\n",
      "Measure Loss tensor(0.0299, grad_fn=<AddBackward0>)\n",
      "tensor(0.1623, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.162264676796513 Epoch 46 from 2000\n",
      "Measure Loss tensor(0.0287, grad_fn=<AddBackward0>)\n",
      "tensor(0.1588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.15881675822346591 Epoch 47 from 2000\n",
      "Measure Loss tensor(0.0277, grad_fn=<AddBackward0>)\n",
      "tensor(0.1552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.1552308394341456 Epoch 48 from 2000\n",
      "Measure Loss tensor(0.0268, grad_fn=<AddBackward0>)\n",
      "tensor(0.1526, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.15262487087175378 Epoch 49 from 2000\n",
      "Measure Loss tensor(0.0259, grad_fn=<AddBackward0>)\n",
      "tensor(0.1495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.14952659804530066 Epoch 50 from 2000\n",
      "Measure Loss tensor(0.0251, grad_fn=<AddBackward0>)\n",
      "tensor(0.1449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.14494307368972947 Epoch 51 from 2000\n",
      "Measure Loss tensor(0.0247, grad_fn=<AddBackward0>)\n",
      "tensor(0.1399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.1399350363904041 Epoch 52 from 2000\n",
      "Measure Loss tensor(0.0242, grad_fn=<AddBackward0>)\n",
      "tensor(0.1358, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.13578336473607128 Epoch 53 from 2000\n",
      "Measure Loss tensor(0.0239, grad_fn=<AddBackward0>)\n",
      "tensor(0.1312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.13116211407444506 Epoch 54 from 2000\n",
      "Measure Loss tensor(0.0237, grad_fn=<AddBackward0>)\n",
      "tensor(0.1247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.12468774719107607 Epoch 55 from 2000\n",
      "Measure Loss tensor(0.0228, grad_fn=<AddBackward0>)\n",
      "tensor(0.1186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.11863614557698238 Epoch 56 from 2000\n",
      "Measure Loss tensor(0.0235, grad_fn=<AddBackward0>)\n",
      "tensor(0.1126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.11257817131970779 Epoch 57 from 2000\n",
      "Measure Loss tensor(0.0242, grad_fn=<AddBackward0>)\n",
      "tensor(0.1043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.10427891009448544 Epoch 58 from 2000\n",
      "Measure Loss tensor(0.0279, grad_fn=<AddBackward0>)\n",
      "tensor(0.0972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.09719672222437606 Epoch 59 from 2000\n",
      "Measure Loss tensor(0.0286, grad_fn=<AddBackward0>)\n",
      "tensor(0.0907, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.09066310718080617 Epoch 60 from 2000\n",
      "Measure Loss tensor(0.0391, grad_fn=<AddBackward0>)\n",
      "tensor(0.0970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.09704368601767492 Epoch 61 from 2000\n",
      "Measure Loss tensor(0.0304, grad_fn=<AddBackward0>)\n",
      "tensor(0.1657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.16572972949300566 Epoch 62 from 2000\n",
      "Measure Loss tensor(0.0686, grad_fn=<AddBackward0>)\n",
      "tensor(0.2100, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.20996427009901075 Epoch 63 from 2000\n",
      "Measure Loss tensor(0.0422, grad_fn=<AddBackward0>)\n",
      "tensor(0.0746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.07455302117873926 Epoch 64 from 2000\n",
      "Measure Loss tensor(0.0429, grad_fn=<AddBackward0>)\n",
      "tensor(0.1235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.12352705188883262 Epoch 65 from 2000\n",
      "Measure Loss tensor(0.0745, grad_fn=<AddBackward0>)\n",
      "tensor(0.1201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.12007011604981235 Epoch 66 from 2000\n",
      "Measure Loss tensor(0.0657, grad_fn=<AddBackward0>)\n",
      "tensor(0.0548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.05475911758106558 Epoch 67 from 2000\n",
      "Measure Loss tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.1222, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.12224117238970635 Epoch 68 from 2000\n",
      "Measure Loss tensor(0.0754, grad_fn=<AddBackward0>)\n",
      "tensor(0.0439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.04385115684689983 Epoch 69 from 2000\n",
      "Measure Loss tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "tensor(0.0923, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.09229491220267064 Epoch 70 from 2000\n",
      "Measure Loss tensor(0.0772, grad_fn=<AddBackward0>)\n",
      "tensor(0.0585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.05848711928282202 Epoch 71 from 2000\n",
      "Measure Loss tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.05221435363086398 Epoch 72 from 2000\n",
      "Measure Loss tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "tensor(0.0707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.07070488949468796 Epoch 73 from 2000\n",
      "Measure Loss tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "tensor(0.0263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.026301067321197084 Epoch 74 from 2000\n",
      "Measure Loss tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "tensor(0.0654, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.06539124433989887 Epoch 75 from 2000\n",
      "Measure Loss tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "tensor(0.0252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.025185433689537302 Epoch 76 from 2000\n",
      "Measure Loss tensor(0.1412, grad_fn=<AddBackward0>)\n",
      "tensor(0.0415, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.041527108224227664 Epoch 77 from 2000\n",
      "Measure Loss tensor(0.1297, grad_fn=<AddBackward0>)\n",
      "tensor(0.0396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.039591972600531905 Epoch 78 from 2000\n",
      "Measure Loss tensor(0.1444, grad_fn=<AddBackward0>)\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01715949680168079 Epoch 79 from 2000\n",
      "Measure Loss tensor(0.1685, grad_fn=<AddBackward0>)\n",
      "tensor(0.0433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.04334628537206871 Epoch 80 from 2000\n",
      "Measure Loss tensor(0.1567, grad_fn=<AddBackward0>)\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01724817443082665 Epoch 81 from 2000\n",
      "Measure Loss tensor(0.1627, grad_fn=<AddBackward0>)\n",
      "tensor(0.0239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.023928918915829586 Epoch 82 from 2000\n",
      "Measure Loss tensor(0.1889, grad_fn=<AddBackward0>)\n",
      "tensor(0.0310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.03101261615509777 Epoch 83 from 2000\n",
      "Measure Loss tensor(0.1823, grad_fn=<AddBackward0>)\n",
      "tensor(0.0104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01041973696092047 Epoch 84 from 2000\n",
      "Measure Loss tensor(0.1792, grad_fn=<AddBackward0>)\n",
      "tensor(0.0262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.02615708028023432 Epoch 85 from 2000\n",
      "Measure Loss tensor(0.2010, grad_fn=<AddBackward0>)\n",
      "tensor(0.0197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.019697758830296334 Epoch 86 from 2000\n",
      "Measure Loss tensor(0.2009, grad_fn=<AddBackward0>)\n",
      "tensor(0.0104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010406278754724278 Epoch 87 from 2000\n",
      "Measure Loss tensor(0.1926, grad_fn=<AddBackward0>)\n",
      "tensor(0.0232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.023245654092960547 Epoch 88 from 2000\n",
      "Measure Loss tensor(0.2065, grad_fn=<AddBackward0>)\n",
      "tensor(0.0134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.013358184629337624 Epoch 89 from 2000\n",
      "Measure Loss tensor(0.2095, grad_fn=<AddBackward0>)\n",
      "tensor(0.0117, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.011746311044685329 Epoch 90 from 2000\n",
      "Measure Loss tensor(0.2002, grad_fn=<AddBackward0>)\n",
      "tensor(0.0196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.019575623362203795 Epoch 91 from 2000\n",
      "Measure Loss tensor(0.2072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01010350503435457 Epoch 92 from 2000\n",
      "Measure Loss tensor(0.2097, grad_fn=<AddBackward0>)\n",
      "tensor(0.0124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01236036567987953 Epoch 93 from 2000\n",
      "Measure Loss tensor(0.2012, grad_fn=<AddBackward0>)\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.015545171802255054 Epoch 94 from 2000\n",
      "Measure Loss tensor(0.2040, grad_fn=<AddBackward0>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008096942684344004 Epoch 95 from 2000\n",
      "Measure Loss tensor(0.2054, grad_fn=<AddBackward0>)\n",
      "tensor(0.0124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.012378140983124872 Epoch 96 from 2000\n",
      "Measure Loss tensor(0.1972, grad_fn=<AddBackward0>)\n",
      "tensor(0.0118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.011763087469976074 Epoch 97 from 2000\n",
      "Measure Loss tensor(0.1982, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007687331051711537 Epoch 98 from 2000\n",
      "Measure Loss tensor(0.1997, grad_fn=<AddBackward0>)\n",
      "tensor(0.0118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.011815629339882115 Epoch 99 from 2000\n",
      "Measure Loss tensor(0.1916, grad_fn=<AddBackward0>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009008255224686396 Epoch 100 from 2000\n",
      "Measure Loss tensor(0.1913, grad_fn=<AddBackward0>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008130983342528441 Epoch 101 from 2000\n",
      "Measure Loss tensor(0.1940, grad_fn=<AddBackward0>)\n",
      "tensor(0.0105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01049833508582685 Epoch 102 from 2000\n",
      "Measure Loss tensor(0.1871, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0073866291595740775 Epoch 103 from 2000\n",
      "Measure Loss tensor(0.1849, grad_fn=<AddBackward0>)\n",
      "tensor(0.0086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008617330412430783 Epoch 104 from 2000\n",
      "Measure Loss tensor(0.1887, grad_fn=<AddBackward0>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008904809059031023 Epoch 105 from 2000\n",
      "Measure Loss tensor(0.1847, grad_fn=<AddBackward0>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006927929549887509 Epoch 106 from 2000\n",
      "Measure Loss tensor(0.1809, grad_fn=<AddBackward0>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008656853806314091 Epoch 107 from 2000\n",
      "Measure Loss tensor(0.1845, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007495466238261326 Epoch 108 from 2000\n",
      "Measure Loss tensor(0.1837, grad_fn=<AddBackward0>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007053789929490276 Epoch 109 from 2000\n",
      "Measure Loss tensor(0.1798, grad_fn=<AddBackward0>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008080766582458387 Epoch 110 from 2000\n",
      "Measure Loss tensor(0.1822, grad_fn=<AddBackward0>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006711877558644542 Epoch 111 from 2000\n",
      "Measure Loss tensor(0.1836, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007178445974626731 Epoch 112 from 2000\n",
      "Measure Loss tensor(0.1807, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007347613054442958 Epoch 113 from 2000\n",
      "Measure Loss tensor(0.1821, grad_fn=<AddBackward0>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0063739261228220635 Epoch 114 from 2000\n",
      "Measure Loss tensor(0.1844, grad_fn=<AddBackward0>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007030619562557152 Epoch 115 from 2000\n",
      "Measure Loss tensor(0.1824, grad_fn=<AddBackward0>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006712117409789387 Epoch 116 from 2000\n",
      "Measure Loss tensor(0.1836, grad_fn=<AddBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006191451240035938 Epoch 117 from 2000\n",
      "Measure Loss tensor(0.1858, grad_fn=<AddBackward0>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0067364747031706555 Epoch 118 from 2000\n",
      "Measure Loss tensor(0.1843, grad_fn=<AddBackward0>)\n",
      "tensor(0.0063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00628600198244055 Epoch 119 from 2000\n",
      "Measure Loss tensor(0.1855, grad_fn=<AddBackward0>)\n",
      "tensor(0.0061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00605940296761927 Epoch 120 from 2000\n",
      "Measure Loss tensor(0.1875, grad_fn=<AddBackward0>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006433353757690527 Epoch 121 from 2000\n",
      "Measure Loss tensor(0.1859, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006038585789590465 Epoch 122 from 2000\n",
      "Measure Loss tensor(0.1869, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005916896540163229 Epoch 123 from 2000\n",
      "Measure Loss tensor(0.1889, grad_fn=<AddBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006184248322753059 Epoch 124 from 2000\n",
      "Measure Loss tensor(0.1869, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005898512091404476 Epoch 125 from 2000\n",
      "Measure Loss tensor(0.1876, grad_fn=<AddBackward0>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005814634108873581 Epoch 126 from 2000\n",
      "Measure Loss tensor(0.1890, grad_fn=<AddBackward0>)\n",
      "tensor(0.0061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006081600502061792 Epoch 127 from 2000\n",
      "Measure Loss tensor(0.1873, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00601622314920071 Epoch 128 from 2000\n",
      "Measure Loss tensor(0.1870, grad_fn=<AddBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00620900390328914 Epoch 129 from 2000\n",
      "Measure Loss tensor(0.1886, grad_fn=<AddBackward0>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007058321771239428 Epoch 130 from 2000\n",
      "Measure Loss tensor(0.1857, grad_fn=<AddBackward0>)\n",
      "tensor(0.0084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008429667568808641 Epoch 131 from 2000\n",
      "Measure Loss tensor(0.1865, grad_fn=<AddBackward0>)\n",
      "tensor(0.0110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01100744978423877 Epoch 132 from 2000\n",
      "Measure Loss tensor(0.1850, grad_fn=<AddBackward0>)\n",
      "tensor(0.0148, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.014814411718674498 Epoch 133 from 2000\n",
      "Measure Loss tensor(0.1841, grad_fn=<AddBackward0>)\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01630702679624129 Epoch 134 from 2000\n",
      "Measure Loss tensor(0.1806, grad_fn=<AddBackward0>)\n",
      "tensor(0.0126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.012601754576098466 Epoch 135 from 2000\n",
      "Measure Loss tensor(0.1802, grad_fn=<AddBackward0>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006861335409654743 Epoch 136 from 2000\n",
      "Measure Loss tensor(0.1766, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006046997449838995 Epoch 137 from 2000\n",
      "Measure Loss tensor(0.1751, grad_fn=<AddBackward0>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009482041080011489 Epoch 138 from 2000\n",
      "Measure Loss tensor(0.1755, grad_fn=<AddBackward0>)\n",
      "tensor(0.0103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010312380258669094 Epoch 139 from 2000\n",
      "Measure Loss tensor(0.1710, grad_fn=<AddBackward0>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006929259024144305 Epoch 140 from 2000\n",
      "Measure Loss tensor(0.1716, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005412912134911664 Epoch 141 from 2000\n",
      "Measure Loss tensor(0.1716, grad_fn=<AddBackward0>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007811167397333011 Epoch 142 from 2000\n",
      "Measure Loss tensor(0.1681, grad_fn=<AddBackward0>)\n",
      "tensor(0.0085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008458667699734737 Epoch 143 from 2000\n",
      "Measure Loss tensor(0.1702, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005908897153413394 Epoch 144 from 2000\n",
      "Measure Loss tensor(0.1697, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005350393032527483 Epoch 145 from 2000\n",
      "Measure Loss tensor(0.1678, grad_fn=<AddBackward0>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0071416578175125724 Epoch 146 from 2000\n",
      "Measure Loss tensor(0.1704, grad_fn=<AddBackward0>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007039021100625599 Epoch 147 from 2000\n",
      "Measure Loss tensor(0.1695, grad_fn=<AddBackward0>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00532498816397467 Epoch 148 from 2000\n",
      "Measure Loss tensor(0.1693, grad_fn=<AddBackward0>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005232784735553898 Epoch 149 from 2000\n",
      "Measure Loss tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006362088346391605 Epoch 150 from 2000\n",
      "Measure Loss tensor(0.1706, grad_fn=<AddBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006237876008230116 Epoch 151 from 2000\n",
      "Measure Loss tensor(0.1717, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005095076414888558 Epoch 152 from 2000\n",
      "Measure Loss tensor(0.1726, grad_fn=<AddBackward0>)\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004919097217220432 Epoch 153 from 2000\n",
      "Measure Loss tensor(0.1722, grad_fn=<AddBackward0>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005678665149619379 Epoch 154 from 2000\n",
      "Measure Loss tensor(0.1738, grad_fn=<AddBackward0>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005786466034857092 Epoch 155 from 2000\n",
      "Measure Loss tensor(0.1735, grad_fn=<AddBackward0>)\n",
      "tensor(0.0050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00503396931537913 Epoch 156 from 2000\n",
      "Measure Loss tensor(0.1737, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004653204874334391 Epoch 157 from 2000\n",
      "Measure Loss tensor(0.1744, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005068994052671857 Epoch 158 from 2000\n",
      "Measure Loss tensor(0.1739, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005392945570115176 Epoch 159 from 2000\n",
      "Measure Loss tensor(0.1745, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005055104118427952 Epoch 160 from 2000\n",
      "Measure Loss tensor(0.1738, grad_fn=<AddBackward0>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004594690474630096 Epoch 161 from 2000\n",
      "Measure Loss tensor(0.1739, grad_fn=<AddBackward0>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0045974172721211706 Epoch 162 from 2000\n",
      "Measure Loss tensor(0.1738, grad_fn=<AddBackward0>)\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004882068735253385 Epoch 163 from 2000\n",
      "Measure Loss tensor(0.1726, grad_fn=<AddBackward0>)\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00494668099766824 Epoch 164 from 2000\n",
      "Measure Loss tensor(0.1733, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004683966828131206 Epoch 165 from 2000\n",
      "Measure Loss tensor(0.1719, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00442766946200328 Epoch 166 from 2000\n",
      "Measure Loss tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0044362879513136 Epoch 167 from 2000\n",
      "Measure Loss tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0045957577143170185 Epoch 168 from 2000\n",
      "Measure Loss tensor(0.1700, grad_fn=<AddBackward0>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004628344179421297 Epoch 169 from 2000\n",
      "Measure Loss tensor(0.1704, grad_fn=<AddBackward0>)\n",
      "tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004458477173428448 Epoch 170 from 2000\n",
      "Measure Loss tensor(0.1692, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004279716392824207 Epoch 171 from 2000\n",
      "Measure Loss tensor(0.1686, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004263847736879545 Epoch 172 from 2000\n",
      "Measure Loss tensor(0.1687, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004355904699698843 Epoch 173 from 2000\n",
      "Measure Loss tensor(0.1673, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004384526835375466 Epoch 174 from 2000\n",
      "Measure Loss tensor(0.1676, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004291180090641508 Epoch 175 from 2000\n",
      "Measure Loss tensor(0.1668, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0041683034750830475 Epoch 176 from 2000\n",
      "Measure Loss tensor(0.1663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004118776936903306 Epoch 177 from 2000\n",
      "Measure Loss tensor(0.1664, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004145700066946887 Epoch 178 from 2000\n",
      "Measure Loss tensor(0.1655, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004176842541964773 Epoch 179 from 2000\n",
      "Measure Loss tensor(0.1657, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004152445987638495 Epoch 180 from 2000\n",
      "Measure Loss tensor(0.1651, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004077600361195221 Epoch 181 from 2000\n",
      "Measure Loss tensor(0.1650, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004003864186438037 Epoch 182 from 2000\n",
      "Measure Loss tensor(0.1648, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003972019305979564 Epoch 183 from 2000\n",
      "Measure Loss tensor(0.1644, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00397831090795637 Epoch 184 from 2000\n",
      "Measure Loss tensor(0.1645, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003987783629871738 Epoch 185 from 2000\n",
      "Measure Loss tensor(0.1640, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003970558904887138 Epoch 186 from 2000\n",
      "Measure Loss tensor(0.1641, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003926390299544521 Epoch 187 from 2000\n",
      "Measure Loss tensor(0.1636, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0038743210467046324 Epoch 188 from 2000\n",
      "Measure Loss tensor(0.1637, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0038338847405145556 Epoch 189 from 2000\n",
      "Measure Loss tensor(0.1634, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0038113924971855838 Epoch 190 from 2000\n",
      "Measure Loss tensor(0.1631, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0038022695156664397 Epoch 191 from 2000\n",
      "Measure Loss tensor(0.1631, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0037967966215772496 Epoch 192 from 2000\n",
      "Measure Loss tensor(0.1625, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003785051720122906 Epoch 193 from 2000\n",
      "Measure Loss tensor(0.1626, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0037627673865894638 Epoch 194 from 2000\n",
      "Measure Loss tensor(0.1620, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0037313465750933605 Epoch 195 from 2000\n",
      "Measure Loss tensor(0.1620, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003696995081219919 Epoch 196 from 2000\n",
      "Measure Loss tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036653245189147143 Epoch 197 from 2000\n",
      "Measure Loss tensor(0.1612, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003638474863320714 Epoch 198 from 2000\n",
      "Measure Loss tensor(0.1609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036164890030271016 Epoch 199 from 2000\n",
      "Measure Loss tensor(0.1604, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035985336000111164 Epoch 200 from 2000\n",
      "Measure Loss tensor(0.1603, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035834402507954072 Epoch 201 from 2000\n",
      "Measure Loss tensor(0.1597, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035694593843551303 Epoch 202 from 2000\n",
      "Measure Loss tensor(0.1596, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003555008508851149 Epoch 203 from 2000\n",
      "Measure Loss tensor(0.1590, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003539460455432288 Epoch 204 from 2000\n",
      "Measure Loss tensor(0.1589, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003523036803026842 Epoch 205 from 2000\n",
      "Measure Loss tensor(0.1582, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035061359595502696 Epoch 206 from 2000\n",
      "Measure Loss tensor(0.1582, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034888769344235946 Epoch 207 from 2000\n",
      "Measure Loss tensor(0.1575, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034725144489669946 Epoch 208 from 2000\n",
      "Measure Loss tensor(0.1576, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003457030543620224 Epoch 209 from 2000\n",
      "Measure Loss tensor(0.1568, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003443649960487568 Epoch 210 from 2000\n",
      "Measure Loss tensor(0.1570, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003432831016286793 Epoch 211 from 2000\n",
      "Measure Loss tensor(0.1562, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034276899435846104 Epoch 212 from 2000\n",
      "Measure Loss tensor(0.1564, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034296971723920718 Epoch 213 from 2000\n",
      "Measure Loss tensor(0.1555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003445001288614972 Epoch 214 from 2000\n",
      "Measure Loss tensor(0.1559, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034807136119431367 Epoch 215 from 2000\n",
      "Measure Loss tensor(0.1547, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035535169987093527 Epoch 216 from 2000\n",
      "Measure Loss tensor(0.1556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036876540729340814 Epoch 217 from 2000\n",
      "Measure Loss tensor(0.1538, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0039339527434259555 Epoch 218 from 2000\n",
      "Measure Loss tensor(0.1554, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004370985145788121 Epoch 219 from 2000\n",
      "Measure Loss tensor(0.1525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005161224211986258 Epoch 220 from 2000\n",
      "Measure Loss tensor(0.1555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006507179961800639 Epoch 221 from 2000\n",
      "Measure Loss tensor(0.1505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00882062049643401 Epoch 222 from 2000\n",
      "Measure Loss tensor(0.1556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.012203536392415264 Epoch 223 from 2000\n",
      "Measure Loss tensor(0.1471, grad_fn=<AddBackward0>)\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01676320909853615 Epoch 224 from 2000\n",
      "Measure Loss tensor(0.1546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0201, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.020132451240905347 Epoch 225 from 2000\n",
      "Measure Loss tensor(0.1422, grad_fn=<AddBackward0>)\n",
      "tensor(0.0204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.020354947300007344 Epoch 226 from 2000\n",
      "Measure Loss tensor(0.1495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.014712577874951959 Epoch 227 from 2000\n",
      "Measure Loss tensor(0.1382, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007640817756790579 Epoch 228 from 2000\n",
      "Measure Loss tensor(0.1407, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004312281717850129 Epoch 229 from 2000\n",
      "Measure Loss tensor(0.1393, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006025685441295713 Epoch 230 from 2000\n",
      "Measure Loss tensor(0.1336, grad_fn=<AddBackward0>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009034156798941357 Epoch 231 from 2000\n",
      "Measure Loss tensor(0.1405, grad_fn=<AddBackward0>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009258148992293153 Epoch 232 from 2000\n",
      "Measure Loss tensor(0.1326, grad_fn=<AddBackward0>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00670565692979416 Epoch 233 from 2000\n",
      "Measure Loss tensor(0.1369, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0041844324446410055 Epoch 234 from 2000\n",
      "Measure Loss tensor(0.1376, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00436244019442729 Epoch 235 from 2000\n",
      "Measure Loss tensor(0.1340, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006452359242747322 Epoch 236 from 2000\n",
      "Measure Loss tensor(0.1418, grad_fn=<AddBackward0>)\n",
      "tensor(0.0072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0072092914562459955 Epoch 237 from 2000\n",
      "Measure Loss tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005438900105836216 Epoch 238 from 2000\n",
      "Measure Loss tensor(0.1413, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003312863293284792 Epoch 239 from 2000\n",
      "Measure Loss tensor(0.1427, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032812213562653787 Epoch 240 from 2000\n",
      "Measure Loss tensor(0.1407, grad_fn=<AddBackward0>)\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004883463136437296 Epoch 241 from 2000\n",
      "Measure Loss tensor(0.1476, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005919630751409357 Epoch 242 from 2000\n",
      "Measure Loss tensor(0.1426, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005362580699030955 Epoch 243 from 2000\n",
      "Measure Loss tensor(0.1481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0039797876934649385 Epoch 244 from 2000\n",
      "Measure Loss tensor(0.1461, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030807720777762935 Epoch 245 from 2000\n",
      "Measure Loss tensor(0.1465, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003102012149437269 Epoch 246 from 2000\n",
      "Measure Loss tensor(0.1490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036514299071014112 Epoch 247 from 2000\n",
      "Measure Loss tensor(0.1453, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004179189783159445 Epoch 248 from 2000\n",
      "Measure Loss tensor(0.1494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004377857534637779 Epoch 249 from 2000\n",
      "Measure Loss tensor(0.1448, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00420559276269674 Epoch 250 from 2000\n",
      "Measure Loss tensor(0.1474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003710267582067099 Epoch 251 from 2000\n",
      "Measure Loss tensor(0.1445, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031276317825849552 Epoch 252 from 2000\n",
      "Measure Loss tensor(0.1444, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027518332507480902 Epoch 253 from 2000\n",
      "Measure Loss tensor(0.1441, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002785029051372518 Epoch 254 from 2000\n",
      "Measure Loss tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031163841232721328 Epoch 255 from 2000\n",
      "Measure Loss tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003397990292926428 Epoch 256 from 2000\n",
      "Measure Loss tensor(0.1398, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033758295680672073 Epoch 257 from 2000\n",
      "Measure Loss tensor(0.1414, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030798239076962157 Epoch 258 from 2000\n",
      "Measure Loss tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002770107609386094 Epoch 259 from 2000\n",
      "Measure Loss tensor(0.1394, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002631279710889922 Epoch 260 from 2000\n",
      "Measure Loss tensor(0.1388, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002662363825648867 Epoch 261 from 2000\n",
      "Measure Loss tensor(0.1377, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027597539908623663 Epoch 262 from 2000\n",
      "Measure Loss tensor(0.1390, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002840522381157914 Epoch 263 from 2000\n",
      "Measure Loss tensor(0.1369, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002870227331427286 Epoch 264 from 2000\n",
      "Measure Loss tensor(0.1388, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0028256039449064824 Epoch 265 from 2000\n",
      "Measure Loss tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002709361963207928 Epoch 266 from 2000\n",
      "Measure Loss tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025664152108354687 Epoch 267 from 2000\n",
      "Measure Loss tensor(0.1377, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024690213935834003 Epoch 268 from 2000\n",
      "Measure Loss tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024550419113090623 Epoch 269 from 2000\n",
      "Measure Loss tensor(0.1385, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025042273825837107 Epoch 270 from 2000\n",
      "Measure Loss tensor(0.1374, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025649459837761334 Epoch 271 from 2000\n",
      "Measure Loss tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026008092861878425 Epoch 272 from 2000\n",
      "Measure Loss tensor(0.1373, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026086856466674066 Epoch 273 from 2000\n",
      "Measure Loss tensor(0.1388, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002601420071251551 Epoch 274 from 2000\n",
      "Measure Loss tensor(0.1371, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025872580311908622 Epoch 275 from 2000\n",
      "Measure Loss tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025619988052089414 Epoch 276 from 2000\n",
      "Measure Loss tensor(0.1368, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002523490492172366 Epoch 277 from 2000\n",
      "Measure Loss tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024755865110703897 Epoch 278 from 2000\n",
      "Measure Loss tensor(0.1363, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002429804389450568 Epoch 279 from 2000\n",
      "Measure Loss tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023942402504680874 Epoch 280 from 2000\n",
      "Measure Loss tensor(0.1356, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023687231927516598 Epoch 281 from 2000\n",
      "Measure Loss tensor(0.1362, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023476558725688307 Epoch 282 from 2000\n",
      "Measure Loss tensor(0.1348, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023261460905705713 Epoch 283 from 2000\n",
      "Measure Loss tensor(0.1353, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023046647658949452 Epoch 284 from 2000\n",
      "Measure Loss tensor(0.1341, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002286530263018714 Epoch 285 from 2000\n",
      "Measure Loss tensor(0.1345, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022753965442375676 Epoch 286 from 2000\n",
      "Measure Loss tensor(0.1334, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002271561177912099 Epoch 287 from 2000\n",
      "Measure Loss tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022725161588865272 Epoch 288 from 2000\n",
      "Measure Loss tensor(0.1326, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002279006826453588 Epoch 289 from 2000\n",
      "Measure Loss tensor(0.1334, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022944132720924106 Epoch 290 from 2000\n",
      "Measure Loss tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023280164295151214 Epoch 291 from 2000\n",
      "Measure Loss tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002392689414686652 Epoch 292 from 2000\n",
      "Measure Loss tensor(0.1311, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002510550016781379 Epoch 293 from 2000\n",
      "Measure Loss tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027153721446942156 Epoch 294 from 2000\n",
      "Measure Loss tensor(0.1300, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030768254031072464 Epoch 295 from 2000\n",
      "Measure Loss tensor(0.1336, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0037068144011049168 Epoch 296 from 2000\n",
      "Measure Loss tensor(0.1283, grad_fn=<AddBackward0>)\n",
      "tensor(0.0048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004838248598511886 Epoch 297 from 2000\n",
      "Measure Loss tensor(0.1345, grad_fn=<AddBackward0>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006797664150669043 Epoch 298 from 2000\n",
      "Measure Loss tensor(0.1255, grad_fn=<AddBackward0>)\n",
      "tensor(0.0103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010250338010311062 Epoch 299 from 2000\n",
      "Measure Loss tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "tensor(0.0157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.015671709959235423 Epoch 300 from 2000\n",
      "Measure Loss tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "tensor(0.0238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.02376030749904023 Epoch 301 from 2000\n",
      "Measure Loss tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "tensor(0.0317, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.031708521818547795 Epoch 302 from 2000\n",
      "Measure Loss tensor(0.1142, grad_fn=<AddBackward0>)\n",
      "tensor(0.0353, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.035338835629631644 Epoch 303 from 2000\n",
      "Measure Loss tensor(0.1279, grad_fn=<AddBackward0>)\n",
      "tensor(0.0259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.02593678404012012 Epoch 304 from 2000\n",
      "Measure Loss tensor(0.1106, grad_fn=<AddBackward0>)\n",
      "tensor(0.0099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00993994445845722 Epoch 305 from 2000\n",
      "Measure Loss tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027819064871076942 Epoch 306 from 2000\n",
      "Measure Loss tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "tensor(0.0103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010302517552192959 Epoch 307 from 2000\n",
      "Measure Loss tensor(0.1030, grad_fn=<AddBackward0>)\n",
      "tensor(0.0165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01653467660392553 Epoch 308 from 2000\n",
      "Measure Loss tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "tensor(0.0098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009754439380618114 Epoch 309 from 2000\n",
      "Measure Loss tensor(0.1072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030210620118132064 Epoch 310 from 2000\n",
      "Measure Loss tensor(0.1052, grad_fn=<AddBackward0>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00689493060117067 Epoch 311 from 2000\n",
      "Measure Loss tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.011236552359187876 Epoch 312 from 2000\n",
      "Measure Loss tensor(0.1079, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007369675268238092 Epoch 313 from 2000\n",
      "Measure Loss tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002537432762912471 Epoch 314 from 2000\n",
      "Measure Loss tensor(0.1181, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0043892715816296855 Epoch 315 from 2000\n",
      "Measure Loss tensor(0.1143, grad_fn=<AddBackward0>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008103754203200317 Epoch 316 from 2000\n",
      "Measure Loss tensor(0.1243, grad_fn=<AddBackward0>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0070535055653773516 Epoch 317 from 2000\n",
      "Measure Loss tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033079432416984965 Epoch 318 from 2000\n",
      "Measure Loss tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002118318424084169 Epoch 319 from 2000\n",
      "Measure Loss tensor(0.1287, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004276715581465912 Epoch 320 from 2000\n",
      "Measure Loss tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "tensor(0.0063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006309608084331519 Epoch 321 from 2000\n",
      "Measure Loss tensor(0.1314, grad_fn=<AddBackward0>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005628252912923659 Epoch 322 from 2000\n",
      "Measure Loss tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003350671072909909 Epoch 323 from 2000\n",
      "Measure Loss tensor(0.1280, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0020469371296540605 Epoch 324 from 2000\n",
      "Measure Loss tensor(0.1288, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027399810696582504 Epoch 325 from 2000\n",
      "Measure Loss tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00405578537104964 Epoch 326 from 2000\n",
      "Measure Loss tensor(0.1279, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004155685338720932 Epoch 327 from 2000\n",
      "Measure Loss tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002958240443967229 Epoch 328 from 2000\n",
      "Measure Loss tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019285928274957536 Epoch 329 from 2000\n",
      "Measure Loss tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021388227678861733 Epoch 330 from 2000\n",
      "Measure Loss tensor(0.1182, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002925980010855192 Epoch 331 from 2000\n",
      "Measure Loss tensor(0.1208, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0029886776185982893 Epoch 332 from 2000\n",
      "Measure Loss tensor(0.1168, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002259891367451336 Epoch 333 from 2000\n",
      "Measure Loss tensor(0.1172, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017970338762846234 Epoch 334 from 2000\n",
      "Measure Loss tensor(0.1177, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002122681409259231 Epoch 335 from 2000\n",
      "Measure Loss tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025453105268570552 Epoch 336 from 2000\n",
      "Measure Loss tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023555587824973497 Epoch 337 from 2000\n",
      "Measure Loss tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018615458169923959 Epoch 338 from 2000\n",
      "Measure Loss tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017457610343887349 Epoch 339 from 2000\n",
      "Measure Loss tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002036813335204211 Epoch 340 from 2000\n",
      "Measure Loss tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00220710889640893 Epoch 341 from 2000\n",
      "Measure Loss tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001998372337801678 Epoch 342 from 2000\n",
      "Measure Loss tensor(0.1177, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017017001483483735 Epoch 343 from 2000\n",
      "Measure Loss tensor(0.1182, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016593636274459293 Epoch 344 from 2000\n",
      "Measure Loss tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018393406692881159 Epoch 345 from 2000\n",
      "Measure Loss tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019625553284532324 Epoch 346 from 2000\n",
      "Measure Loss tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018760486272632194 Epoch 347 from 2000\n",
      "Measure Loss tensor(0.1185, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016907307263715596 Epoch 348 from 2000\n",
      "Measure Loss tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015960661182622573 Epoch 349 from 2000\n",
      "Measure Loss tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016470309392552663 Epoch 350 from 2000\n",
      "Measure Loss tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017406399465451452 Epoch 351 from 2000\n",
      "Measure Loss tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017555717432774904 Epoch 352 from 2000\n",
      "Measure Loss tensor(0.1168, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016749156039586898 Epoch 353 from 2000\n",
      "Measure Loss tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015765886745432484 Epoch 354 from 2000\n",
      "Measure Loss tensor(0.1163, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015390735503045183 Epoch 355 from 2000\n",
      "Measure Loss tensor(0.1155, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015662202605415158 Epoch 356 from 2000\n",
      "Measure Loss tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001604713472974472 Epoch 357 from 2000\n",
      "Measure Loss tensor(0.1143, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016039865217434314 Epoch 358 from 2000\n",
      "Measure Loss tensor(0.1148, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015603096761411534 Epoch 359 from 2000\n",
      "Measure Loss tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015094127064180867 Epoch 360 from 2000\n",
      "Measure Loss tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014851699201004632 Epoch 361 from 2000\n",
      "Measure Loss tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014926430872751696 Epoch 362 from 2000\n",
      "Measure Loss tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015100219534752966 Epoch 363 from 2000\n",
      "Measure Loss tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015110772302910373 Epoch 364 from 2000\n",
      "Measure Loss tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014879009734832058 Epoch 365 from 2000\n",
      "Measure Loss tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014538062192342332 Epoch 366 from 2000\n",
      "Measure Loss tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001428525950099715 Epoch 367 from 2000\n",
      "Measure Loss tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014222895472233103 Epoch 368 from 2000\n",
      "Measure Loss tensor(0.1132, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014286019594972094 Epoch 369 from 2000\n",
      "Measure Loss tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014336434351642385 Epoch 370 from 2000\n",
      "Measure Loss tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014270235039931538 Epoch 371 from 2000\n",
      "Measure Loss tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014091147244526606 Epoch 372 from 2000\n",
      "Measure Loss tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013876604691256417 Epoch 373 from 2000\n",
      "Measure Loss tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013710144989184077 Epoch 374 from 2000\n",
      "Measure Loss tensor(0.1121, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013622920908540241 Epoch 375 from 2000\n",
      "Measure Loss tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013595381899658345 Epoch 376 from 2000\n",
      "Measure Loss tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013577360879827856 Epoch 377 from 2000\n",
      "Measure Loss tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013533397284263861 Epoch 378 from 2000\n",
      "Measure Loss tensor(0.1108, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013450184668704723 Epoch 379 from 2000\n",
      "Measure Loss tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013334578261853206 Epoch 380 from 2000\n",
      "Measure Loss tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013205883592809654 Epoch 381 from 2000\n",
      "Measure Loss tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001308218026834558 Epoch 382 from 2000\n",
      "Measure Loss tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012980658168604106 Epoch 383 from 2000\n",
      "Measure Loss tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012904296931777323 Epoch 384 from 2000\n",
      "Measure Loss tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012848805727277194 Epoch 385 from 2000\n",
      "Measure Loss tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012800519323956164 Epoch 386 from 2000\n",
      "Measure Loss tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012745259759837464 Epoch 387 from 2000\n",
      "Measure Loss tensor(0.1086, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001267421654812818 Epoch 388 from 2000\n",
      "Measure Loss tensor(0.1087, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012588766299473232 Epoch 389 from 2000\n",
      "Measure Loss tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012495026149042438 Epoch 390 from 2000\n",
      "Measure Loss tensor(0.1084, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012403438939794606 Epoch 391 from 2000\n",
      "Measure Loss tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012310899816888972 Epoch 392 from 2000\n",
      "Measure Loss tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012226655716099195 Epoch 393 from 2000\n",
      "Measure Loss tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001214374956062861 Epoch 394 from 2000\n",
      "Measure Loss tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012069501803741518 Epoch 395 from 2000\n",
      "Measure Loss tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011999532209372216 Epoch 396 from 2000\n",
      "Measure Loss tensor(0.1072, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011931175354938871 Epoch 397 from 2000\n",
      "Measure Loss tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001186536442058546 Epoch 398 from 2000\n",
      "Measure Loss tensor(0.1068, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011798499953435612 Epoch 399 from 2000\n",
      "Measure Loss tensor(0.1067, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011734964690686496 Epoch 400 from 2000\n",
      "Measure Loss tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011667862830409279 Epoch 401 from 2000\n",
      "Measure Loss tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011603373257633237 Epoch 402 from 2000\n",
      "Measure Loss tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011538111449365938 Epoch 403 from 2000\n",
      "Measure Loss tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011476140009553941 Epoch 404 from 2000\n",
      "Measure Loss tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011413687177575277 Epoch 405 from 2000\n",
      "Measure Loss tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011355889288811784 Epoch 406 from 2000\n",
      "Measure Loss tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011302436835953335 Epoch 407 from 2000\n",
      "Measure Loss tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011255099508594379 Epoch 408 from 2000\n",
      "Measure Loss tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011219553368982896 Epoch 409 from 2000\n",
      "Measure Loss tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011197082639736568 Epoch 410 from 2000\n",
      "Measure Loss tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011198805355682904 Epoch 411 from 2000\n",
      "Measure Loss tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011237819199637782 Epoch 412 from 2000\n",
      "Measure Loss tensor(0.1036, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011333574832027857 Epoch 413 from 2000\n",
      "Measure Loss tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011520965793073165 Epoch 414 from 2000\n",
      "Measure Loss tensor(0.1030, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011861708591651512 Epoch 415 from 2000\n",
      "Measure Loss tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001245251677638839 Epoch 416 from 2000\n",
      "Measure Loss tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013473185596921974 Epoch 417 from 2000\n",
      "Measure Loss tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015207635260753374 Epoch 418 from 2000\n",
      "Measure Loss tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018214726660817073 Epoch 419 from 2000\n",
      "Measure Loss tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023330370819300804 Epoch 420 from 2000\n",
      "Measure Loss tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003227141108339409 Epoch 421 from 2000\n",
      "Measure Loss tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004727345346231953 Epoch 422 from 2000\n",
      "Measure Loss tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007299690529861119 Epoch 423 from 2000\n",
      "Measure Loss tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "tensor(0.0113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01127266164724334 Epoch 424 from 2000\n",
      "Measure Loss tensor(0.0938, grad_fn=<AddBackward0>)\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01724960914967843 Epoch 425 from 2000\n",
      "Measure Loss tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "tensor(0.0238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.02377762522003015 Epoch 426 from 2000\n",
      "Measure Loss tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "tensor(0.0287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.028661598729482863 Epoch 427 from 2000\n",
      "Measure Loss tensor(0.1007, grad_fn=<AddBackward0>)\n",
      "tensor(0.0251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.025112492771930908 Epoch 428 from 2000\n",
      "Measure Loss tensor(0.0847, grad_fn=<AddBackward0>)\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01383581939848921 Epoch 429 from 2000\n",
      "Measure Loss tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030409860588125324 Epoch 430 from 2000\n",
      "Measure Loss tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0037014931426076407 Epoch 431 from 2000\n",
      "Measure Loss tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01122469891211114 Epoch 432 from 2000\n",
      "Measure Loss tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "tensor(0.0120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01203674420497603 Epoch 433 from 2000\n",
      "Measure Loss tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005315760927489435 Epoch 434 from 2000\n",
      "Measure Loss tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001556111786498173 Epoch 435 from 2000\n",
      "Measure Loss tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00512109561794989 Epoch 436 from 2000\n",
      "Measure Loss tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008914987472906916 Epoch 437 from 2000\n",
      "Measure Loss tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007045642358617199 Epoch 438 from 2000\n",
      "Measure Loss tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026613466230042096 Epoch 439 from 2000\n",
      "Measure Loss tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010264208728373366 Epoch 440 from 2000\n",
      "Measure Loss tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031794290712723176 Epoch 441 from 2000\n",
      "Measure Loss tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006047255593325137 Epoch 442 from 2000\n",
      "Measure Loss tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006519715692268073 Epoch 443 from 2000\n",
      "Measure Loss tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004505767911676466 Epoch 444 from 2000\n",
      "Measure Loss tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019327054335774533 Epoch 445 from 2000\n",
      "Measure Loss tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010237338260331895 Epoch 446 from 2000\n",
      "Measure Loss tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0020434834828437646 Epoch 447 from 2000\n",
      "Measure Loss tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033440680024807716 Epoch 448 from 2000\n",
      "Measure Loss tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033018199185786356 Epoch 449 from 2000\n",
      "Measure Loss tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019768243861916354 Epoch 450 from 2000\n",
      "Measure Loss tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009873765405438873 Epoch 451 from 2000\n",
      "Measure Loss tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013017611533358668 Epoch 452 from 2000\n",
      "Measure Loss tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021259385026868435 Epoch 453 from 2000\n",
      "Measure Loss tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021786409942464178 Epoch 454 from 2000\n",
      "Measure Loss tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014173162061049767 Epoch 455 from 2000\n",
      "Measure Loss tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009173656268108959 Epoch 456 from 2000\n",
      "Measure Loss tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012011713649497835 Epoch 457 from 2000\n",
      "Measure Loss tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016636774115133422 Epoch 458 from 2000\n",
      "Measure Loss tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016032964383044914 Epoch 459 from 2000\n",
      "Measure Loss tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011187125175256816 Epoch 460 from 2000\n",
      "Measure Loss tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008204067838947374 Epoch 461 from 2000\n",
      "Measure Loss tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009753267485457357 Epoch 462 from 2000\n",
      "Measure Loss tensor(0.0949, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001287770236293889 Epoch 463 from 2000\n",
      "Measure Loss tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013588263068870267 Epoch 464 from 2000\n",
      "Measure Loss tensor(0.0949, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011240984251259395 Epoch 465 from 2000\n",
      "Measure Loss tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008516161040368366 Epoch 466 from 2000\n",
      "Measure Loss tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007923597011592466 Epoch 467 from 2000\n",
      "Measure Loss tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009364058775459999 Epoch 468 from 2000\n",
      "Measure Loss tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010737092291010913 Epoch 469 from 2000\n",
      "Measure Loss tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010397132374077443 Epoch 470 from 2000\n",
      "Measure Loss tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008774575729200997 Epoch 471 from 2000\n",
      "Measure Loss tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007524999395096232 Epoch 472 from 2000\n",
      "Measure Loss tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007645608093387083 Epoch 473 from 2000\n",
      "Measure Loss tensor(0.0901, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008554528474992527 Epoch 474 from 2000\n",
      "Measure Loss tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009007362980552118 Epoch 475 from 2000\n",
      "Measure Loss tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008507795828105028 Epoch 476 from 2000\n",
      "Measure Loss tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000762088565183639 Epoch 477 from 2000\n",
      "Measure Loss tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007172242240611641 Epoch 478 from 2000\n",
      "Measure Loss tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007368144750426656 Epoch 479 from 2000\n",
      "Measure Loss tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007776273269809249 Epoch 480 from 2000\n",
      "Measure Loss tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007892016440932951 Epoch 481 from 2000\n",
      "Measure Loss tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007591154568805216 Epoch 482 from 2000\n",
      "Measure Loss tensor(0.0899, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007135695319658858 Epoch 483 from 2000\n",
      "Measure Loss tensor(0.0901, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006842260850527457 Epoch 484 from 2000\n",
      "Measure Loss tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006838574412546515 Epoch 485 from 2000\n",
      "Measure Loss tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007008511731996749 Epoch 486 from 2000\n",
      "Measure Loss tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007145600302466869 Epoch 487 from 2000\n",
      "Measure Loss tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007102536726311429 Epoch 488 from 2000\n",
      "Measure Loss tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006885760339407567 Epoch 489 from 2000\n",
      "Measure Loss tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006620392901810957 Epoch 490 from 2000\n",
      "Measure Loss tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006445509124942933 Epoch 491 from 2000\n",
      "Measure Loss tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006420003045528322 Epoch 492 from 2000\n",
      "Measure Loss tensor(0.0882, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006494391343530579 Epoch 493 from 2000\n",
      "Measure Loss tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006553713786880046 Epoch 494 from 2000\n",
      "Measure Loss tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006519825048608703 Epoch 495 from 2000\n",
      "Measure Loss tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006390889128472156 Epoch 496 from 2000\n",
      "Measure Loss tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000623377327015595 Epoch 497 from 2000\n",
      "Measure Loss tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006118161549070837 Epoch 498 from 2000\n",
      "Measure Loss tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006074490219409217 Epoch 499 from 2000\n",
      "Measure Loss tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006079485553978834 Epoch 500 from 2000\n",
      "Measure Loss tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006089022781763857 Epoch 501 from 2000\n",
      "Measure Loss tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006068855906755648 Epoch 502 from 2000\n",
      "Measure Loss tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006012136068792363 Epoch 503 from 2000\n",
      "Measure Loss tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005929627668644755 Epoch 504 from 2000\n",
      "Measure Loss tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005845123224308174 Epoch 505 from 2000\n",
      "Measure Loss tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005771787035631358 Epoch 506 from 2000\n",
      "Measure Loss tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005718656666940872 Epoch 507 from 2000\n",
      "Measure Loss tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005686468525638087 Epoch 508 from 2000\n",
      "Measure Loss tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005664295340727364 Epoch 509 from 2000\n",
      "Measure Loss tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005645503769930049 Epoch 510 from 2000\n",
      "Measure Loss tensor(0.0857, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005616550079781006 Epoch 511 from 2000\n",
      "Measure Loss tensor(0.0858, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005574469599738481 Epoch 512 from 2000\n",
      "Measure Loss tensor(0.0854, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005521305094585897 Epoch 513 from 2000\n",
      "Measure Loss tensor(0.0854, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005462897768388397 Epoch 514 from 2000\n",
      "Measure Loss tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005406429309335973 Epoch 515 from 2000\n",
      "Measure Loss tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005355329476763199 Epoch 516 from 2000\n",
      "Measure Loss tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005309999752841207 Epoch 517 from 2000\n",
      "Measure Loss tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005271883182976793 Epoch 518 from 2000\n",
      "Measure Loss tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005237215147604217 Epoch 519 from 2000\n",
      "Measure Loss tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005204099990633151 Epoch 520 from 2000\n",
      "Measure Loss tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005172241796577977 Epoch 521 from 2000\n",
      "Measure Loss tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005140301545326233 Epoch 522 from 2000\n",
      "Measure Loss tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005105780760322807 Epoch 523 from 2000\n",
      "Measure Loss tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005070064748601674 Epoch 524 from 2000\n",
      "Measure Loss tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005031620741468166 Epoch 525 from 2000\n",
      "Measure Loss tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004992825072547977 Epoch 526 from 2000\n",
      "Measure Loss tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004952920024918507 Epoch 527 from 2000\n",
      "Measure Loss tensor(0.0834, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004913869267587236 Epoch 528 from 2000\n",
      "Measure Loss tensor(0.0834, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00048746851001744306 Epoch 529 from 2000\n",
      "Measure Loss tensor(0.0831, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004837806165814589 Epoch 530 from 2000\n",
      "Measure Loss tensor(0.0831, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00047987643754539744 Epoch 531 from 2000\n",
      "Measure Loss tensor(0.0829, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004762073590930187 Epoch 532 from 2000\n",
      "Measure Loss tensor(0.0828, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004724365567155491 Epoch 533 from 2000\n",
      "Measure Loss tensor(0.0826, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004688921034416683 Epoch 534 from 2000\n",
      "Measure Loss tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004654155647034374 Epoch 535 from 2000\n",
      "Measure Loss tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00046200937542622005 Epoch 536 from 2000\n",
      "Measure Loss tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004586444034718327 Epoch 537 from 2000\n",
      "Measure Loss tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00045535226820729916 Epoch 538 from 2000\n",
      "Measure Loss tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00045220069487426104 Epoch 539 from 2000\n",
      "Measure Loss tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000449221012269851 Epoch 540 from 2000\n",
      "Measure Loss tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044646867842926415 Epoch 541 from 2000\n",
      "Measure Loss tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000444049281213932 Epoch 542 from 2000\n",
      "Measure Loss tensor(0.0816, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044229277205928315 Epoch 543 from 2000\n",
      "Measure Loss tensor(0.0813, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044113946083274973 Epoch 544 from 2000\n",
      "Measure Loss tensor(0.0814, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044127515294847217 Epoch 545 from 2000\n",
      "Measure Loss tensor(0.0810, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044360115657804085 Epoch 546 from 2000\n",
      "Measure Loss tensor(0.0812, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004491853789849949 Epoch 547 from 2000\n",
      "Measure Loss tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004606014877967383 Epoch 548 from 2000\n",
      "Measure Loss tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004818550783155897 Epoch 549 from 2000\n",
      "Measure Loss tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005201466145485277 Epoch 550 from 2000\n",
      "Measure Loss tensor(0.0810, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005880374358221249 Epoch 551 from 2000\n",
      "Measure Loss tensor(0.0797, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000709270400333016 Epoch 552 from 2000\n",
      "Measure Loss tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009232033818838629 Epoch 553 from 2000\n",
      "Measure Loss tensor(0.0789, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013090590972276188 Epoch 554 from 2000\n",
      "Measure Loss tensor(0.0814, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019912823054327286 Epoch 555 from 2000\n",
      "Measure Loss tensor(0.0777, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003228511258463763 Epoch 556 from 2000\n",
      "Measure Loss tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0053532485270048015 Epoch 557 from 2000\n",
      "Measure Loss tensor(0.0755, grad_fn=<AddBackward0>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009054985541052479 Epoch 558 from 2000\n",
      "Measure Loss tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.014586654724567825 Epoch 559 from 2000\n",
      "Measure Loss tensor(0.0716, grad_fn=<AddBackward0>)\n",
      "tensor(0.0224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.022396739210053366 Epoch 560 from 2000\n",
      "Measure Loss tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "tensor(0.0286, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.028599884126574306 Epoch 561 from 2000\n",
      "Measure Loss tensor(0.0663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.029323939292128325 Epoch 562 from 2000\n",
      "Measure Loss tensor(0.0738, grad_fn=<AddBackward0>)\n",
      "tensor(0.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.017981312792582117 Epoch 563 from 2000\n",
      "Measure Loss tensor(0.0639, grad_fn=<AddBackward0>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0045823778111094325 Epoch 564 from 2000\n",
      "Measure Loss tensor(0.0635, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002612362446966967 Epoch 565 from 2000\n",
      "Measure Loss tensor(0.0681, grad_fn=<AddBackward0>)\n",
      "tensor(0.0108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010831668652332187 Epoch 566 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.0138, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.013777731785229102 Epoch 567 from 2000\n",
      "Measure Loss tensor(0.0689, grad_fn=<AddBackward0>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006350534688784544 Epoch 568 from 2000\n",
      "Measure Loss tensor(0.0678, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009699751185383478 Epoch 569 from 2000\n",
      "Measure Loss tensor(0.0682, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004150883754029137 Epoch 570 from 2000\n",
      "Measure Loss tensor(0.0766, grad_fn=<AddBackward0>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009348024753907107 Epoch 571 from 2000\n",
      "Measure Loss tensor(0.0722, grad_fn=<AddBackward0>)\n",
      "tensor(0.0097, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009700299518182645 Epoch 572 from 2000\n",
      "Measure Loss tensor(0.0794, grad_fn=<AddBackward0>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005289075311750086 Epoch 573 from 2000\n",
      "Measure Loss tensor(0.0773, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001311851557190272 Epoch 574 from 2000\n",
      "Measure Loss tensor(0.0784, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010214954012190563 Epoch 575 from 2000\n",
      "Measure Loss tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003628054171989961 Epoch 576 from 2000\n",
      "Measure Loss tensor(0.0767, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005976435220943037 Epoch 577 from 2000\n",
      "Measure Loss tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005559992700363284 Epoch 578 from 2000\n",
      "Measure Loss tensor(0.0747, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002984299923693757 Epoch 579 from 2000\n",
      "Measure Loss tensor(0.0760, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008272922543777438 Epoch 580 from 2000\n",
      "Measure Loss tensor(0.0743, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010277148443125455 Epoch 581 from 2000\n",
      "Measure Loss tensor(0.0711, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025121617096796217 Epoch 582 from 2000\n",
      "Measure Loss tensor(0.0738, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0028038788781693563 Epoch 583 from 2000\n",
      "Measure Loss tensor(0.0699, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001560602507717705 Epoch 584 from 2000\n",
      "Measure Loss tensor(0.0711, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006509295180559607 Epoch 585 from 2000\n",
      "Measure Loss tensor(0.0720, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00114100438680858 Epoch 586 from 2000\n",
      "Measure Loss tensor(0.0701, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018783992195590246 Epoch 587 from 2000\n",
      "Measure Loss tensor(0.0734, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001611659357446374 Epoch 588 from 2000\n",
      "Measure Loss tensor(0.0722, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007993944304808026 Epoch 589 from 2000\n",
      "Measure Loss tensor(0.0734, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000520986274471057 Epoch 590 from 2000\n",
      "Measure Loss tensor(0.0752, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009333551035731951 Epoch 591 from 2000\n",
      "Measure Loss tensor(0.0739, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001326237950988651 Epoch 592 from 2000\n",
      "Measure Loss tensor(0.0764, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011810271324436254 Epoch 593 from 2000\n",
      "Measure Loss tensor(0.0749, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007247224164873944 Epoch 594 from 2000\n",
      "Measure Loss tensor(0.0756, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00046691429505898627 Epoch 595 from 2000\n",
      "Measure Loss tensor(0.0756, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000590530153800394 Epoch 596 from 2000\n",
      "Measure Loss tensor(0.0741, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008344525490130783 Epoch 597 from 2000\n",
      "Measure Loss tensor(0.0751, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008573371164883573 Epoch 598 from 2000\n",
      "Measure Loss tensor(0.0730, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006283487593991883 Epoch 599 from 2000\n",
      "Measure Loss tensor(0.0733, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004037134051870458 Epoch 600 from 2000\n",
      "Measure Loss tensor(0.0728, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004039283147175037 Epoch 601 from 2000\n",
      "Measure Loss tensor(0.0715, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005576295996628363 Epoch 602 from 2000\n",
      "Measure Loss tensor(0.0725, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006213977205222693 Epoch 603 from 2000\n",
      "Measure Loss tensor(0.0712, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005070664410126082 Epoch 604 from 2000\n",
      "Measure Loss tensor(0.0718, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003615688839723811 Epoch 605 from 2000\n",
      "Measure Loss tensor(0.0719, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00034652767810503573 Epoch 606 from 2000\n",
      "Measure Loss tensor(0.0715, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004381025208592012 Epoch 607 from 2000\n",
      "Measure Loss tensor(0.0726, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004901634622832499 Epoch 608 from 2000\n",
      "Measure Loss tensor(0.0720, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00043170583799992287 Epoch 609 from 2000\n",
      "Measure Loss tensor(0.0728, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003319476200677413 Epoch 610 from 2000\n",
      "Measure Loss tensor(0.0728, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002960112022941948 Epoch 611 from 2000\n",
      "Measure Loss tensor(0.0726, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00034162586823415956 Epoch 612 from 2000\n",
      "Measure Loss tensor(0.0732, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00039759240085653904 Epoch 613 from 2000\n",
      "Measure Loss tensor(0.0724, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003958759513124373 Epoch 614 from 2000\n",
      "Measure Loss tensor(0.0729, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003386725876572639 Epoch 615 from 2000\n",
      "Measure Loss tensor(0.0723, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002836738663377828 Epoch 616 from 2000\n",
      "Measure Loss tensor(0.0720, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00027712450025589965 Epoch 617 from 2000\n",
      "Measure Loss tensor(0.0721, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00030941358413872814 Epoch 618 from 2000\n",
      "Measure Loss tensor(0.0712, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00033425607715317334 Epoch 619 from 2000\n",
      "Measure Loss tensor(0.0716, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003210302391076676 Epoch 620 from 2000\n",
      "Measure Loss tensor(0.0709, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002843205008101336 Epoch 621 from 2000\n",
      "Measure Loss tensor(0.0709, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00025972312570839395 Epoch 622 from 2000\n",
      "Measure Loss tensor(0.0709, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002651854830233602 Epoch 623 from 2000\n",
      "Measure Loss tensor(0.0705, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002848871169400807 Epoch 624 from 2000\n",
      "Measure Loss tensor(0.0709, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00029245201043344194 Epoch 625 from 2000\n",
      "Measure Loss tensor(0.0705, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002785381034500153 Epoch 626 from 2000\n",
      "Measure Loss tensor(0.0708, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002567329457677216 Epoch 627 from 2000\n",
      "Measure Loss tensor(0.0708, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00024533939151423305 Epoch 628 from 2000\n",
      "Measure Loss tensor(0.0707, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002493893847683483 Epoch 629 from 2000\n",
      "Measure Loss tensor(0.0709, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00025908045880641687 Epoch 630 from 2000\n",
      "Measure Loss tensor(0.0706, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00026238219821246036 Epoch 631 from 2000\n",
      "Measure Loss tensor(0.0708, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00025533114700727053 Epoch 632 from 2000\n",
      "Measure Loss tensor(0.0704, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00024369258993130002 Epoch 633 from 2000\n",
      "Measure Loss tensor(0.0704, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023550414575299777 Epoch 634 from 2000\n",
      "Measure Loss tensor(0.0703, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023437510343700141 Epoch 635 from 2000\n",
      "Measure Loss tensor(0.0700, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002372585307125626 Epoch 636 from 2000\n",
      "Measure Loss tensor(0.0700, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023882143659859908 Epoch 637 from 2000\n",
      "Measure Loss tensor(0.0697, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023646638133414136 Epoch 638 from 2000\n",
      "Measure Loss tensor(0.0697, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023122187308060205 Epoch 639 from 2000\n",
      "Measure Loss tensor(0.0695, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002261915302515426 Epoch 640 from 2000\n",
      "Measure Loss tensor(0.0695, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022329957738078234 Epoch 641 from 2000\n",
      "Measure Loss tensor(0.0694, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022250711612516834 Epoch 642 from 2000\n",
      "Measure Loss tensor(0.0693, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022205586510139184 Epoch 643 from 2000\n",
      "Measure Loss tensor(0.0694, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002207738432268952 Epoch 644 from 2000\n",
      "Measure Loss tensor(0.0692, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021858129183163928 Epoch 645 from 2000\n",
      "Measure Loss tensor(0.0693, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021602860117004696 Epoch 646 from 2000\n",
      "Measure Loss tensor(0.0692, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021378964510692083 Epoch 647 from 2000\n",
      "Measure Loss tensor(0.0692, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002118351844803019 Epoch 648 from 2000\n",
      "Measure Loss tensor(0.0691, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002102103308378544 Epoch 649 from 2000\n",
      "Measure Loss tensor(0.0690, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020839597702131078 Epoch 650 from 2000\n",
      "Measure Loss tensor(0.0690, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002065646755269268 Epoch 651 from 2000\n",
      "Measure Loss tensor(0.0688, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002048483565581794 Epoch 652 from 2000\n",
      "Measure Loss tensor(0.0688, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020344463011620235 Epoch 653 from 2000\n",
      "Measure Loss tensor(0.0686, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020218244140732714 Epoch 654 from 2000\n",
      "Measure Loss tensor(0.0686, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020083378652806062 Epoch 655 from 2000\n",
      "Measure Loss tensor(0.0684, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019921919209696756 Epoch 656 from 2000\n",
      "Measure Loss tensor(0.0684, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000197362338388213 Epoch 657 from 2000\n",
      "Measure Loss tensor(0.0683, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019537857882872726 Epoch 658 from 2000\n",
      "Measure Loss tensor(0.0682, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019360083910343115 Epoch 659 from 2000\n",
      "Measure Loss tensor(0.0682, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001920225431168964 Epoch 660 from 2000\n",
      "Measure Loss tensor(0.0681, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019089197902707352 Epoch 661 from 2000\n",
      "Measure Loss tensor(0.0680, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001898665695431199 Epoch 662 from 2000\n",
      "Measure Loss tensor(0.0680, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018877531780602327 Epoch 663 from 2000\n",
      "Measure Loss tensor(0.0679, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018749818930533657 Epoch 664 from 2000\n",
      "Measure Loss tensor(0.0679, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018607567616809727 Epoch 665 from 2000\n",
      "Measure Loss tensor(0.0678, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018441639018593176 Epoch 666 from 2000\n",
      "Measure Loss tensor(0.0678, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018285220471839483 Epoch 667 from 2000\n",
      "Measure Loss tensor(0.0677, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018137728437546137 Epoch 668 from 2000\n",
      "Measure Loss tensor(0.0676, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018001978117612437 Epoch 669 from 2000\n",
      "Measure Loss tensor(0.0676, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017873716669765803 Epoch 670 from 2000\n",
      "Measure Loss tensor(0.0675, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001776769612876075 Epoch 671 from 2000\n",
      "Measure Loss tensor(0.0674, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001765216696101675 Epoch 672 from 2000\n",
      "Measure Loss tensor(0.0673, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017529494417388067 Epoch 673 from 2000\n",
      "Measure Loss tensor(0.0673, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000174179225279058 Epoch 674 from 2000\n",
      "Measure Loss tensor(0.0672, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017295202935025704 Epoch 675 from 2000\n",
      "Measure Loss tensor(0.0671, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017179939335399392 Epoch 676 from 2000\n",
      "Measure Loss tensor(0.0671, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017065490955665196 Epoch 677 from 2000\n",
      "Measure Loss tensor(0.0670, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000169518248644011 Epoch 678 from 2000\n",
      "Measure Loss tensor(0.0670, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001683949468879375 Epoch 679 from 2000\n",
      "Measure Loss tensor(0.0669, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016719769382321669 Epoch 680 from 2000\n",
      "Measure Loss tensor(0.0669, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016610521724169352 Epoch 681 from 2000\n",
      "Measure Loss tensor(0.0668, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016495619974889222 Epoch 682 from 2000\n",
      "Measure Loss tensor(0.0668, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016385746485427072 Epoch 683 from 2000\n",
      "Measure Loss tensor(0.0667, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016277000528551827 Epoch 684 from 2000\n",
      "Measure Loss tensor(0.0667, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016167047992021524 Epoch 685 from 2000\n",
      "Measure Loss tensor(0.0666, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016053114046069264 Epoch 686 from 2000\n",
      "Measure Loss tensor(0.0665, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015950758730251112 Epoch 687 from 2000\n",
      "Measure Loss tensor(0.0665, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001584566466072387 Epoch 688 from 2000\n",
      "Measure Loss tensor(0.0664, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015744251387800383 Epoch 689 from 2000\n",
      "Measure Loss tensor(0.0663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001564646678755258 Epoch 690 from 2000\n",
      "Measure Loss tensor(0.0663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001555429072314961 Epoch 691 from 2000\n",
      "Measure Loss tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015458500883839215 Epoch 692 from 2000\n",
      "Measure Loss tensor(0.0662, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001537321438625237 Epoch 693 from 2000\n",
      "Measure Loss tensor(0.0661, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015286929632811288 Epoch 694 from 2000\n",
      "Measure Loss tensor(0.0661, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015214576584768848 Epoch 695 from 2000\n",
      "Measure Loss tensor(0.0660, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015144152343997823 Epoch 696 from 2000\n",
      "Measure Loss tensor(0.0660, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001510197546085424 Epoch 697 from 2000\n",
      "Measure Loss tensor(0.0659, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015085202762278135 Epoch 698 from 2000\n",
      "Measure Loss tensor(0.0659, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015114120588895726 Epoch 699 from 2000\n",
      "Measure Loss tensor(0.0658, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001521710725639498 Epoch 700 from 2000\n",
      "Measure Loss tensor(0.0658, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015434133132469875 Epoch 701 from 2000\n",
      "Measure Loss tensor(0.0656, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015854371770408987 Epoch 702 from 2000\n",
      "Measure Loss tensor(0.0658, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016598944245444202 Epoch 703 from 2000\n",
      "Measure Loss tensor(0.0655, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017920668785373722 Epoch 704 from 2000\n",
      "Measure Loss tensor(0.0657, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020210216649469413 Epoch 705 from 2000\n",
      "Measure Loss tensor(0.0653, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00024234696410915953 Epoch 706 from 2000\n",
      "Measure Loss tensor(0.0657, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003125713045023665 Epoch 707 from 2000\n",
      "Measure Loss tensor(0.0650, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00043775362862242075 Epoch 708 from 2000\n",
      "Measure Loss tensor(0.0658, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006579655014506503 Epoch 709 from 2000\n",
      "Measure Loss tensor(0.0646, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010577181819891844 Epoch 710 from 2000\n",
      "Measure Loss tensor(0.0660, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017592499669306528 Epoch 711 from 2000\n",
      "Measure Loss tensor(0.0638, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030411043480555768 Epoch 712 from 2000\n",
      "Measure Loss tensor(0.0663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0051891606143784665 Epoch 713 from 2000\n",
      "Measure Loss tensor(0.0622, grad_fn=<AddBackward0>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008942576025592593 Epoch 714 from 2000\n",
      "Measure Loss tensor(0.0663, grad_fn=<AddBackward0>)\n",
      "tensor(0.0142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.014174922513876897 Epoch 715 from 2000\n",
      "Measure Loss tensor(0.0591, grad_fn=<AddBackward0>)\n",
      "tensor(0.0213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.02132397285873395 Epoch 716 from 2000\n",
      "Measure Loss tensor(0.0647, grad_fn=<AddBackward0>)\n",
      "tensor(0.0254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.025400957318077164 Epoch 717 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(0.0238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.023804954818431617 Epoch 718 from 2000\n",
      "Measure Loss tensor(0.0588, grad_fn=<AddBackward0>)\n",
      "tensor(0.0127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0127101277748943 Epoch 719 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003253518482513763 Epoch 720 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0040692960248382565 Epoch 721 from 2000\n",
      "Measure Loss tensor(0.0565, grad_fn=<AddBackward0>)\n",
      "tensor(0.0098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009842874486790327 Epoch 722 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.011011912817924309 Epoch 723 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006536786403182532 Epoch 724 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031190097957166964 Epoch 725 from 2000\n",
      "Measure Loss tensor(0.0613, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034384946267062904 Epoch 726 from 2000\n",
      "Measure Loss tensor(0.0650, grad_fn=<AddBackward0>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005157596160245174 Epoch 727 from 2000\n",
      "Measure Loss tensor(0.0630, grad_fn=<AddBackward0>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006686641889518965 Epoch 728 from 2000\n",
      "Measure Loss tensor(0.0684, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0072836477859190224 Epoch 729 from 2000\n",
      "Measure Loss tensor(0.0634, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007644941479407844 Epoch 730 from 2000\n",
      "Measure Loss tensor(0.0673, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007478184668103925 Epoch 731 from 2000\n",
      "Measure Loss tensor(0.0612, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005903897439076571 Epoch 732 from 2000\n",
      "Measure Loss tensor(0.0626, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003369855180558012 Epoch 733 from 2000\n",
      "Measure Loss tensor(0.0594, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011388349762205404 Epoch 734 from 2000\n",
      "Measure Loss tensor(0.0576, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016831586403597747 Epoch 735 from 2000\n",
      "Measure Loss tensor(0.0597, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035391094454291666 Epoch 736 from 2000\n",
      "Measure Loss tensor(0.0560, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032190707785268256 Epoch 737 from 2000\n",
      "Measure Loss tensor(0.0589, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011593802006912824 Epoch 738 from 2000\n",
      "Measure Loss tensor(0.0589, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004240502940048516 Epoch 739 from 2000\n",
      "Measure Loss tensor(0.0583, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017974924438827291 Epoch 740 from 2000\n",
      "Measure Loss tensor(0.0627, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002805572693730928 Epoch 741 from 2000\n",
      "Measure Loss tensor(0.0604, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001890960776090877 Epoch 742 from 2000\n",
      "Measure Loss tensor(0.0633, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005849881313291659 Epoch 743 from 2000\n",
      "Measure Loss tensor(0.0635, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00036939055150401285 Epoch 744 from 2000\n",
      "Measure Loss tensor(0.0626, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010639573528720362 Epoch 745 from 2000\n",
      "Measure Loss tensor(0.0648, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015977288324250773 Epoch 746 from 2000\n",
      "Measure Loss tensor(0.0623, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013379782437358389 Epoch 747 from 2000\n",
      "Measure Loss tensor(0.0634, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007723157886680103 Epoch 748 from 2000\n",
      "Measure Loss tensor(0.0619, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005264230747602397 Epoch 749 from 2000\n",
      "Measure Loss tensor(0.0613, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006273279390473347 Epoch 750 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006822360593393235 Epoch 751 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005399267477707094 Epoch 752 from 2000\n",
      "Measure Loss tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004922274496855711 Epoch 753 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005834971398710513 Epoch 754 from 2000\n",
      "Measure Loss tensor(0.0604, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000541060736477582 Epoch 755 from 2000\n",
      "Measure Loss tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00034261290201675063 Epoch 756 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002420148682026471 Epoch 757 from 2000\n",
      "Measure Loss tensor(0.0618, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00036699495010437857 Epoch 758 from 2000\n",
      "Measure Loss tensor(0.0615, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005075200492272099 Epoch 759 from 2000\n",
      "Measure Loss tensor(0.0621, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00043890096725417014 Epoch 760 from 2000\n",
      "Measure Loss tensor(0.0623, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002451755509244912 Epoch 761 from 2000\n",
      "Measure Loss tensor(0.0622, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001463310035955205 Epoch 762 from 2000\n",
      "Measure Loss tensor(0.0623, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002292645868383191 Epoch 763 from 2000\n",
      "Measure Loss tensor(0.0621, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003556800262015285 Epoch 764 from 2000\n",
      "Measure Loss tensor(0.0620, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003565532376489991 Epoch 765 from 2000\n",
      "Measure Loss tensor(0.0615, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002447717407327262 Epoch 766 from 2000\n",
      "Measure Loss tensor(0.0615, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014972795967542617 Epoch 767 from 2000\n",
      "Measure Loss tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015511258174041366 Epoch 768 from 2000\n",
      "Measure Loss tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021382817809463295 Epoch 769 from 2000\n",
      "Measure Loss tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023863919134015457 Epoch 770 from 2000\n",
      "Measure Loss tensor(0.0607, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021135319203154839 Epoch 771 from 2000\n",
      "Measure Loss tensor(0.0612, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017193147624368062 Epoch 772 from 2000\n",
      "Measure Loss tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015784939910163705 Epoch 773 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016252008228277663 Epoch 774 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016188397753577307 Epoch 775 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001538117833534409 Epoch 776 from 2000\n",
      "Measure Loss tensor(0.0618, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001513498874817334 Epoch 777 from 2000\n",
      "Measure Loss tensor(0.0615, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015859104144178455 Epoch 778 from 2000\n",
      "Measure Loss tensor(0.0619, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016240642527190328 Epoch 779 from 2000\n",
      "Measure Loss tensor(0.0614, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015137022353967544 Epoch 780 from 2000\n",
      "Measure Loss tensor(0.0616, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001308578687254232 Epoch 781 from 2000\n",
      "Measure Loss tensor(0.0613, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011688480541502191 Epoch 782 from 2000\n",
      "Measure Loss tensor(0.0611, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011975080905123734 Epoch 783 from 2000\n",
      "Measure Loss tensor(0.0612, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013332696627570405 Epoch 784 from 2000\n",
      "Measure Loss tensor(0.0607, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014195757415576304 Epoch 785 from 2000\n",
      "Measure Loss tensor(0.0611, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013609884941507633 Epoch 786 from 2000\n",
      "Measure Loss tensor(0.0607, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012109669727907769 Epoch 787 from 2000\n",
      "Measure Loss tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010986414099689275 Epoch 788 from 2000\n",
      "Measure Loss tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010915550046344703 Epoch 789 from 2000\n",
      "Measure Loss tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000115071617668169 Epoch 790 from 2000\n",
      "Measure Loss tensor(0.0611, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011979307799884992 Epoch 791 from 2000\n",
      "Measure Loss tensor(0.0609, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011905323036814309 Epoch 792 from 2000\n",
      "Measure Loss tensor(0.0612, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011445840430625995 Epoch 793 from 2000\n",
      "Measure Loss tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001101824458843776 Epoch 794 from 2000\n",
      "Measure Loss tensor(0.0611, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010835242312619873 Epoch 795 from 2000\n",
      "Measure Loss tensor(0.0611, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001080686387725133 Epoch 796 from 2000\n",
      "Measure Loss tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001071381251188704 Epoch 797 from 2000\n",
      "Measure Loss tensor(0.0610, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010492827223380684 Epoch 798 from 2000\n",
      "Measure Loss tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010281186984580556 Epoch 799 from 2000\n",
      "Measure Loss tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010218700816555252 Epoch 800 from 2000\n",
      "Measure Loss tensor(0.0607, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010305016931473196 Epoch 801 from 2000\n",
      "Measure Loss tensor(0.0607, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010418729880678519 Epoch 802 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010396118910919063 Epoch 803 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010203943182031062 Epoch 804 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.9191e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.91908224025289e-05 Epoch 805 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.6911e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.691138804060839e-05 Epoch 806 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.5967e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.596696019861949e-05 Epoch 807 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.6116e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.611568347496655e-05 Epoch 808 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.6471e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.647063516757816e-05 Epoch 809 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.6490e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.649040089246828e-05 Epoch 810 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.5900e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.589954595748766e-05 Epoch 811 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.5089e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.508921270870849e-05 Epoch 812 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(9.4314e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.431410787727042e-05 Epoch 813 from 2000\n",
      "Measure Loss tensor(0.0605, grad_fn=<AddBackward0>)\n",
      "tensor(9.3803e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.380347472397463e-05 Epoch 814 from 2000\n",
      "Measure Loss tensor(0.0605, grad_fn=<AddBackward0>)\n",
      "tensor(9.3387e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.338744857304865e-05 Epoch 815 from 2000\n",
      "Measure Loss tensor(0.0604, grad_fn=<AddBackward0>)\n",
      "tensor(9.2931e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.293119019799083e-05 Epoch 816 from 2000\n",
      "Measure Loss tensor(0.0604, grad_fn=<AddBackward0>)\n",
      "tensor(9.2126e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.21259834517461e-05 Epoch 817 from 2000\n",
      "Measure Loss tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(9.1169e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.116885957573658e-05 Epoch 818 from 2000\n",
      "Measure Loss tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(9.0202e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.020211533512967e-05 Epoch 819 from 2000\n",
      "Measure Loss tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(8.9377e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.937726272301098e-05 Epoch 820 from 2000\n",
      "Measure Loss tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(8.8856e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.885581777046373e-05 Epoch 821 from 2000\n",
      "Measure Loss tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(8.8491e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.849149810386278e-05 Epoch 822 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(8.8213e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.821317555385204e-05 Epoch 823 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(8.7827e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.782713104266078e-05 Epoch 824 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(8.7367e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.73671039543172e-05 Epoch 825 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(8.6862e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.686181382891946e-05 Epoch 826 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(8.6362e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.636205909022984e-05 Epoch 827 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(8.5922e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.592163702833524e-05 Epoch 828 from 2000\n",
      "Measure Loss tensor(0.0601, grad_fn=<AddBackward0>)\n",
      "tensor(8.5598e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.559826027308102e-05 Epoch 829 from 2000\n",
      "Measure Loss tensor(0.0601, grad_fn=<AddBackward0>)\n",
      "tensor(8.5317e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.531696805028881e-05 Epoch 830 from 2000\n",
      "Measure Loss tensor(0.0601, grad_fn=<AddBackward0>)\n",
      "tensor(8.5052e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.505188451358526e-05 Epoch 831 from 2000\n",
      "Measure Loss tensor(0.0601, grad_fn=<AddBackward0>)\n",
      "tensor(8.4771e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.477144191845215e-05 Epoch 832 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(8.4520e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.451998518584092e-05 Epoch 833 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(8.4260e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.425987851684661e-05 Epoch 834 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(8.4076e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.40760570691022e-05 Epoch 835 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(8.4032e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.403205297341241e-05 Epoch 836 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(8.4143e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.414253331062873e-05 Epoch 837 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(8.4514e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.451391782459113e-05 Epoch 838 from 2000\n",
      "Measure Loss tensor(0.0598, grad_fn=<AddBackward0>)\n",
      "tensor(8.5270e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.527023776452148e-05 Epoch 839 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(8.6540e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.653970055425372e-05 Epoch 840 from 2000\n",
      "Measure Loss tensor(0.0598, grad_fn=<AddBackward0>)\n",
      "tensor(8.8560e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.856020239530369e-05 Epoch 841 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(9.1782e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.178239242888936e-05 Epoch 842 from 2000\n",
      "Measure Loss tensor(0.0597, grad_fn=<AddBackward0>)\n",
      "tensor(9.6933e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.693320458960551e-05 Epoch 843 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001051404642322816 Epoch 844 from 2000\n",
      "Measure Loss tensor(0.0596, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011824940510100744 Epoch 845 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013918951992390716 Epoch 846 from 2000\n",
      "Measure Loss tensor(0.0595, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017329985017770923 Epoch 847 from 2000\n",
      "Measure Loss tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002287211151442588 Epoch 848 from 2000\n",
      "Measure Loss tensor(0.0593, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00032083050977492873 Epoch 849 from 2000\n",
      "Measure Loss tensor(0.0601, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004723710542658364 Epoch 850 from 2000\n",
      "Measure Loss tensor(0.0589, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007290686885917749 Epoch 851 from 2000\n",
      "Measure Loss tensor(0.0603, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011541439884289915 Epoch 852 from 2000\n",
      "Measure Loss tensor(0.0583, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018824876693634095 Epoch 853 from 2000\n",
      "Measure Loss tensor(0.0606, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003065844057839933 Epoch 854 from 2000\n",
      "Measure Loss tensor(0.0573, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005055568065095696 Epoch 855 from 2000\n",
      "Measure Loss tensor(0.0608, grad_fn=<AddBackward0>)\n",
      "tensor(0.0080, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00801108743606085 Epoch 856 from 2000\n",
      "Measure Loss tensor(0.0552, grad_fn=<AddBackward0>)\n",
      "tensor(0.0124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.012409421061812507 Epoch 857 from 2000\n",
      "Measure Loss tensor(0.0602, grad_fn=<AddBackward0>)\n",
      "tensor(0.0170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.017002644272947346 Epoch 858 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0207, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.020703849916192688 Epoch 859 from 2000\n",
      "Measure Loss tensor(0.0569, grad_fn=<AddBackward0>)\n",
      "tensor(0.0185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.018511531572629723 Epoch 860 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.01105820026346855 Epoch 861 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002763976154725666 Epoch 862 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015320578900392104 Epoch 863 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00654873644816294 Epoch 864 from 2000\n",
      "Measure Loss tensor(0.0559, grad_fn=<AddBackward0>)\n",
      "tensor(0.0104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0103873390486634 Epoch 865 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009457212577224013 Epoch 866 from 2000\n",
      "Measure Loss tensor(0.0586, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005083578173353389 Epoch 867 from 2000\n",
      "Measure Loss tensor(0.0570, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001341583253888939 Epoch 868 from 2000\n",
      "Measure Loss tensor(0.0592, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00026974381147061815 Epoch 869 from 2000\n",
      "Measure Loss tensor(0.0613, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017380890289586859 Epoch 870 from 2000\n",
      "Measure Loss tensor(0.0593, grad_fn=<AddBackward0>)\n",
      "tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004498712510485321 Epoch 871 from 2000\n",
      "Measure Loss tensor(0.0626, grad_fn=<AddBackward0>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00698415519645529 Epoch 872 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(0.0080, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008036139627421147 Epoch 873 from 2000\n",
      "Measure Loss tensor(0.0599, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006473033280780143 Epoch 874 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032209809897073916 Epoch 875 from 2000\n",
      "Measure Loss tensor(0.0554, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005847160411314201 Epoch 876 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007043059613158773 Epoch 877 from 2000\n",
      "Measure Loss tensor(0.0529, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025783889106575906 Epoch 878 from 2000\n",
      "Measure Loss tensor(0.0556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033726787846055277 Epoch 879 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021950834191490467 Epoch 880 from 2000\n",
      "Measure Loss tensor(0.0566, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005690096067576113 Epoch 881 from 2000\n",
      "Measure Loss tensor(0.0569, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023339460677699578 Epoch 882 from 2000\n",
      "Measure Loss tensor(0.0573, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001176694966707698 Epoch 883 from 2000\n",
      "Measure Loss tensor(0.0605, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022165735895217902 Epoch 884 from 2000\n",
      "Measure Loss tensor(0.0582, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024453297742248036 Epoch 885 from 2000\n",
      "Measure Loss tensor(0.0611, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017594239568907858 Epoch 886 from 2000\n",
      "Measure Loss tensor(0.0587, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007941445001177006 Epoch 887 from 2000\n",
      "Measure Loss tensor(0.0591, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021584979302299867 Epoch 888 from 2000\n",
      "Measure Loss tensor(0.0586, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00030114386969043477 Epoch 889 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007678906492638916 Epoch 890 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010621080160283613 Epoch 891 from 2000\n",
      "Measure Loss tensor(0.0562, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008840053027614721 Epoch 892 from 2000\n",
      "Measure Loss tensor(0.0568, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00041673749517291453 Epoch 893 from 2000\n",
      "Measure Loss tensor(0.0564, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013487864628703534 Epoch 894 from 2000\n",
      "Measure Loss tensor(0.0566, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002541865920961345 Epoch 895 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005429648629014868 Epoch 896 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006554824314969315 Epoch 897 from 2000\n",
      "Measure Loss tensor(0.0585, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004952278347568753 Epoch 898 from 2000\n",
      "Measure Loss tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00024467527115735414 Epoch 899 from 2000\n",
      "Measure Loss tensor(0.0589, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010782641127294777 Epoch 900 from 2000\n",
      "Measure Loss tensor(0.0589, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015131445641286805 Epoch 901 from 2000\n",
      "Measure Loss tensor(0.0585, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002954956940577917 Epoch 902 from 2000\n",
      "Measure Loss tensor(0.0592, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00040208627488082547 Epoch 903 from 2000\n",
      "Measure Loss tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00038288635560299093 Epoch 904 from 2000\n",
      "Measure Loss tensor(0.0586, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002558457917074179 Epoch 905 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012730350194168797 Epoch 906 from 2000\n",
      "Measure Loss tensor(0.0576, grad_fn=<AddBackward0>)\n",
      "tensor(8.6429e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.6429289602575e-05 Epoch 907 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001362171757860038 Epoch 908 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020985462550327574 Epoch 909 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023893265372933512 Epoch 910 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020503412190378886 Epoch 911 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013807056443860015 Epoch 912 from 2000\n",
      "Measure Loss tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(8.8130e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.813019889019603e-05 Epoch 913 from 2000\n",
      "Measure Loss tensor(0.0581, grad_fn=<AddBackward0>)\n",
      "tensor(8.4120e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.411984559851066e-05 Epoch 914 from 2000\n",
      "Measure Loss tensor(0.0584, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001159181310075728 Epoch 915 from 2000\n",
      "Measure Loss tensor(0.0582, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001504699261328962 Epoch 916 from 2000\n",
      "Measure Loss tensor(0.0586, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016159616956417627 Epoch 917 from 2000\n",
      "Measure Loss tensor(0.0581, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001456664674204286 Epoch 918 from 2000\n",
      "Measure Loss tensor(0.0584, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011550319925644967 Epoch 919 from 2000\n",
      "Measure Loss tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(8.8810e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.881022272540057e-05 Epoch 920 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(7.8196e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.819591215943515e-05 Epoch 921 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(8.5304e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.530361310872676e-05 Epoch 922 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010032742783971227 Epoch 923 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010985644279035964 Epoch 924 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010738391171910847 Epoch 925 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(9.5847e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.584737507439124e-05 Epoch 926 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(8.3070e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.30695681378808e-05 Epoch 927 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(7.5135e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.513489760722354e-05 Epoch 928 from 2000\n",
      "Measure Loss tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(7.4382e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.438154107400016e-05 Epoch 929 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(7.9191e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.919060519470732e-05 Epoch 930 from 2000\n",
      "Measure Loss tensor(0.0581, grad_fn=<AddBackward0>)\n",
      "tensor(8.5444e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.544372096403782e-05 Epoch 931 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(8.8713e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.87128380195074e-05 Epoch 932 from 2000\n",
      "Measure Loss tensor(0.0581, grad_fn=<AddBackward0>)\n",
      "tensor(8.7088e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.708780755535226e-05 Epoch 933 from 2000\n",
      "Measure Loss tensor(0.0579, grad_fn=<AddBackward0>)\n",
      "tensor(8.1771e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.177123475699855e-05 Epoch 934 from 2000\n",
      "Measure Loss tensor(0.0580, grad_fn=<AddBackward0>)\n",
      "tensor(7.5398e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.539795164948038e-05 Epoch 935 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(7.0321e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.032065502895574e-05 Epoch 936 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.7818e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.78180477727393e-05 Epoch 937 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.8094e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.809410197430417e-05 Epoch 938 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(7.0368e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.036816044127373e-05 Epoch 939 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(7.2870e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.286998890079844e-05 Epoch 940 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(7.4083e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.40828548279309e-05 Epoch 941 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(7.3521e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.352108912979986e-05 Epoch 942 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(7.1615e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.161531396228504e-05 Epoch 943 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.9097e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.909681398739712e-05 Epoch 944 from 2000\n",
      "Measure Loss tensor(0.0576, grad_fn=<AddBackward0>)\n",
      "tensor(6.6560e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.655958548254564e-05 Epoch 945 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.4594e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.459435285674236e-05 Epoch 946 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.3519e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.351927950665713e-05 Epoch 947 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.3342e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.334196757812316e-05 Epoch 948 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(6.3686e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.368612220867969e-05 Epoch 949 from 2000\n",
      "Measure Loss tensor(0.0576, grad_fn=<AddBackward0>)\n",
      "tensor(6.4190e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.418975479024127e-05 Epoch 950 from 2000\n",
      "Measure Loss tensor(0.0576, grad_fn=<AddBackward0>)\n",
      "tensor(6.4571e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.457127982250827e-05 Epoch 951 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(6.4717e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.471657340071769e-05 Epoch 952 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(6.4503e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.450349465581642e-05 Epoch 953 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(6.3907e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.390747566977373e-05 Epoch 954 from 2000\n",
      "Measure Loss tensor(0.0575, grad_fn=<AddBackward0>)\n",
      "tensor(6.3040e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.303951384207034e-05 Epoch 955 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(6.2068e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.206790298991911e-05 Epoch 956 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(6.1125e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.112472724390055e-05 Epoch 957 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(6.0308e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.0308291168225584e-05 Epoch 958 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(5.9701e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.970142951397219e-05 Epoch 959 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(5.9219e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.921906219197786e-05 Epoch 960 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(5.8930e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.8930302249048624e-05 Epoch 961 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(5.8768e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.8768159785786447e-05 Epoch 962 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(5.8690e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.8690020376879616e-05 Epoch 963 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(5.8617e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.861726079042611e-05 Epoch 964 from 2000\n",
      "Measure Loss tensor(0.0573, grad_fn=<AddBackward0>)\n",
      "tensor(5.8636e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.863648473934075e-05 Epoch 965 from 2000\n",
      "Measure Loss tensor(0.0573, grad_fn=<AddBackward0>)\n",
      "tensor(5.8681e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.8681161416878754e-05 Epoch 966 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(5.8819e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.881934269174466e-05 Epoch 967 from 2000\n",
      "Measure Loss tensor(0.0573, grad_fn=<AddBackward0>)\n",
      "tensor(5.9021e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.902137245348253e-05 Epoch 968 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(5.9408e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.9408020395226155e-05 Epoch 969 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(6.0052e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.005205201253326e-05 Epoch 970 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(6.1164e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.11635619783855e-05 Epoch 971 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(6.2926e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.292611989272806e-05 Epoch 972 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(6.5746e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.574614322027683e-05 Epoch 973 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(7.0335e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.033452319049361e-05 Epoch 974 from 2000\n",
      "Measure Loss tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "tensor(7.7773e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.777325545135477e-05 Epoch 975 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(8.9742e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.974231334244635e-05 Epoch 976 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010967447138545414 Epoch 977 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014196553746055542 Epoch 978 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019698479253298383 Epoch 979 from 2000\n",
      "Measure Loss tensor(0.0570, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00028645057449054574 Epoch 980 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044273746253660956 Epoch 981 from 2000\n",
      "Measure Loss tensor(0.0568, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000693133455595043 Epoch 982 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011402533084148314 Epoch 983 from 2000\n",
      "Measure Loss tensor(0.0564, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018168586952493687 Epoch 984 from 2000\n",
      "Measure Loss tensor(0.0569, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003024576997426305 Epoch 985 from 2000\n",
      "Measure Loss tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0045625128193179905 Epoch 986 from 2000\n",
      "Measure Loss tensor(0.0563, grad_fn=<AddBackward0>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007070616334195652 Epoch 987 from 2000\n",
      "Measure Loss tensor(0.0544, grad_fn=<AddBackward0>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008711975168914535 Epoch 988 from 2000\n",
      "Measure Loss tensor(0.0544, grad_fn=<AddBackward0>)\n",
      "tensor(0.0102, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010173907745860838 Epoch 989 from 2000\n",
      "Measure Loss tensor(0.0526, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007584451956244591 Epoch 990 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00396505464002321 Epoch 991 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013627498676168149 Epoch 992 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0020217340262020283 Epoch 993 from 2000\n",
      "Measure Loss tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004131715263039907 Epoch 994 from 2000\n",
      "Measure Loss tensor(0.0528, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004175131599244888 Epoch 995 from 2000\n",
      "Measure Loss tensor(0.0559, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026567592560081195 Epoch 996 from 2000\n",
      "Measure Loss tensor(0.0558, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006898788875796171 Epoch 997 from 2000\n",
      "Measure Loss tensor(0.0562, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00024011205090421857 Epoch 998 from 2000\n",
      "Measure Loss tensor(0.0584, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001317844883957275 Epoch 999 from 2000\n",
      "Measure Loss tensor(0.0561, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027230806955647344 Epoch 1000 from 2000\n",
      "Measure Loss tensor(0.0587, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035886750696791114 Epoch 1001 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003053290090736735 Epoch 1002 from 2000\n",
      "Measure Loss tensor(0.0567, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022769763033907856 Epoch 1003 from 2000\n",
      "Measure Loss tensor(0.0530, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022802567088299895 Epoch 1004 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003348709889704484 Epoch 1005 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004366540474854713 Epoch 1006 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0038966474564388616 Epoch 1007 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025410748309487702 Epoch 1008 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015067804160185036 Epoch 1009 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012731852709577459 Epoch 1010 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001187135223591126 Epoch 1011 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007323202470139866 Epoch 1012 from 2000\n",
      "Measure Loss tensor(0.0555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00029946024731290554 Epoch 1013 from 2000\n",
      "Measure Loss tensor(0.0564, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003261632550513016 Epoch 1014 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008866020249473504 Epoch 1015 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001693206137688651 Epoch 1016 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002424976644448906 Epoch 1017 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031303618809376673 Epoch 1018 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003942008093373386 Epoch 1019 from 2000\n",
      "Measure Loss tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004802886762295521 Epoch 1020 from 2000\n",
      "Measure Loss tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00545398402400583 Epoch 1021 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005053833543641619 Epoch 1022 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003632574892256429 Epoch 1023 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00170371910910187 Epoch 1024 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00041423010316225816 Epoch 1025 from 2000\n",
      "Measure Loss tensor(0.0528, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00025136038409480637 Epoch 1026 from 2000\n",
      "Measure Loss tensor(0.0544, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000939993038468619 Epoch 1027 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019957189352233025 Epoch 1028 from 2000\n",
      "Measure Loss tensor(0.0576, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003138803623254888 Epoch 1029 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004471669312214597 Epoch 1030 from 2000\n",
      "Measure Loss tensor(0.0591, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005968875376099162 Epoch 1031 from 2000\n",
      "Measure Loss tensor(0.0529, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0075518633044315536 Epoch 1032 from 2000\n",
      "Measure Loss tensor(0.0577, grad_fn=<AddBackward0>)\n",
      "tensor(0.0084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00836421338850199 Epoch 1033 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007571857326755477 Epoch 1034 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0047053158860395515 Epoch 1035 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016634755276364137 Epoch 1036 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000572887347040426 Epoch 1037 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016425625817053798 Epoch 1038 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031118581298135075 Epoch 1039 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036128847968458348 Epoch 1040 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032403686161337863 Epoch 1041 from 2000\n",
      "Measure Loss tensor(0.0563, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023565111906854916 Epoch 1042 from 2000\n",
      "Measure Loss tensor(0.0563, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014366623190541503 Epoch 1043 from 2000\n",
      "Measure Loss tensor(0.0566, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006509204751087418 Epoch 1044 from 2000\n",
      "Measure Loss tensor(0.0571, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00027402263402499555 Epoch 1045 from 2000\n",
      "Measure Loss tensor(0.0560, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00039991837869270174 Epoch 1046 from 2000\n",
      "Measure Loss tensor(0.0563, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009030222357260894 Epoch 1047 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014163437488889637 Epoch 1048 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001516118432597391 Epoch 1049 from 2000\n",
      "Measure Loss tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011297538861152743 Epoch 1050 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005293056814776113 Epoch 1051 from 2000\n",
      "Measure Loss tensor(0.0530, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017043507114824106 Epoch 1052 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021102735812437745 Epoch 1053 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00046638350594426815 Epoch 1054 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006834332438013719 Epoch 1055 from 2000\n",
      "Measure Loss tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007374669620331879 Epoch 1056 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000663547719175241 Epoch 1057 from 2000\n",
      "Measure Loss tensor(0.0565, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005127029043645742 Epoch 1058 from 2000\n",
      "Measure Loss tensor(0.0556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00033936534661912793 Epoch 1059 from 2000\n",
      "Measure Loss tensor(0.0564, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018723370607545337 Epoch 1060 from 2000\n",
      "Measure Loss tensor(0.0556, grad_fn=<AddBackward0>)\n",
      "tensor(9.9954e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.995420801610044e-05 Epoch 1061 from 2000\n",
      "Measure Loss tensor(0.0555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001051624031526511 Epoch 1062 from 2000\n",
      "Measure Loss tensor(0.0554, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018568073986893603 Epoch 1063 from 2000\n",
      "Measure Loss tensor(0.0544, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000282782417237454 Epoch 1064 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00032750915883253444 Epoch 1065 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00028921596937786926 Epoch 1066 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019389419746166576 Epoch 1067 from 2000\n",
      "Measure Loss tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010297202244251554 Epoch 1068 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(6.1981e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.198079781167917e-05 Epoch 1069 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(7.4675e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.467525924670254e-05 Epoch 1070 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011416537234827018 Epoch 1071 from 2000\n",
      "Measure Loss tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001522457646743401 Epoch 1072 from 2000\n",
      "Measure Loss tensor(0.0552, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001750009043977978 Epoch 1073 from 2000\n",
      "Measure Loss tensor(0.0559, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018063471734181357 Epoch 1074 from 2000\n",
      "Measure Loss tensor(0.0554, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017242778756370843 Epoch 1075 from 2000\n",
      "Measure Loss tensor(0.0556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015288060147567 Epoch 1076 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012587270199709865 Epoch 1077 from 2000\n",
      "Measure Loss tensor(0.0552, grad_fn=<AddBackward0>)\n",
      "tensor(9.5495e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.549502152631588e-05 Epoch 1078 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(6.8858e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.885759900980181e-05 Epoch 1079 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(5.2531e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.253102685434691e-05 Epoch 1080 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(5.0439e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.043886328325884e-05 Epoch 1081 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(6.0453e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.045260156761169e-05 Epoch 1082 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(7.5478e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.547810939183955e-05 Epoch 1083 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(8.7898e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.789759306010428e-05 Epoch 1084 from 2000\n",
      "Measure Loss tensor(0.0552, grad_fn=<AddBackward0>)\n",
      "tensor(9.3371e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.337139545340754e-05 Epoch 1085 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(9.2188e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.218784231030538e-05 Epoch 1086 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(8.6649e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.664873042735274e-05 Epoch 1087 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(7.9450e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.945016252851642e-05 Epoch 1088 from 2000\n",
      "Measure Loss tensor(0.0554, grad_fn=<AddBackward0>)\n",
      "tensor(7.2204e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.220353820686795e-05 Epoch 1089 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(6.5453e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.545345177891643e-05 Epoch 1090 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(5.9140e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.914031586889515e-05 Epoch 1091 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(5.3201e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.3201446676379404e-05 Epoch 1092 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(4.8130e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.8130114364924874e-05 Epoch 1093 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(4.4789e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.4788808809128964e-05 Epoch 1094 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(4.3696e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.369627245734948e-05 Epoch 1095 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(4.4597e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.4597454529698595e-05 Epoch 1096 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(4.6603e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.660312861351783e-05 Epoch 1097 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(4.8833e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.88333407573001e-05 Epoch 1098 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(5.0660e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.0659577238207846e-05 Epoch 1099 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(5.2032e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.2031911970590005e-05 Epoch 1100 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(5.3046e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.3046239016035005e-05 Epoch 1101 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(5.4073e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.407292939082222e-05 Epoch 1102 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(5.5386e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.538585548077079e-05 Epoch 1103 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(5.7082e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.7082026048692364e-05 Epoch 1104 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(5.9072e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.907210098952544e-05 Epoch 1105 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(6.1214e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.121426635767661e-05 Epoch 1106 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(6.3661e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.366052870176149e-05 Epoch 1107 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(6.6398e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.639796115034624e-05 Epoch 1108 from 2000\n",
      "Measure Loss tensor(0.0547, grad_fn=<AddBackward0>)\n",
      "tensor(6.9917e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.991700121969916e-05 Epoch 1109 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(7.4368e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.436751998119156e-05 Epoch 1110 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(8.0484e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.048374664461602e-05 Epoch 1111 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(8.8735e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.873494235066466e-05 Epoch 1112 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010012074484043686 Epoch 1113 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011572189905362143 Epoch 1114 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001376717474277984 Epoch 1115 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016844470708204316 Epoch 1116 from 2000\n",
      "Measure Loss tensor(0.0545, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000212953615035321 Epoch 1117 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002768065090901406 Epoch 1118 from 2000\n",
      "Measure Loss tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00037102737408021115 Epoch 1119 from 2000\n",
      "Measure Loss tensor(0.0551, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005074720514519536 Epoch 1120 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007107519379421789 Epoch 1121 from 2000\n",
      "Measure Loss tensor(0.0552, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010044323335407354 Epoch 1122 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001441555440417853 Epoch 1123 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002058638081389769 Epoch 1124 from 2000\n",
      "Measure Loss tensor(0.0530, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002954716833622639 Epoch 1125 from 2000\n",
      "Measure Loss tensor(0.0552, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004133112982655768 Epoch 1126 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005697326152469435 Epoch 1127 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007362147745481046 Epoch 1128 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00897869148837607 Epoch 1129 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009519725889333454 Epoch 1130 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008685239088234153 Epoch 1131 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0059797514413844075 Epoch 1132 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027692519364814895 Epoch 1133 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000560311062053465 Epoch 1134 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00031920997744529976 Epoch 1135 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017016941145624358 Epoch 1136 from 2000\n",
      "Measure Loss tensor(0.0555, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003830704500267754 Epoch 1137 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006222022200238323 Epoch 1138 from 2000\n",
      "Measure Loss tensor(0.0578, grad_fn=<AddBackward0>)\n",
      "tensor(0.0085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00851306883959601 Epoch 1139 from 2000\n",
      "Measure Loss tensor(0.0526, grad_fn=<AddBackward0>)\n",
      "tensor(0.0106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.010629888090906493 Epoch 1140 from 2000\n",
      "Measure Loss tensor(0.0560, grad_fn=<AddBackward0>)\n",
      "tensor(0.0113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.011270253613264402 Epoch 1141 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009563299207179764 Epoch 1142 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0050760338607347966 Epoch 1143 from 2000\n",
      "Measure Loss tensor(0.0474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011248253585857595 Epoch 1144 from 2000\n",
      "Measure Loss tensor(0.0474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008927919491246348 Epoch 1145 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003493241911559515 Epoch 1146 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005236695457589903 Epoch 1147 from 2000\n",
      "Measure Loss tensor(0.0531, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004231801789546555 Epoch 1148 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019571576174378974 Epoch 1149 from 2000\n",
      "Measure Loss tensor(0.0545, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003309381674154442 Epoch 1150 from 2000\n",
      "Measure Loss tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002610070832115502 Epoch 1151 from 2000\n",
      "Measure Loss tensor(0.0549, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013982320201920845 Epoch 1152 from 2000\n",
      "Measure Loss tensor(0.0574, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002869503685412778 Epoch 1153 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0038357865815144116 Epoch 1154 from 2000\n",
      "Measure Loss tensor(0.0556, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036166504371037476 Epoch 1155 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002303811908023349 Epoch 1156 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007406504517069253 Epoch 1157 from 2000\n",
      "Measure Loss tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017400142194934153 Epoch 1158 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008111243553617377 Epoch 1159 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001611266725741078 Epoch 1160 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016146585857999545 Epoch 1161 from 2000\n",
      "Measure Loss tensor(0.0531, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008953894881010373 Epoch 1162 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023851224581446898 Epoch 1163 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014888720265173415 Epoch 1164 from 2000\n",
      "Measure Loss tensor(0.0557, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005473525674484672 Epoch 1165 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010310309183958345 Epoch 1166 from 2000\n",
      "Measure Loss tensor(0.0562, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012405054829593238 Epoch 1167 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010562105500967615 Epoch 1168 from 2000\n",
      "Measure Loss tensor(0.0547, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006078374774021515 Epoch 1169 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020544304315770672 Epoch 1170 from 2000\n",
      "Measure Loss tensor(0.0528, grad_fn=<AddBackward0>)\n",
      "tensor(9.4762e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.476183626415957e-05 Epoch 1171 from 2000\n",
      "Measure Loss tensor(0.0531, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002680751100730107 Epoch 1172 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004812330767548795 Epoch 1173 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005076378590070431 Epoch 1174 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000342464691801059 Epoch 1175 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015063341545572462 Epoch 1176 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(8.3625e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.362464915336288e-05 Epoch 1177 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015583086970410244 Epoch 1178 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002782446791911096 Epoch 1179 from 2000\n",
      "Measure Loss tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00034776034271550074 Epoch 1180 from 2000\n",
      "Measure Loss tensor(0.0553, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00032041449240951045 Epoch 1181 from 2000\n",
      "Measure Loss tensor(0.0543, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021902639776786694 Epoch 1182 from 2000\n",
      "Measure Loss tensor(0.0545, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010954265600624424 Epoch 1183 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(5.4547e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.454673094118236e-05 Epoch 1184 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(7.2502e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.250173287094967e-05 Epoch 1185 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013036597388279002 Epoch 1186 from 2000\n",
      "Measure Loss tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017247433104307237 Epoch 1187 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016617548092213825 Epoch 1188 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011934672374569675 Epoch 1189 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(6.7788e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.778817550832566e-05 Epoch 1190 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(4.2774e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.2774207441904814e-05 Epoch 1191 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(5.2011e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.201149759286925e-05 Epoch 1192 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(8.1218e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.121808675158705e-05 Epoch 1193 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010790706024667266 Epoch 1194 from 2000\n",
      "Measure Loss tensor(0.0547, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001163634336806892 Epoch 1195 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010329047445009935 Epoch 1196 from 2000\n",
      "Measure Loss tensor(0.0544, grad_fn=<AddBackward0>)\n",
      "tensor(7.7858e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.785813878618752e-05 Epoch 1197 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(5.3534e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.353419295007006e-05 Epoch 1198 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(4.1217e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.1216573292144815e-05 Epoch 1199 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(4.3295e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.329502395195322e-05 Epoch 1200 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(5.4162e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.416196983313273e-05 Epoch 1201 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(6.4765e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.47649592267757e-05 Epoch 1202 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(6.8396e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.839589516095386e-05 Epoch 1203 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(6.3830e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.383018065143242e-05 Epoch 1204 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(5.4259e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.425876292242111e-05 Epoch 1205 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(4.4813e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.4812795900102284e-05 Epoch 1206 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(3.9223e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.9223235166850796e-05 Epoch 1207 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(3.8692e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.86919495684549e-05 Epoch 1208 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(4.1855e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.1855118962184326e-05 Epoch 1209 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(4.6316e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.6316424961011654e-05 Epoch 1210 from 2000\n",
      "Measure Loss tensor(0.0540, grad_fn=<AddBackward0>)\n",
      "tensor(4.9572e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.957196148432278e-05 Epoch 1211 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(5.0224e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.0224286078161274e-05 Epoch 1212 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(4.8098e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.809810644817162e-05 Epoch 1213 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(4.4121e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.412079931029936e-05 Epoch 1214 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(3.9840e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.984001127651377e-05 Epoch 1215 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.6461e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.6460628637567254e-05 Epoch 1216 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.4833e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.483285859249433e-05 Epoch 1217 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(3.4887e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.488683540100519e-05 Epoch 1218 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.6140e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.614020692650296e-05 Epoch 1219 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(3.7677e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.767678929702867e-05 Epoch 1220 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.8761e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.8761006226278435e-05 Epoch 1221 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(3.8988e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.8987897601805896e-05 Epoch 1222 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.8355e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.835515866116477e-05 Epoch 1223 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(3.7031e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.703126262385525e-05 Epoch 1224 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.5460e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.546002798676729e-05 Epoch 1225 from 2000\n",
      "Measure Loss tensor(0.0537, grad_fn=<AddBackward0>)\n",
      "tensor(3.4014e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.4014272263922606e-05 Epoch 1226 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.2891e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.289142655602587e-05 Epoch 1227 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.2247e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.224689350288884e-05 Epoch 1228 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.2039e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.2039020061631754e-05 Epoch 1229 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(3.2164e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.216402070916478e-05 Epoch 1230 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.2479e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.2479211219915106e-05 Epoch 1231 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(3.2834e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.283390864126002e-05 Epoch 1232 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.3170e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.316960374816296e-05 Epoch 1233 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(3.3403e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.340308407710969e-05 Epoch 1234 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.3513e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.351334145656622e-05 Epoch 1235 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(3.3586e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.358560169084989e-05 Epoch 1236 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.3666e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.366615211174921e-05 Epoch 1237 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(3.3923e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.3923307612296604e-05 Epoch 1238 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(3.4473e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.4473181008022075e-05 Epoch 1239 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(3.5530e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.552998523819572e-05 Epoch 1240 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(3.7417e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.741744048793397e-05 Epoch 1241 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(4.0470e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.046950937915322e-05 Epoch 1242 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(4.5490e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.54895958923146e-05 Epoch 1243 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(5.3304e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.330393384235154e-05 Epoch 1244 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(6.6017e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.601733195904451e-05 Epoch 1245 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(8.5870e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.587021224990597e-05 Epoch 1246 from 2000\n",
      "Measure Loss tensor(0.0535, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011857206164423197 Epoch 1247 from 2000\n",
      "Measure Loss tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016992766838904764 Epoch 1248 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002564971190359332 Epoch 1249 from 2000\n",
      "Measure Loss tensor(0.0531, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00039229456304678043 Epoch 1250 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006263768464524169 Epoch 1251 from 2000\n",
      "Measure Loss tensor(0.0528, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009837442165262064 Epoch 1252 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016064979593700907 Epoch 1253 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024766700729658957 Epoch 1254 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003944373981886259 Epoch 1255 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005496730018914531 Epoch 1256 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007658187124203368 Epoch 1257 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0080762660272946 Epoch 1258 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007481348690635306 Epoch 1259 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00420518615028746 Epoch 1260 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014237860943408908 Epoch 1261 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009205436147171333 Epoch 1262 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00238642403130061 Epoch 1263 from 2000\n",
      "Measure Loss tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003993486435355521 Epoch 1264 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036958775660997116 Epoch 1265 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002467131218042437 Epoch 1266 from 2000\n",
      "Measure Loss tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008599296386697168 Epoch 1267 from 2000\n",
      "Measure Loss tensor(0.0530, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015764703543779855 Epoch 1268 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005375303304606191 Epoch 1269 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013843127371358074 Epoch 1270 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019085616210847167 Epoch 1271 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014890239511398896 Epoch 1272 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007435640512274441 Epoch 1273 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00035551060917678873 Epoch 1274 from 2000\n",
      "Measure Loss tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006423843521650031 Epoch 1275 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011181943058502296 Epoch 1276 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001141729736666368 Epoch 1277 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007476123113012814 Epoch 1278 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003305423742706612 Epoch 1279 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00028325975507142347 Epoch 1280 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000575434990291413 Epoch 1281 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008923973495278378 Epoch 1282 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010401190706570875 Epoch 1283 from 2000\n",
      "Measure Loss tensor(0.0510, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009790618117426135 Epoch 1284 from 2000\n",
      "Measure Loss tensor(0.0528, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009991621329406457 Epoch 1285 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001300926301253482 Epoch 1286 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018721930877198935 Epoch 1287 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0025586886114150136 Epoch 1288 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003106458921995577 Epoch 1289 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003555117529697965 Epoch 1290 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00384826550040042 Epoch 1291 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004131457574022324 Epoch 1292 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004158869631257487 Epoch 1293 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003896206722567432 Epoch 1294 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003186133585634971 Epoch 1295 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002249690442494271 Epoch 1296 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013018182589971247 Epoch 1297 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006245574964985445 Epoch 1298 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002729548305138123 Epoch 1299 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019083174120222734 Epoch 1300 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002772378631947396 Epoch 1301 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004885861058717201 Epoch 1302 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008237567825081543 Epoch 1303 from 2000\n",
      "Measure Loss tensor(0.0541, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012953232724858144 Epoch 1304 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018948625568752226 Epoch 1305 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002498839588864117 Epoch 1306 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002983603756740609 Epoch 1307 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030806118839922145 Epoch 1308 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027495720168820878 Epoch 1309 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0019689493799423478 Epoch 1310 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010729069541523687 Epoch 1311 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00036067300336753677 Epoch 1312 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(5.7623e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.762293643141234e-05 Epoch 1313 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016603435236736352 Epoch 1314 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005633022637497229 Epoch 1315 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011305418195659003 Epoch 1316 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017834934400268458 Epoch 1317 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002503420371354252 Epoch 1318 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0031539476661443635 Epoch 1319 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035997378970397775 Epoch 1320 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0034855504383294504 Epoch 1321 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027714485728674763 Epoch 1322 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015982069269983685 Epoch 1323 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005674571826233472 Epoch 1324 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012722681700423304 Epoch 1325 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003474127493119563 Epoch 1326 from 2000\n",
      "Measure Loss tensor(0.0510, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009697257718136484 Epoch 1327 from 2000\n",
      "Measure Loss tensor(0.0539, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016909456810114648 Epoch 1328 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002371146382650302 Epoch 1329 from 2000\n",
      "Measure Loss tensor(0.0548, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002899957443515352 Epoch 1330 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032695641522288753 Epoch 1331 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032801898990924097 Epoch 1332 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002865229329020811 Epoch 1333 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001963639304567612 Epoch 1334 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000933673223950745 Epoch 1335 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022917339384827655 Epoch 1336 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001411865870775408 Epoch 1337 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005553657811138483 Epoch 1338 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001131323650022274 Epoch 1339 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015998760320184249 Epoch 1340 from 2000\n",
      "Measure Loss tensor(0.0538, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018170260594132317 Epoch 1341 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018019350788981783 Epoch 1342 from 2000\n",
      "Measure Loss tensor(0.0536, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015582066494701035 Epoch 1343 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011725522078387123 Epoch 1344 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007141374391536425 Epoch 1345 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003162343224737583 Epoch 1346 from 2000\n",
      "Measure Loss tensor(0.0510, grad_fn=<AddBackward0>)\n",
      "tensor(8.7033e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.70331075272072e-05 Epoch 1347 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(7.1882e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.188194784431296e-05 Epoch 1348 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002170776280927535 Epoch 1349 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00041022493524420234 Epoch 1350 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005538111861773704 Epoch 1351 from 2000\n",
      "Measure Loss tensor(0.0526, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005977694828421905 Epoch 1352 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000550935694985061 Epoch 1353 from 2000\n",
      "Measure Loss tensor(0.0530, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004394026365059941 Epoch 1354 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00030400784824720394 Epoch 1355 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017625736439954703 Epoch 1356 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(8.2711e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.27109563822618e-05 Epoch 1357 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(3.6416e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.641569076778756e-05 Epoch 1358 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(3.6916e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.691608706206815e-05 Epoch 1359 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(7.0624e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.062434739824326e-05 Epoch 1360 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011691751095390627 Epoch 1361 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015645817209828598 Epoch 1362 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017660962151945967 Epoch 1363 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017513750184319145 Epoch 1364 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015544966340265109 Epoch 1365 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012558373664697242 Epoch 1366 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(9.2923e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.292281512043036e-05 Epoch 1367 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(6.3821e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.382105984535855e-05 Epoch 1368 from 2000\n",
      "Measure Loss tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "tensor(4.2089e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.2089063623033975e-05 Epoch 1369 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(2.9326e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.932606693769753e-05 Epoch 1370 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(2.5171e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.5171087949103194e-05 Epoch 1371 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(2.7925e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.7925170728336556e-05 Epoch 1372 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(3.5177e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.51773993411066e-05 Epoch 1373 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(4.4331e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.4331247525754104e-05 Epoch 1374 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(5.3216e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.321564591054125e-05 Epoch 1375 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(6.0199e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.01987104274318e-05 Epoch 1376 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(6.4608e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.460758054392542e-05 Epoch 1377 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(6.6432e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.643194026464026e-05 Epoch 1378 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(6.6198e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.619758905503182e-05 Epoch 1379 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(6.4450e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.444967415954284e-05 Epoch 1380 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(6.1792e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.179204033791719e-05 Epoch 1381 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(5.8545e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.854473141609469e-05 Epoch 1382 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(5.4993e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.499347933390042e-05 Epoch 1383 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(5.1341e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.134120118352639e-05 Epoch 1384 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(4.7838e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.783758100737696e-05 Epoch 1385 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(4.4599e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.459858420789419e-05 Epoch 1386 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(4.1781e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.1780880511543665e-05 Epoch 1387 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(3.9318e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.931788194823042e-05 Epoch 1388 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(3.7314e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.731441258018618e-05 Epoch 1389 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(3.5712e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.571245306761821e-05 Epoch 1390 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(3.4581e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.458095780900466e-05 Epoch 1391 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(3.3895e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.389500734118197e-05 Epoch 1392 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(3.3760e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.375979396700184e-05 Epoch 1393 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(3.4149e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.414911860441155e-05 Epoch 1394 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(3.5131e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.513054398773885e-05 Epoch 1395 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(3.6762e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.6762451510469025e-05 Epoch 1396 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(3.9296e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.9295829264989314e-05 Epoch 1397 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(4.2956e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.295628993764239e-05 Epoch 1398 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(4.8239e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.823948132629972e-05 Epoch 1399 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(5.5688e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.5687875772949695e-05 Epoch 1400 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(6.6264e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.626393280477373e-05 Epoch 1401 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(8.1212e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.121243964304001e-05 Epoch 1402 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010276760801497913 Epoch 1403 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001339297632052592 Epoch 1404 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017971650130122087 Epoch 1405 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00024688725365665487 Epoch 1406 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003473451042834487 Epoch 1407 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004961782810705217 Epoch 1408 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00072124254308304 Epoch 1409 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010542857804702386 Epoch 1410 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015563909055895618 Epoch 1411 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022789547893862093 Epoch 1412 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033298445489195307 Epoch 1413 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0047081363171934485 Epoch 1414 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006475099218013772 Epoch 1415 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008193781846278975 Epoch 1416 from 2000\n",
      "Measure Loss tensor(0.0472, grad_fn=<AddBackward0>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009530418315884422 Epoch 1417 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009277620974634006 Epoch 1418 from 2000\n",
      "Measure Loss tensor(0.0457, grad_fn=<AddBackward0>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007334644069062028 Epoch 1419 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003989992416470754 Epoch 1420 from 2000\n",
      "Measure Loss tensor(0.0464, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011503641192959272 Epoch 1421 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000157141554530584 Epoch 1422 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011078636679163704 Epoch 1423 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032673400115074435 Epoch 1424 from 2000\n",
      "Measure Loss tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006010932683582044 Epoch 1425 from 2000\n",
      "Measure Loss tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.009304044820005415 Epoch 1426 from 2000\n",
      "Measure Loss tensor(0.0545, grad_fn=<AddBackward0>)\n",
      "tensor(0.0125, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.012541611531681948 Epoch 1427 from 2000\n",
      "Measure Loss tensor(0.0474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.014684755203179203 Epoch 1428 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.0124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.012413540602272281 Epoch 1429 from 2000\n",
      "Measure Loss tensor(0.0430, grad_fn=<AddBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0062262533541581865 Epoch 1430 from 2000\n",
      "Measure Loss tensor(0.0439, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00116491352004204 Epoch 1431 from 2000\n",
      "Measure Loss tensor(0.0443, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002559735916297173 Epoch 1432 from 2000\n",
      "Measure Loss tensor(0.0436, grad_fn=<AddBackward0>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006655822131902107 Epoch 1433 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007053995986255934 Epoch 1434 from 2000\n",
      "Measure Loss tensor(0.0478, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003992508785757196 Epoch 1435 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009073034345365726 Epoch 1436 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002442973344426334 Epoch 1437 from 2000\n",
      "Measure Loss tensor(0.0530, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018281879666954735 Epoch 1438 from 2000\n",
      "Measure Loss tensor(0.0550, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0042766318977055726 Epoch 1439 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006084940026726271 Epoch 1440 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0057489891963044245 Epoch 1441 from 2000\n",
      "Measure Loss tensor(0.0477, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0032902163269540987 Epoch 1442 from 2000\n",
      "Measure Loss tensor(0.0475, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007039714141512249 Epoch 1443 from 2000\n",
      "Measure Loss tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006010206844375961 Epoch 1444 from 2000\n",
      "Measure Loss tensor(0.0458, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023190651023329775 Epoch 1445 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0029420165440429535 Epoch 1446 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016980098138325963 Epoch 1447 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002895143055050169 Epoch 1448 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021134787146239107 Epoch 1449 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001229148173614364 Epoch 1450 from 2000\n",
      "Measure Loss tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0022438191835580774 Epoch 1451 from 2000\n",
      "Measure Loss tensor(0.0524, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024461068951188145 Epoch 1452 from 2000\n",
      "Measure Loss tensor(0.0532, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0016831132779770472 Epoch 1453 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005982137668012748 Epoch 1454 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(7.2659e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.265867697816571e-05 Epoch 1455 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00041951724400039053 Epoch 1456 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009880072320892735 Epoch 1457 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009892631104642185 Epoch 1458 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00047311658279246033 Epoch 1459 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(7.4457e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.44566155877713e-05 Epoch 1460 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016423609605786265 Epoch 1461 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005294720720228406 Epoch 1462 from 2000\n",
      "Measure Loss tensor(0.0534, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007566708829769038 Epoch 1463 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006573124088752244 Epoch 1464 from 2000\n",
      "Measure Loss tensor(0.0529, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00034371143702827296 Epoch 1465 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(8.5143e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.514320827959327e-05 Epoch 1466 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(6.1452e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.145158099191947e-05 Epoch 1467 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022083707269586993 Epoch 1468 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00035208713373990696 Epoch 1469 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003091113687019124 Epoch 1470 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015126803685964834 Epoch 1471 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(4.2818e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.281843118202066e-05 Epoch 1472 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(6.7462e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.746165484428735e-05 Epoch 1473 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016617494178946241 Epoch 1474 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022818739538546718 Epoch 1475 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020078321970083668 Epoch 1476 from 2000\n",
      "Measure Loss tensor(0.0525, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011582193398343568 Epoch 1477 from 2000\n",
      "Measure Loss tensor(0.0522, grad_fn=<AddBackward0>)\n",
      "tensor(4.5055e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.505544630043512e-05 Epoch 1478 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(3.5284e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.528388496577161e-05 Epoch 1479 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(7.6207e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.620749565780419e-05 Epoch 1480 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011800372593603455 Epoch 1481 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011940044234969819 Epoch 1482 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(8.2082e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.208150689622011e-05 Epoch 1483 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(4.1302e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.1302111584143996e-05 Epoch 1484 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(2.8887e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8886981258584225e-05 Epoch 1485 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(4.6548e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.654771639207146e-05 Epoch 1486 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(7.1531e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.153057378130113e-05 Epoch 1487 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(8.0632e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.063182805703942e-05 Epoch 1488 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(6.7870e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.786973430801275e-05 Epoch 1489 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(4.4962e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.496164526256612e-05 Epoch 1490 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(2.8587e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8586869389414403e-05 Epoch 1491 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(2.7565e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.7565394668350987e-05 Epoch 1492 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(3.8014e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.801351527771808e-05 Epoch 1493 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(4.8632e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.8632453464745095e-05 Epoch 1494 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(5.0324e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.032366731911693e-05 Epoch 1495 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(4.2405e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.2404972614057296e-05 Epoch 1496 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(3.1410e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.1409570635543826e-05 Epoch 1497 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(2.4826e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.4826219395470205e-05 Epoch 1498 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(2.5494e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.549361684532474e-05 Epoch 1499 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(3.0865e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.086483676724559e-05 Epoch 1500 from 2000\n",
      "Measure Loss tensor(0.0517, grad_fn=<AddBackward0>)\n",
      "tensor(3.5705e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.5704728194788996e-05 Epoch 1501 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(3.6450e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.645007064263425e-05 Epoch 1502 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(3.2828e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.282788574122547e-05 Epoch 1503 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(2.7446e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.7446158355188443e-05 Epoch 1504 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(2.3435e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.343473567598676e-05 Epoch 1505 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(2.2563e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.256302227230817e-05 Epoch 1506 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.4352e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.4352321503734673e-05 Epoch 1507 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.6841e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.684136124379048e-05 Epoch 1508 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(2.8115e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8114574265886297e-05 Epoch 1509 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.7316e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.7315967164782918e-05 Epoch 1510 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.5033e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.503262792241286e-05 Epoch 1511 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.2580e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.2579902078462775e-05 Epoch 1512 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.1155e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.1154678586211224e-05 Epoch 1513 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.1138e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.1138440634206023e-05 Epoch 1514 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(2.2060e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.2059660805341615e-05 Epoch 1515 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.3105e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.3104794219377335e-05 Epoch 1516 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.3545e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.3544670145807526e-05 Epoch 1517 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(2.3112e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.3111958968339443e-05 Epoch 1518 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(2.2089e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.2089250299503995e-05 Epoch 1519 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(2.1009e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.1009245635289532e-05 Epoch 1520 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(2.0391e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.039134943685774e-05 Epoch 1521 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(2.0387e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.0386991493447553e-05 Epoch 1522 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(2.0941e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.0941386413629096e-05 Epoch 1523 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(2.1793e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.17932695950599e-05 Epoch 1524 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(2.2836e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.283615620455646e-05 Epoch 1525 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(2.4012e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.4012012677339196e-05 Epoch 1526 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(2.5623e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.5622890058114393e-05 Epoch 1527 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(2.8187e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8187395490999406e-05 Epoch 1528 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(3.2595e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.259464835248358e-05 Epoch 1529 from 2000\n",
      "Measure Loss tensor(0.0510, grad_fn=<AddBackward0>)\n",
      "tensor(3.9947e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.994737323308627e-05 Epoch 1530 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(5.2399e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.239858594062986e-05 Epoch 1531 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(7.2425e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.242452604546809e-05 Epoch 1532 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010612552940291573 Epoch 1533 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015958152227112143 Epoch 1534 from 2000\n",
      "Measure Loss tensor(0.0514, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002516763236721606 Epoch 1535 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00039599981487888937 Epoch 1536 from 2000\n",
      "Measure Loss tensor(0.0516, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006520202679455093 Epoch 1537 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001036834640074743 Epoch 1538 from 2000\n",
      "Measure Loss tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017308570672335444 Epoch 1539 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002654290848405871 Epoch 1540 from 2000\n",
      "Measure Loss tensor(0.0519, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004267781139418927 Epoch 1541 from 2000\n",
      "Measure Loss tensor(0.0476, grad_fn=<AddBackward0>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00572680080942415 Epoch 1542 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007757793521184789 Epoch 1543 from 2000\n",
      "Measure Loss tensor(0.0458, grad_fn=<AddBackward0>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0073552238239836774 Epoch 1544 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005864404331163065 Epoch 1545 from 2000\n",
      "Measure Loss tensor(0.0454, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002347026574123545 Epoch 1546 from 2000\n",
      "Measure Loss tensor(0.0473, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003087336429905527 Epoch 1547 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000892728165276552 Epoch 1548 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0028370619311201177 Epoch 1549 from 2000\n",
      "Measure Loss tensor(0.0527, grad_fn=<AddBackward0>)\n",
      "tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004506914637106824 Epoch 1550 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004170773903772942 Epoch 1551 from 2000\n",
      "Measure Loss tensor(0.0529, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0029131637555113066 Epoch 1552 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010791083344010933 Epoch 1553 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020020314059585186 Epoch 1554 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006153657700827287 Epoch 1555 from 2000\n",
      "Measure Loss tensor(0.0480, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014729686147403561 Epoch 1556 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001693595621068727 Epoch 1557 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009172132000636697 Epoch 1558 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00018394680845636635 Epoch 1559 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001929569358637613 Epoch 1560 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007380990812674949 Epoch 1561 from 2000\n",
      "Measure Loss tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001135693387821751 Epoch 1562 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009336653326065689 Epoch 1563 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044918128928024895 Epoch 1564 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010217850842826676 Epoch 1565 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015328183141812444 Epoch 1566 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004298066632026644 Epoch 1567 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005672516936632657 Epoch 1568 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044580546193130166 Epoch 1569 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001846294987408067 Epoch 1570 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(5.1259e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.1259282740829296e-05 Epoch 1571 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011785922830321266 Epoch 1572 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002651370177431619 Epoch 1573 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000345878226345804 Epoch 1574 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002921769374971135 Epoch 1575 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001769009979159458 Epoch 1576 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(7.8866e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.886634317492278e-05 Epoch 1577 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(5.5052e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.50521700080907e-05 Epoch 1578 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(9.4861e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.486070032186942e-05 Epoch 1579 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014740270623296253 Epoch 1580 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016827187839924608 Epoch 1581 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013869036518691973 Epoch 1582 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(8.5145e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.514528625922105e-05 Epoch 1583 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(4.0719e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.0719094143542416e-05 Epoch 1584 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(2.9630e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.963024658690223e-05 Epoch 1585 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(5.0538e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.053788789164765e-05 Epoch 1586 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(8.2091e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.209053574069093e-05 Epoch 1587 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010094501544902004 Epoch 1588 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(9.2858e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.28577357248393e-05 Epoch 1589 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(6.5347e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.534665498037935e-05 Epoch 1590 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(3.5188e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.5188431698611214e-05 Epoch 1591 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(2.0068e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.0067781941469775e-05 Epoch 1592 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(2.5168e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.516797673139754e-05 Epoch 1593 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(4.1869e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.186940375623483e-05 Epoch 1594 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(5.6111e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.61112933307179e-05 Epoch 1595 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(5.7873e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.787312931994827e-05 Epoch 1596 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(4.8083e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.808342550462518e-05 Epoch 1597 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(3.3693e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.369340721474371e-05 Epoch 1598 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(2.3298e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.3297900974349127e-05 Epoch 1599 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(2.0859e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.0859187996537376e-05 Epoch 1600 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(2.5232e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.5231737468112642e-05 Epoch 1601 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(3.2333e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.2333025459802246e-05 Epoch 1602 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(3.7999e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.799853562055904e-05 Epoch 1603 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(4.0415e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.041492923485331e-05 Epoch 1604 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(3.9921e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.9920967695999664e-05 Epoch 1605 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(3.8736e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.8735611491789497e-05 Epoch 1606 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(3.9170e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.917027749207194e-05 Epoch 1607 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(4.3310e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.330983316419657e-05 Epoch 1608 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(5.2367e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.236708296507266e-05 Epoch 1609 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(6.7278e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.72780458180972e-05 Epoch 1610 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(8.8813e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.881257980763297e-05 Epoch 1611 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011893404751369815 Epoch 1612 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016068805007982305 Epoch 1613 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022141493016369318 Epoch 1614 from 2000\n",
      "Measure Loss tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003099645537791102 Epoch 1615 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004452358187558289 Epoch 1616 from 2000\n",
      "Measure Loss tensor(0.0510, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006443794850034543 Epoch 1617 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009495277813786655 Epoch 1618 from 2000\n",
      "Measure Loss tensor(0.0511, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013857700958915206 Epoch 1619 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002040053054783171 Epoch 1620 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002911522833097664 Epoch 1621 from 2000\n",
      "Measure Loss tensor(0.0475, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00413538422502848 Epoch 1622 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005483505491656971 Epoch 1623 from 2000\n",
      "Measure Loss tensor(0.0461, grad_fn=<AddBackward0>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006997329909474888 Epoch 1624 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0077539921678454 Epoch 1625 from 2000\n",
      "Measure Loss tensor(0.0448, grad_fn=<AddBackward0>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0076505700210954585 Epoch 1626 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005796793378774901 Epoch 1627 from 2000\n",
      "Measure Loss tensor(0.0451, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003268764386712025 Epoch 1628 from 2000\n",
      "Measure Loss tensor(0.0470, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010796879360559497 Epoch 1629 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002814061724969733 Epoch 1630 from 2000\n",
      "Measure Loss tensor(0.0480, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008340870083396642 Epoch 1631 from 2000\n",
      "Measure Loss tensor(0.0520, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021750266712271325 Epoch 1632 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003965347899979764 Epoch 1633 from 2000\n",
      "Measure Loss tensor(0.0533, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005951835471163394 Epoch 1634 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(0.0082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.008176858561632699 Epoch 1635 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00948046643651679 Epoch 1636 from 2000\n",
      "Measure Loss tensor(0.0456, grad_fn=<AddBackward0>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00889519063716326 Epoch 1637 from 2000\n",
      "Measure Loss tensor(0.0454, grad_fn=<AddBackward0>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005492943462949154 Epoch 1638 from 2000\n",
      "Measure Loss tensor(0.0439, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018568963965667228 Epoch 1639 from 2000\n",
      "Measure Loss tensor(0.0435, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009415703511350999 Epoch 1640 from 2000\n",
      "Measure Loss tensor(0.0464, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002742885526369443 Epoch 1641 from 2000\n",
      "Measure Loss tensor(0.0466, grad_fn=<AddBackward0>)\n",
      "tensor(0.0048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004786823298706178 Epoch 1642 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005367152537551624 Epoch 1643 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004740341613032031 Epoch 1644 from 2000\n",
      "Measure Loss tensor(0.0513, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033697005830883218 Epoch 1645 from 2000\n",
      "Measure Loss tensor(0.0507, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0020561843660936675 Epoch 1646 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001007891621107369 Epoch 1647 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006194067621083899 Epoch 1648 from 2000\n",
      "Measure Loss tensor(0.0474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007980224801654328 Epoch 1649 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012008427954040336 Epoch 1650 from 2000\n",
      "Measure Loss tensor(0.0471, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013631684271990234 Epoch 1651 from 2000\n",
      "Measure Loss tensor(0.0478, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001055706520239813 Epoch 1652 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005806516036833766 Epoch 1653 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002875922070969073 Epoch 1654 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00037293169987555243 Epoch 1655 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006772583542928953 Epoch 1656 from 2000\n",
      "Measure Loss tensor(0.0508, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009172871243357016 Epoch 1657 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009220815324073425 Epoch 1658 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000661529740049412 Epoch 1659 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003305213150470553 Epoch 1660 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012405966297026662 Epoch 1661 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015646333532396716 Epoch 1662 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003279452639193459 Epoch 1663 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00044054336477367166 Epoch 1664 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00039546570926383946 Epoch 1665 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00023683814101822635 Epoch 1666 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010000039423010705 Epoch 1667 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(6.3538e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.353829837739153e-05 Epoch 1668 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001225563827443174 Epoch 1669 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021107696799671947 Epoch 1670 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00025622369553307366 Epoch 1671 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022836318343505402 Epoch 1672 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014578283811798203 Epoch 1673 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(6.5435e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.543456768685619e-05 Epoch 1674 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(3.3056e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.305611362871107e-05 Epoch 1675 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(5.6310e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.631027592502327e-05 Epoch 1676 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010423127163859206 Epoch 1677 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001356275893657006 Epoch 1678 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012975353962053535 Epoch 1679 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(9.2570e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.256953417429556e-05 Epoch 1680 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(4.9276e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.927593222269242e-05 Epoch 1681 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(2.3265e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.326451550491196e-05 Epoch 1682 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(2.4197e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.4197352896319655e-05 Epoch 1683 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(4.4295e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.429499590636701e-05 Epoch 1684 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(6.5987e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.598718018400299e-05 Epoch 1685 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(7.3997e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.399678318807047e-05 Epoch 1686 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(6.3942e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.394188214853852e-05 Epoch 1687 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(4.3561e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.356128639830758e-05 Epoch 1688 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(2.5024e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.502439921262921e-05 Epoch 1689 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(1.7164e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7163993834897202e-05 Epoch 1690 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(2.0922e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.092235155399243e-05 Epoch 1691 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(3.0953e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.0952510288841304e-05 Epoch 1692 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(4.0028e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.002843521182978e-05 Epoch 1693 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(4.3159e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.3159244929457736e-05 Epoch 1694 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(3.9497e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.94973098097524e-05 Epoch 1695 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(3.1426e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.142635420422763e-05 Epoch 1696 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(2.2911e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.291118662311325e-05 Epoch 1697 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(1.7284e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7283995402768343e-05 Epoch 1698 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(1.5949e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5948552563995987e-05 Epoch 1699 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(1.8236e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.8235929753405468e-05 Epoch 1700 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(2.2058e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.2057530474345703e-05 Epoch 1701 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(2.5147e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.51467274894877e-05 Epoch 1702 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(2.5989e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.59892319062628e-05 Epoch 1703 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(2.4427e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.4426738288427056e-05 Epoch 1704 from 2000\n",
      "Measure Loss tensor(0.0500, grad_fn=<AddBackward0>)\n",
      "tensor(2.1256e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.125628075329207e-05 Epoch 1705 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(1.7857e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7856683246194166e-05 Epoch 1706 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(1.5394e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5394350759458305e-05 Epoch 1707 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(1.4424e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4423740569447243e-05 Epoch 1708 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(1.4823e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4822618781801517e-05 Epoch 1709 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(1.5940e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.594020212560907e-05 Epoch 1710 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.7056e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7055503257694532e-05 Epoch 1711 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.7597e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7597373101354968e-05 Epoch 1712 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.7435e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7434865153400596e-05 Epoch 1713 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.6646e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.6645907628607702e-05 Epoch 1714 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.5548e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5547586054633688e-05 Epoch 1715 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.4478e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4477849740027786e-05 Epoch 1716 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.3684e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3684239459771749e-05 Epoch 1717 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.3284e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3283596610607351e-05 Epoch 1718 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.3226e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3226347940108201e-05 Epoch 1719 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.3378e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.337798782905608e-05 Epoch 1720 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.3597e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3596551760985495e-05 Epoch 1721 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.3761e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3761154039440099e-05 Epoch 1722 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.3731e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.373143556594031e-05 Epoch 1723 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.3564e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3564170169368084e-05 Epoch 1724 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.3249e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3248710432361215e-05 Epoch 1725 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.2864e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2864154359329032e-05 Epoch 1726 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.2475e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.247524350612057e-05 Epoch 1727 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.2103e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2102626271323726e-05 Epoch 1728 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.1824e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1824239993698169e-05 Epoch 1729 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.1626e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1625707292573029e-05 Epoch 1730 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(1.1547e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.154668301222087e-05 Epoch 1731 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1536e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1536126037333132e-05 Epoch 1732 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1548e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1548192613570057e-05 Epoch 1733 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1584e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1583571562085688e-05 Epoch 1734 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1587e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1587317586634861e-05 Epoch 1735 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1586e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1585844252510905e-05 Epoch 1736 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1555e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1554633183957753e-05 Epoch 1737 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1511e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1510672853967172e-05 Epoch 1738 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1424e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1424345693074077e-05 Epoch 1739 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1350e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1349976413670404e-05 Epoch 1740 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1256e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1256153016727477e-05 Epoch 1741 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.1173e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1172867476014699e-05 Epoch 1742 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1099e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1099248731336898e-05 Epoch 1743 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.1039e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1038761591614482e-05 Epoch 1744 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.1001e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.100108242547502e-05 Epoch 1745 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.1009e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1009276212193823e-05 Epoch 1746 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.1051e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.105063094265478e-05 Epoch 1747 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.1153e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1153156373665586e-05 Epoch 1748 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.1368e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1368185943645643e-05 Epoch 1749 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(1.1698e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1698333581290253e-05 Epoch 1750 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.2202e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2201630229434085e-05 Epoch 1751 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(1.2963e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2962897527410114e-05 Epoch 1752 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(1.4098e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4098131869408893e-05 Epoch 1753 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(1.5782e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5782472505076616e-05 Epoch 1754 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(1.8290e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.8290476857510716e-05 Epoch 1755 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(2.1993e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.1993073966181657e-05 Epoch 1756 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(2.7534e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.7534338839463565e-05 Epoch 1757 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(3.5859e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.5859423274236844e-05 Epoch 1758 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(4.8492e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.849219713492415e-05 Epoch 1759 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(6.7736e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.773554314217879e-05 Epoch 1760 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(9.7466e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.746550852983952e-05 Epoch 1761 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014342586458872456 Epoch 1762 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021540780992650778 Epoch 1763 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00032761935802623206 Epoch 1764 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005044722470206062 Epoch 1765 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007791336447281487 Epoch 1766 from 2000\n",
      "Measure Loss tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0012069420378581746 Epoch 1767 from 2000\n",
      "Measure Loss tensor(0.0474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018508835387645126 Epoch 1768 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027942341419252806 Epoch 1769 from 2000\n",
      "Measure Loss tensor(0.0462, grad_fn=<AddBackward0>)\n",
      "tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004072244152520332 Epoch 1770 from 2000\n",
      "Measure Loss tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0055883246303390114 Epoch 1771 from 2000\n",
      "Measure Loss tensor(0.0446, grad_fn=<AddBackward0>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007008753916570744 Epoch 1772 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.007483662318687084 Epoch 1773 from 2000\n",
      "Measure Loss tensor(0.0435, grad_fn=<AddBackward0>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006591134388946346 Epoch 1774 from 2000\n",
      "Measure Loss tensor(0.0468, grad_fn=<AddBackward0>)\n",
      "tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004175076577743881 Epoch 1775 from 2000\n",
      "Measure Loss tensor(0.0450, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002058403467021639 Epoch 1776 from 2000\n",
      "Measure Loss tensor(0.0456, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015245452841338005 Epoch 1777 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002705043614000156 Epoch 1778 from 2000\n",
      "Measure Loss tensor(0.0464, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0043167385499884685 Epoch 1779 from 2000\n",
      "Measure Loss tensor(0.0521, grad_fn=<AddBackward0>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005926305044856116 Epoch 1780 from 2000\n",
      "Measure Loss tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.006004057631541656 Epoch 1781 from 2000\n",
      "Measure Loss tensor(0.0503, grad_fn=<AddBackward0>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0055899033183039225 Epoch 1782 from 2000\n",
      "Measure Loss tensor(0.0460, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003626361754942453 Epoch 1783 from 2000\n",
      "Measure Loss tensor(0.0457, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002294235291690671 Epoch 1784 from 2000\n",
      "Measure Loss tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021619857284492635 Epoch 1785 from 2000\n",
      "Measure Loss tensor(0.0437, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0029825925298707675 Epoch 1786 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003039727938028563 Epoch 1787 from 2000\n",
      "Measure Loss tensor(0.0459, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018987824241988621 Epoch 1788 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006290364463255308 Epoch 1789 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003546323933871602 Epoch 1790 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010665399203679358 Epoch 1791 from 2000\n",
      "Measure Loss tensor(0.0515, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001883718522038061 Epoch 1792 from 2000\n",
      "Measure Loss tensor(0.0474, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002127504227066567 Epoch 1793 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017386136788759324 Epoch 1794 from 2000\n",
      "Measure Loss tensor(0.0473, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001494824921688613 Epoch 1795 from 2000\n",
      "Measure Loss tensor(0.0462, grad_fn=<AddBackward0>)\n",
      "tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0018940024993839418 Epoch 1796 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002640142094690117 Epoch 1797 from 2000\n",
      "Measure Loss tensor(0.0449, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0027630016005606777 Epoch 1798 from 2000\n",
      "Measure Loss tensor(0.0478, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0020729675286763672 Epoch 1799 from 2000\n",
      "Measure Loss tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001086573233442987 Epoch 1800 from 2000\n",
      "Measure Loss tensor(0.0472, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006335650933925172 Epoch 1801 from 2000\n",
      "Measure Loss tensor(0.0496, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007208583881732972 Epoch 1802 from 2000\n",
      "Measure Loss tensor(0.0475, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008664185479969967 Epoch 1803 from 2000\n",
      "Measure Loss tensor(0.0506, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007145040533606225 Epoch 1804 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00036385230669737584 Epoch 1805 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00017493304893434388 Epoch 1806 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00033510336585062053 Epoch 1807 from 2000\n",
      "Measure Loss tensor(0.0476, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006801852720102548 Epoch 1808 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008633258921373874 Epoch 1809 from 2000\n",
      "Measure Loss tensor(0.0473, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007786243945545686 Epoch 1810 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0005875344559581277 Epoch 1811 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000541847921931324 Epoch 1812 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006346178343746688 Epoch 1813 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007046143840516267 Epoch 1814 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000602204796946689 Epoch 1815 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00042175262561079896 Epoch 1816 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002928564387669198 Epoch 1817 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00029113497636238994 Epoch 1818 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003477283882180135 Epoch 1819 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003580274132143576 Epoch 1820 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002753072988964217 Epoch 1821 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016209485405505551 Epoch 1822 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(9.4395e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.439502761782619e-05 Epoch 1823 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(9.4550e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.455042816064998e-05 Epoch 1824 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011768873613382665 Epoch 1825 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011318560179692205 Epoch 1826 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(7.2542e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.254191338995483e-05 Epoch 1827 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(3.0755e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.075463148077401e-05 Epoch 1828 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(1.9995e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.9995218568605307e-05 Epoch 1829 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(3.9828e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.9828405783117915e-05 Epoch 1830 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(6.2756e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.275582088587644e-05 Epoch 1831 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(6.4325e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.432454848215007e-05 Epoch 1832 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(4.6276e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.627622083532168e-05 Epoch 1833 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(2.9007e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.9007385193863807e-05 Epoch 1834 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(2.9815e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.9814942436567404e-05 Epoch 1835 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(4.7089e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.7088813707874824e-05 Epoch 1836 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(6.6278e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.627786544481208e-05 Epoch 1837 from 2000\n",
      "Measure Loss tensor(0.0497, grad_fn=<AddBackward0>)\n",
      "tensor(7.5524e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.552396821993034e-05 Epoch 1838 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(7.6640e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.663993103825007e-05 Epoch 1839 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(8.0215e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.021514738652669e-05 Epoch 1840 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(9.6508e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.650755828685841e-05 Epoch 1841 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001265417918518112 Epoch 1842 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001666764977035241 Epoch 1843 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021207264578475529 Epoch 1844 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002689236404376589 Epoch 1845 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003455242428782375 Epoch 1846 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004589487892290322 Epoch 1847 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006175521404936706 Epoch 1848 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008415266816730172 Epoch 1849 from 2000\n",
      "Measure Loss tensor(0.0493, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0011353201625623115 Epoch 1850 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015368210788692977 Epoch 1851 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002031576915586136 Epoch 1852 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026614227555347807 Epoch 1853 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003306937565838306 Epoch 1854 from 2000\n",
      "Measure Loss tensor(0.0475, grad_fn=<AddBackward0>)\n",
      "tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003954559673531789 Epoch 1855 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004288900943959849 Epoch 1856 from 2000\n",
      "Measure Loss tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004274421522829918 Epoch 1857 from 2000\n",
      "Measure Loss tensor(0.0477, grad_fn=<AddBackward0>)\n",
      "tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003651975509473692 Epoch 1858 from 2000\n",
      "Measure Loss tensor(0.0463, grad_fn=<AddBackward0>)\n",
      "tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026778337473752603 Epoch 1859 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015538354777629082 Epoch 1860 from 2000\n",
      "Measure Loss tensor(0.0470, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006749439202876097 Epoch 1861 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000191989575188064 Epoch 1862 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010951874885546816 Epoch 1863 from 2000\n",
      "Measure Loss tensor(0.0495, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0003348304957063355 Epoch 1864 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007778206040981241 Epoch 1865 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013957370310767807 Epoch 1866 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021029562549928944 Epoch 1867 from 2000\n",
      "Measure Loss tensor(0.0473, grad_fn=<AddBackward0>)\n",
      "tensor(0.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0028143178899592515 Epoch 1868 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003202473302491684 Epoch 1869 from 2000\n",
      "Measure Loss tensor(0.0459, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003117751192845655 Epoch 1870 from 2000\n",
      "Measure Loss tensor(0.0469, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023829125485309802 Epoch 1871 from 2000\n",
      "Measure Loss tensor(0.0459, grad_fn=<AddBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013509604963296451 Epoch 1872 from 2000\n",
      "Measure Loss tensor(0.0468, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00045626065061051875 Epoch 1873 from 2000\n",
      "Measure Loss tensor(0.0477, grad_fn=<AddBackward0>)\n",
      "tensor(6.2926e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.29255602950516e-05 Epoch 1874 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00021363472025634752 Epoch 1875 from 2000\n",
      "Measure Loss tensor(0.0499, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000746761477960443 Epoch 1876 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014919906643758845 Epoch 1877 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023084691888578795 Epoch 1878 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003113183785641419 Epoch 1879 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0036022258159963167 Epoch 1880 from 2000\n",
      "Measure Loss tensor(0.0460, grad_fn=<AddBackward0>)\n",
      "tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0035340523471569067 Epoch 1881 from 2000\n",
      "Measure Loss tensor(0.0470, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026230508366033196 Epoch 1882 from 2000\n",
      "Measure Loss tensor(0.0450, grad_fn=<AddBackward0>)\n",
      "tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0013216189971568442 Epoch 1883 from 2000\n",
      "Measure Loss tensor(0.0464, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00030826853653826905 Epoch 1884 from 2000\n",
      "Measure Loss tensor(0.0469, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012019690825369031 Epoch 1885 from 2000\n",
      "Measure Loss tensor(0.0478, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006809811922552751 Epoch 1886 from 2000\n",
      "Measure Loss tensor(0.0501, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.001597690501665881 Epoch 1887 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002570914793414197 Epoch 1888 from 2000\n",
      "Measure Loss tensor(0.0512, grad_fn=<AddBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0033687911292499505 Epoch 1889 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0039054044400507696 Epoch 1890 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0037746566184326653 Epoch 1891 from 2000\n",
      "Measure Loss tensor(0.0459, grad_fn=<AddBackward0>)\n",
      "tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.002902256878483577 Epoch 1892 from 2000\n",
      "Measure Loss tensor(0.0464, grad_fn=<AddBackward0>)\n",
      "tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0014502301765504841 Epoch 1893 from 2000\n",
      "Measure Loss tensor(0.0454, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00033260024571485114 Epoch 1894 from 2000\n",
      "Measure Loss tensor(0.0460, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00019503607503483443 Epoch 1895 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0008866897200163488 Epoch 1896 from 2000\n",
      "Measure Loss tensor(0.0479, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017907883994564915 Epoch 1897 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0024048672467766787 Epoch 1898 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0026125240982569635 Epoch 1899 from 2000\n",
      "Measure Loss tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0023546554495798475 Epoch 1900 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0017575256285417944 Epoch 1901 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009591165129946394 Epoch 1902 from 2000\n",
      "Measure Loss tensor(0.0469, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000305355308959786 Epoch 1903 from 2000\n",
      "Measure Loss tensor(0.0467, grad_fn=<AddBackward0>)\n",
      "tensor(7.7652e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.765226025859223e-05 Epoch 1904 from 2000\n",
      "Measure Loss tensor(0.0473, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002887812796091893 Epoch 1905 from 2000\n",
      "Measure Loss tensor(0.0473, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0006678077137965647 Epoch 1906 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0009152936539678205 Epoch 1907 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000924261609320115 Epoch 1908 from 2000\n",
      "Measure Loss tensor(0.0498, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.000729058407191728 Epoch 1909 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0004534848891881174 Epoch 1910 from 2000\n",
      "Measure Loss tensor(0.0494, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00020746901784186747 Epoch 1911 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(7.5161e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.516107085266162e-05 Epoch 1912 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(7.5434e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.54344843189922e-05 Epoch 1913 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00016185072835758394 Epoch 1914 from 2000\n",
      "Measure Loss tensor(0.0478, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00025076307491897937 Epoch 1915 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00027772484180384145 Epoch 1916 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0002351721675877101 Epoch 1917 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0001586764884127626 Epoch 1918 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(9.4506e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.450614873565013e-05 Epoch 1919 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(6.6957e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.695736314727254e-05 Epoch 1920 from 2000\n",
      "Measure Loss tensor(0.0492, grad_fn=<AddBackward0>)\n",
      "tensor(7.6378e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.637794873956245e-05 Epoch 1921 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00010500601804632446 Epoch 1922 from 2000\n",
      "Measure Loss tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00013253937931249083 Epoch 1923 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00014166438258771097 Epoch 1924 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00012601043118401985 Epoch 1925 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(9.2372e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.23715155325614e-05 Epoch 1926 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(5.6226e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.622590156771179e-05 Epoch 1927 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(3.3351e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.335073418275255e-05 Epoch 1928 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(3.0623e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.062261349449124e-05 Epoch 1929 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(4.4299e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.4298643601220795e-05 Epoch 1930 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(6.3761e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 6.376132576020724e-05 Epoch 1931 from 2000\n",
      "Measure Loss tensor(0.0491, grad_fn=<AddBackward0>)\n",
      "tensor(7.8684e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.868376465087287e-05 Epoch 1932 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(8.2355e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.235455647718369e-05 Epoch 1933 from 2000\n",
      "Measure Loss tensor(0.0489, grad_fn=<AddBackward0>)\n",
      "tensor(7.4185e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 7.418452990083791e-05 Epoch 1934 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(5.7655e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.765521474889907e-05 Epoch 1935 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(3.8679e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.867914369180334e-05 Epoch 1936 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(2.3016e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.301622994528773e-05 Epoch 1937 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(1.4526e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4525858397639987e-05 Epoch 1938 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(1.4182e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4182072289752061e-05 Epoch 1939 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(2.0177e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.017708008478265e-05 Epoch 1940 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(2.8951e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8951037692582692e-05 Epoch 1941 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(3.6672e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.667223265088677e-05 Epoch 1942 from 2000\n",
      "Measure Loss tensor(0.0488, grad_fn=<AddBackward0>)\n",
      "tensor(4.0746e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.0746352389760634e-05 Epoch 1943 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(4.0070e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.0069788860300364e-05 Epoch 1944 from 2000\n",
      "Measure Loss tensor(0.0487, grad_fn=<AddBackward0>)\n",
      "tensor(3.5490e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.5490061287825205e-05 Epoch 1945 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(2.8607e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8606902718468624e-05 Epoch 1946 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(2.1374e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.1374247376467183e-05 Epoch 1947 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.5280e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5279998190668984e-05 Epoch 1948 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(1.1333e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1333206961595639e-05 Epoch 1949 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(9.8368e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.83681480548304e-06 Epoch 1950 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(1.0584e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.0584499087045963e-05 Epoch 1951 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(1.2835e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2835041057130994e-05 Epoch 1952 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(1.5598e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.559805628521396e-05 Epoch 1953 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(1.7984e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7984320670467982e-05 Epoch 1954 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(1.9406e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.9405604327076257e-05 Epoch 1955 from 2000\n",
      "Measure Loss tensor(0.0486, grad_fn=<AddBackward0>)\n",
      "tensor(1.9709e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.9709204347705387e-05 Epoch 1956 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.9024e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.9023845692406594e-05 Epoch 1957 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(1.7620e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.7619657326905765e-05 Epoch 1958 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.5758e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5757929936694342e-05 Epoch 1959 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.3741e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3741238633728337e-05 Epoch 1960 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.1855e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1854974434035525e-05 Epoch 1961 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.0372e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.0371919845867096e-05 Epoch 1962 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(9.4495e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.449456586084181e-06 Epoch 1963 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(9.0866e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.086626071867262e-06 Epoch 1964 from 2000\n",
      "Measure Loss tensor(0.0485, grad_fn=<AddBackward0>)\n",
      "tensor(9.1241e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.124110691952702e-06 Epoch 1965 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(9.4489e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.448929704153097e-06 Epoch 1966 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(9.9300e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 9.930046729166362e-06 Epoch 1967 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.0499e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.0498597252262177e-05 Epoch 1968 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.1087e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1087100836930708e-05 Epoch 1969 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.1654e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.1653664590896193e-05 Epoch 1970 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.2123e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2123468135154941e-05 Epoch 1971 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.2506e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2505607379012113e-05 Epoch 1972 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.2843e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.2842515013435361e-05 Epoch 1973 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.3192e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.3192184655047944e-05 Epoch 1974 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.3620e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.361996212477261e-05 Epoch 1975 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.4199e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.4199378614315056e-05 Epoch 1976 from 2000\n",
      "Measure Loss tensor(0.0484, grad_fn=<AddBackward0>)\n",
      "tensor(1.5006e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.5005903423840535e-05 Epoch 1977 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.6151e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.615124931665745e-05 Epoch 1978 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(1.7773e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 1.777263995837048e-05 Epoch 1979 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(2.0131e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.0130709003854872e-05 Epoch 1980 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(2.3453e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.345258808881369e-05 Epoch 1981 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(2.8206e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 2.8205706460593088e-05 Epoch 1982 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(3.4971e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 3.497099664363855e-05 Epoch 1983 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(4.4850e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 4.484989012986144e-05 Epoch 1984 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(5.9100e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 5.909996796952574e-05 Epoch 1985 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(8.0361e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 8.036078336061819e-05 Epoch 1986 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00011137261256695864 Epoch 1987 from 2000\n",
      "Measure Loss tensor(0.0481, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00015848374002448313 Epoch 1988 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00022727001875997633 Epoch 1989 from 2000\n",
      "Measure Loss tensor(0.0480, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00033346736433345777 Epoch 1990 from 2000\n",
      "Measure Loss tensor(0.0483, grad_fn=<AddBackward0>)\n",
      "tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.00048620626529096073 Epoch 1991 from 2000\n",
      "Measure Loss tensor(0.0478, grad_fn=<AddBackward0>)\n",
      "tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0007235611086956165 Epoch 1992 from 2000\n",
      "Measure Loss tensor(0.0482, grad_fn=<AddBackward0>)\n",
      "tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0010507601194552074 Epoch 1993 from 2000\n",
      "Measure Loss tensor(0.0475, grad_fn=<AddBackward0>)\n",
      "tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0015523415716732995 Epoch 1994 from 2000\n",
      "Measure Loss tensor(0.0480, grad_fn=<AddBackward0>)\n",
      "tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0021764317054682155 Epoch 1995 from 2000\n",
      "Measure Loss tensor(0.0469, grad_fn=<AddBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.0030670067796222332 Epoch 1996 from 2000\n",
      "Measure Loss tensor(0.0476, grad_fn=<AddBackward0>)\n",
      "tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.003917278117980554 Epoch 1997 from 2000\n",
      "Measure Loss tensor(0.0459, grad_fn=<AddBackward0>)\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.004862563798152883 Epoch 1998 from 2000\n",
      "Measure Loss tensor(0.0469, grad_fn=<AddBackward0>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "PINN Loss 0.005116666182480998 Epoch 1999 from 2000\n",
      "tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8.6176e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4777e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.0424e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.4258e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.1398e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.6974e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.4474e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.3649e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.3072e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.1645e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.0043e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8.3547e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.7777e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.6489e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.4886e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.2465e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "After LBFGS-B: PINN Loss 0.005116666182480998 Epoch 1999 from 2000\n"
     ]
    }
   ],
   "source": [
    "folder = r'/home/suarez08/PhD_PINNs/Results_Simulation/14.06/'+str(ev_x)+'_invp_'+str(n_epoch)\n",
    "# Call the datasets functions, losses and weights for the training and for the performance measure\n",
    "[dirichlet_bc_2, pde_loss_2, initial_condition_2], [bc_dataset_2, pde_dataset_2, ic_dataset_2], [boundary_weights_2,\n",
    "                                                                                     residual_weights_2,\n",
    "                                                                                     initial_weights_2] = Dataset_loss('Quad', [lb, ub], 1, deg=DEG)\n",
    "[dirichlet_bc, pde_loss, initial_condition], [bc_dataset, pde_dataset, ic_dataset], [boundary_weights,\n",
    "                                                                                     residual_weights,\n",
    "                                                                                     initial_weights] = Dataset_loss('Mse', [lb, ub], len(initial_weights_2), deg=DEG)\n",
    "[dirichlet_bc_3, pde_loss_3, initial_condition_3], [bc_dataset_3, pde_dataset_3, ic_dataset_3], [boundary_weights_3,\n",
    "                                                                                     residual_weights_3,\n",
    "                                                                                     initial_weights_3] = Dataset_loss('Sobolev_1', [lb, ub], len(initial_weights_2), deg=DEG,\n",
    "                                                                                                                      k_res = 2, k_bdy= 4, k_ini =2)\n",
    "model_1 = pf.models.MLP(input_size=2, output_size=1, hidden_size=50, num_hidden=10, lb=lb, ub=ub, activation=torch.nn.ELU())\n",
    "model_2 = pf.models.MLP(input_size=2, output_size=1, hidden_size=50, num_hidden=10, lb=lb, ub=ub, activation=torch.nn.ELU())\n",
    "model_3 = pf.models.MLP(input_size=2, output_size=1, hidden_size=50, num_hidden=10, lb=lb, ub=ub, activation=torch.nn.ELU())\n",
    "performance_var = [initial_condition, [dirichlet_bc], pde_loss]\n",
    "#pinn_1 = pf.PINN(model_1, 3, 1, pde_loss,initial_condition, performance_var, [dirichlet_bc], use_gpu=False\n",
    "#                )\n",
    "#loss_1 = pinn_1.fit(n_epoch, 'Adam', 1e-3,\n",
    "#                   pinn_path=folder+'best_model_Mse_'+str(DEG)+'_'+str(n_epoch)+'_.pt')\n",
    "#pinn_2 = pf.PINN(model_2, 3, 1, pde_loss_2, initial_condition_2, performance_var, [dirichlet_bc_2] ,use_gpu=False)\n",
    "#loss_2= pinn_2.fit(n_epoch, 'Adam', 1e-3,pinn_path=folder+\n",
    "#                'best_model_Quad_'+str(DEG)+'_'+str(n_epoch)+'_.pt')\n",
    "pinn_3 = pf.PINN(model_3, 2, 1, pde_loss_3, initial_condition_3, performance_var, [dirichlet_bc_3] ,use_gpu=False)\n",
    "loss_3 = pinn_3.fit(n_epoch, 'Adam', 1e-3,\n",
    "                    pinn_path = folder+'best_model_Wass_'+str(DEG)+'_'+str(n_epoch)+'_.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#Produce\n",
    "#!mkdir /Users/juanesteban/PhD_PINNs/Results_Simulation/27.04/\n",
    "fig = plt.figure()\n",
    " #ax2 = fig.add_subplot(2, 1, 1)\n",
    "#plt.semilogy(loss_1.numpy(), label='MSE Loss')\n",
    "#plt.semilogy(loss_2.numpy(), label='Quadrature Loss')\n",
    "plt.semilogy(loss_3.numpy(), label='Sobolev Loss')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.ylabel('H1 Performance')\n",
    "plt.title('$\\lambda$: '+str(lam)+', Degree: '+str(DEG))\n",
    "plt.legend()\n",
    "#plt.savefig(folder + 'Sob_H2_H2_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_lam(deg, pinn):\n",
    "    residual_set, res_weights , Hkw_res = Hk_coeff(deg,0)\n",
    "    Xs = torch.Tensor(residual_set).T\n",
    "    Xs.requires_grad = True\n",
    "    lam = res_right(Xs,pinn(Xs)).dot(torch.Tensor(res_weights))/pinn(Xs).T[0].dot(torch.Tensor(res_weights))\n",
    "    print(res_right(Xs,pinn(Xs)).dot(torch.Tensor(res_weights)))\n",
    "    print(pinn(Xs).T[0].dot(torch.Tensor(res_weights)))\n",
    "    return lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = np.linspace(lb[0], ub[0],100)\n",
    "y_t = np.linspace(lb[1], ub[1],100)\n",
    "t = 0\n",
    "X_c = torch.tensor([[[i, j] for i in x_t] for j in y_t])\n",
    "X_c1 = X_c.reshape(100*100,2)\n",
    "X_c1.requires_grad = True\n",
    "X_m,Y_m = np.meshgrid(x_t,y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pinn_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-aced01c761c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(schroedinger1d(X_c, pinn_1(X_c)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpinn_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'best_model_Mse_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mPRED_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlam_pred\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_c1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpinn_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_c1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_m\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mev_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlam_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pinn_1' is not defined"
     ]
    }
   ],
   "source": [
    "#print(schroedinger1d(X_c, pinn_1(X_c)))\n",
    "pinn_1.load_model(folder+'best_model_Mse_'+str(DEG)+'_'+str(n_epoch)+'_.pt')\n",
    "PRED_1 = pinn_1(X_c.float())\n",
    "lam_pred  = np.max(abs(res_right(X_c1,pinn_1(X_c1.float()))-lam*(Psi(X_m,Y_m,1,ev_x).reshape(100*100))).detach().numpy())\n",
    "lam_pred = \"{:.4f}\".format(lam_pred)\n",
    "Li_dist = Li_dist =np.max(abs((PRED_1[:, :, 0])-Psi(X_m,Y_m,1,ev_x)).flatten().detach().numpy())\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')#fig.add_subplot(2, 1, 2, projection='3d')\n",
    "c1 = ax.plot_surface(X_m, Y_m, PRED_1[:,:,0].detach().numpy(),label='Trained Psi',\n",
    "                    color='blue')\n",
    "c3 = ax.plot_wireframe(X_m, Y_m, Psi(X_m,Y_m,0,1),label ='Real Psi',color = 'red')\n",
    "ax.text(0.5, 0.5, 1, str(Li_dist)+\"-\"+str(lam_pred), color='black')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('$\\lambda$: '+str(lam)+', Deg: '+str(DEG)+', $\\lambda$_p: '+str(lam_pred))\n",
    "plt.savefig(folder + 'Final_dist_mse.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Li_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pinn_3.load_model(folder+'best_model_Wass_'+str(DEG)+'_'+str(n_epoch)+'_.pt')\n",
    "PRED_3 = model_3(X_c.float())\n",
    "lam_pred  = np.mean(abs(res_right(X_c1,pinn_3(X_c1.float()))-lam*(Psi(X_c1[:,0].float(),X_c1[:,1].float()))).flatten().detach().numpy())\n",
    "lam_pred = \"{:.2f}\".format(lam_pred)\n",
    "Li_dist = np.max(abs((PRED_3[:, :, 0])-Psi(X_m,Y_m)).flatten().detach().numpy())\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca(projection='3d')#\n",
    "c2 = ax.plot_surface(X_m, Y_m, PRED_3[:, :, 0].detach().numpy(), label='Trained Psi_wass',\n",
    "                    color='orange')\n",
    "c3 = ax.plot_wireframe(X_m, Y_m, Psi(X_m,Y_m),label ='Real Psi',color = 'red')\n",
    "ax.text(0.5, 0.5, 1, str(Li_dist)+\"-\"+str(lam_pred), color='red')\n",
    "plt.title('$\\lambda$: '+str(lam)+', Deg: '+str(DEG)+', $\\lambda$_p: '+str(lam_pred))\n",
    "#plt.savefig(folder + 'Final_'+str(DEG)+'_dist_H2_H2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7468015"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Li_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.68'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030648574"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Li_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
